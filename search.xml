<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>矢量语义与嵌入之 TF-IDF 检索</title>
    <url>/280EQA3/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>自然语言处理就是研究如何让计算机理解人类语言的一门技术。将计算机不能理解的人类语言编码成可以理解的向量化表示，是 NLP 工作的开始与基础。TF-IDF 是 NLP 入门的基础知识。通过对这种编码方式的学习，可以使我们更加容易理解 NLP 工作的本质。</p>
<span id="more"></span>

<blockquote>
<p>这块内容第一次学的时候比较难理解，后来看到<a href="https://mofanpy.com/tutorials/machine-learning/nlp/intro-search/">莫烦老师</a>讲的才算是彻底明白。补充一下这块内容。</p>
</blockquote>
<p>NLP 的首要任务：Vector Semantics and Embedding，也可以说是文本向量化，或着文本特征提取。想要表达的就是，NLP 首先要做的是，将文本语言变成计算机可以理解的语言——向量，否则后面的工作自然是无法开展的。</p>
<p>在文本向量化的问题中，词袋模型（Bag-of-Words Model）和词嵌入（Word Embedding）是两种最常用的模型。更准确地说，词向量只能表征单个词，如果要表示文本，需要做一些额外的处理。本篇笔记首先学习词袋模型与 TF-IDF。</p>
<h2 id="0x01-TF-IDF-的使用场景"><a href="#0x01-TF-IDF-的使用场景" class="headerlink" title="0x01 TF-IDF 的使用场景"></a>0x01 <code>TF-IDF</code> 的使用场景</h2><p>以搜索引擎为例，有了批量性地召回相对合适的内容后，比如我已经从 1 亿个网页中召回了 100 万个，但 100 万对于我来说，已经够让我看上好几年了。怎么能再继续提升一下精确度，找到我更在乎的内容呢？</p>
<p>所以需要对召回的这 100 万个内容，做一个【问题与内容】的相似度排序，只返回那些头部内容（问题指的是用户搜索的词条，内容就是我们召回的网页）。</p>
<p>因此简单来说，<code>TF-IDF</code> 要做的就是相似度排序，我们取相似度最高的内容返回给用户。从本质上来说，<code>TF-IDF</code> 是一种 <strong>向量表达</strong> 把语言 <strong>向量化</strong>，将词语，句子，文章转为词向量，句向量，文章向量。因为计算机只能理解数字，所以把我们要 <strong>把计算机不能理解的文本转为向量</strong>。等看完后面的内容就会深刻理解为什么 <code>TF-IDF</code> 是 <strong>向量表达</strong>。</p>
<h2 id="0x02-TF-IDF-的原理"><a href="#0x02-TF-IDF-的原理" class="headerlink" title="0x02 TF-IDF 的原理"></a>0x02 <code>TF-IDF</code> 的原理</h2><p><code>TF</code> ：词频（Term Frequency）<br><code>IDF</code> ：逆文本频率指数（Inverse Document Frequency）</p>
<ul>
<li>词频 <code>TF</code> ：反应文章的 <strong>局部信息</strong>。<ul>
<li>在一篇文章中，越重要的内容，出现（强调）的次数越多，那么词频 <code>TF</code> 就会越高。所以这些高词频的词，就可以代表这篇文章。</li>
<li>但伴随而来的问题是，文章中许多的语气词或者“你我他”这种词或者标点符号，同样也会出现很多次，但这些词往往也是高频词，但是没有意义。如何解决这种情况？那就需要 <code>IDF</code>。</li>
</ul>
</li>
<li>逆文本频率指数 <code>IDF</code> ：反应系统的 <strong>全局信息</strong>。<ul>
<li><code>IDF</code> 可以帮助我们判断词语在系统中的 <strong>区分力</strong> 大小。<ul>
<li>比如，如果 <strong>每篇文章</strong> 中都有“我”，那么它在所有文章中的 <strong>区分力都不强</strong>。</li>
<li>如果你搜索的关键词是“莫烦”，<strong>全网都没有几个</strong> 叫“莫烦”的，那么“莫烦” <code>IDF</code> 就会很大，即“莫烦”的 <strong>区分力更强</strong>。</li>
</ul>
</li>
<li>既然 <code>TF</code> 是以单篇文章为中心的局部词信息，但并不知道如果放到全局（所有文章），哪些 <code>TF</code> 高频词是全局垃圾词（中性词），是搜索时没有意义的词； <code>IDF</code> 是统计所有文章的全局词信息，可以分辨 <code>TF</code> 中的哪些高频词是全局垃圾词，但并不知道每个单篇文章中的高频关键词。那么为什么不把两种指标结合，发挥各自的优势？因此通过 <code>TF-IDF</code> 来表达一篇文章。</li>
</ul>
</li>
<li><code>TF-IDF</code> : <code>TF</code> × <code>IDF</code><ul>
<li>将两种指数相乘，得到 <code>TF-IDF</code> 表达一篇文章。</li>
<li>降低没有意义的词的重要性，突出文章中真正具有关键意义的内容（词语）。</li>
</ul>
</li>
</ul>
<p>举个例子：比如有以下三篇文章，我们选取图中的四个词来表达这三篇文章。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203211003678.png" alt="|700"></p>
<p>计算出这四个词的 <code>TF</code> 与 <code>IDF</code> 之后，将两者相乘，得到 <code>TF-IDF</code>。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203211033747.png"></p>
<p>通过这些词的分数高低，可以判断每篇文章的关键词是什么。比如文章 1，即使“爱”的词频最高，是文章 1 中出现最多的词，但是通过 <code>IDF</code> 判断得知是全局垃圾词，反而“莫烦”的全局区分力更强（即使词频不高），所以根据综合评分 <code>TF-IDF</code> 得知——在文章 1 中，“莫烦” 的权值更重，更能代表这篇文章。</p>
<p>至此，我们得到了三篇文章的向量化表示，那么如何在搜索中应用这三个文章向量呢？</p>
<h2 id="0x03-TF-IDF-的应用"><a href="#0x03-TF-IDF-的应用" class="headerlink" title="0x03 TF-IDF 的应用"></a>0x03 <code>TF-IDF</code> 的应用</h2><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203211047534.png"></p>
<p>假设我们搜索“莫烦 Python”，计算机首先通过词表的模式，计算这个搜索问句（“莫烦 Python”）的 <code>TF-IDF</code> 值。然后计算搜索问句与每篇文章的 <code>TF-IDF</code> 值的 <code>cosine</code> 距离。简单来说就是将所有文章，按照词向量的维度放入四维空间（对应四个词），然后将搜索问句向量也放进去，最后查找哪一篇文章离这个搜索问句最近，越近说明相似度越高。由此找到与搜索问句最匹配的文章。图中是用三维空间来说明，一样的意思。</p>
<p>动手实操一下吧： <a href="https://radimrehurek.com/gensim/models/tfidfmodel.html">models.tfidfmodel – TF-IDF model — gensim</a></p>
<h2 id="0x04-向量化表达"><a href="#0x04-向量化表达" class="headerlink" title="0x04 向量化表达"></a>0x04 向量化表达</h2><p>向量化是 NLP 工作的基础。将问句，词语，句子或者文章，通过数字的形式投射到空间中，也就是将这些语言转为计算机可以理解的向量，然后按照向量的模式指向空间中的某个位置。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203211101372.png"></p>
<p>如图，比如 <code>0,23,116.5,21</code> 就是文章 1 的向量表达，下面是文章 2 与搜索问题的向量表达。所以 <code>TF-IDF</code> 本质上是一种向量表达。NLP 中还有许多其他的向量表达与应用，<code>TF-IDF</code> 是其中一种比较基础且好用的一种方式。</p>
<p>在实际应用中，一般不存在的词是不放入每条数据的词袋的，通过这样的方式来节约内存。这种技术叫稀疏矩阵（Sparse Matrix）：只存储有内容的值，而忽略无内容的值。因此词袋模型的文本表征，其实是一种稀疏向量表达，因为向量里面的元素大部分都是 0，只有出现在当前文本的词语才有词频的赋值。</p>
<blockquote>
<p>拓展：<br><code>TF-IDF</code> 就是一张将 <code>词语重要程度</code> 转换成 <code>向量</code> 的文档展示方式，那么在这些向量中，必定会有主导型元素，而这些元素其实就是这篇文档中很重要的关键词了。因此除了搜索匹配之外，<code>TF-IDF</code> 还有很多的应用，比如将挑选文档中的关键词（将主导元素提取出来）。<br>另外，由于 <code>IDF</code> 是所有文档的全局信息，那么带有不同属性的文档集群可能拥有不同性质的 <code>IDF</code> 分布。比如金融领域的 <code>IDF</code> 与生物领域的 <code>IDF</code> ，如果我要搜索金融相关的信息，却是在生物领域的 <code>IDF</code> 下搜索，那么得到的结果必然是不准确的。因此我需要一个带有金融属性的 <code>IDF</code> 表来优化对金融子领域的搜索。这也是 IDF 比较重要的应用方式之一。</p>
</blockquote>
<h2 id="0x05-词袋模型与-TF-IDF"><a href="#0x05-词袋模型与-TF-IDF" class="headerlink" title="0x05 词袋模型与 TF-IDF"></a>0x05 词袋模型与 <code>TF-IDF</code></h2><p>词袋（BoW）模型是数字文本表示的最简单形式。像单词本身一样，我们可以将一个句子表示为一个词向量包（一个数字串）。初入 NLP 可能会对这些概念产生困惑，比如什么是词袋模型，<code>TF-IDF</code> 就是词袋模型吗，下面来解释一下。</p>
<h3 id="词集与词袋模型"><a href="#词集与词袋模型" class="headerlink" title="词集与词袋模型"></a>词集与词袋模型</h3><p>词袋模型将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。这个模型的主要作用也就是对文本做单词切分，有点从一篇文章里提取关键词这种意思，旨在 <strong>用向量来描述文本的主要内容</strong>，其中包含了词集与词袋两种。</p>
<p><strong>词集模型</strong>：单词构成的集合，集合中每个元素只有一个，即词集中的每个元素为一个单词，词集中的每个单词只出现一次，不重复。</p>
<p><strong>词袋模型</strong>：在词集的基础上加入了频率这个维度，即统计单词在文档中出现的次数（令牌化和出现频数统计），通常我们在应用中都选用词袋模型。或者说，在词集的基础上，如果一个单词在文档中出现不止一次，统计其出现的次数。说人话就是，将每篇文章看成一袋子词，并忽略每个词出现的顺序。</p>
<h3 id="词袋模型与-TF-IDF-联合使用"><a href="#词袋模型与-TF-IDF-联合使用" class="headerlink" title="词袋模型与 TF-IDF 联合使用"></a>词袋模型与 <code>TF-IDF</code> 联合使用</h3><p>词袋创建一组向量，其中包含文档中的单词出现次数，而 <code>TF-IDF</code> 编码可以识别其中每个单词的区分力，然后赋予权重。因此通过词袋模型是一种基础模型， <code>TF-IDF</code> 是一种编码方式，基于词袋模型，我们可以采用 <code>TF-IDF</code> 编码，也可以使用 One-Hot 编码或者其他的编码方式。</p>
<p>我理解就是，恰好词袋模型实现了词频统计，恰好这正是 <code>TF</code> 要做的事情，所以再加上 <code>IDF</code> 联合起来，基于词袋模型实现了我们要做的  <code>TF-IDF</code> 编码。</p>
<h2 id="0x06-文本向量化"><a href="#0x06-文本向量化" class="headerlink" title="0x06 文本向量化"></a>0x06 文本向量化</h2><p>刚入门 NLP 想必会对这些专业名词头晕眼花，下面简单列举一下关系：</p>
<ul>
<li>文本向量化（文本表征 Word Representation）<ul>
<li>词袋模型及其编码方法（BoW）<ul>
<li>One-Hot 编码</li>
<li>TF 编码</li>
<li>TF-IDF 编码</li>
<li>N-gram 编码</li>
</ul>
</li>
<li>词嵌入模型（Word Embedding）<ul>
<li>Word2Vec<ul>
<li>Skip-Gram 模型</li>
<li>CBOW 模型</li>
</ul>
</li>
<li>GloVe</li>
</ul>
</li>
<li>主题模型（Topic Model）<ul>
<li>LSA 模型</li>
<li>PLSA 模型</li>
<li>LDA 模型</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Bag-of-Words Model（BoW）和 <code>TF-IDF</code> 编码都是帮助我们将文本句子转换为向量的技术，是使得机器理解文本的技术。</p>
<p>但是 BoW 这样的技术会有一些弊端，比如语序关系丢失（忽略上下文），异常依赖于优秀的词汇库，缺乏相似词之间的表达，并且向量稀疏。为了解决这些问题，下一篇笔记学习另一类向量化的方法——Word Embedding 词嵌入。即自然语言中的词语转化为稠密的向量，相似的词会有相似的向量表示，这样的转化方便挖掘文字中词语和句子之间的特征。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>TF-IDF:<br> <a href="https://blog.csdn.net/weixin_44799217/article/details/116423520">自然语言处理(NLP)之使用 TF-IDF 模型计算文本相似度</a><br> <a href="https://mofanpy.com/tutorials/machine-learning/nlp/intro-search/">你天天用的搜索引擎是怎么工作的 - 自然语言处理 | 莫烦 Python</a><br> <a href="https://mofanpy.com/tutorials/machine-learning/nlp/tfidf/">统计学让搜索速度起飞 - 自然语言处理 | 莫烦 Python</a></p>
<p>词袋模型与 TF-IDF:<br> <a href="https://www.jianshu.com/p/0422853b57a8">词袋模型与 TF-IDF</a><br> <a href="https://blog.csdn.net/fendouaini/article/details/108655680">词袋模型和 TF-IDF</a><br> <a href="https://blog.csdn.net/baidu_41797613/article/details/121268152">词袋模型与 TF-IDF 模型</a><br> <a href="https://www.jiqizhixin.com/graph/technologies/87c62b00-48b2-4e2a-8122-9876a3d3e59e">词袋模型 | 机器之心</a></p>
<p> 词袋模型与词嵌入：<br>  <a href="https://davidchen93.blog.csdn.net/article/details/79993369?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_default&utm_relevant_index=1">词袋模型和词向量模型概念介绍</a><br>  <a href="https://zhuanlan.zhihu.com/p/71065945">从词袋模型 TF-IDF 到词嵌入 Word Embedding</a></p>
<p>斯坦福经典 NLP 教材：<br>  <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">https://web.stanford.edu/~jurafsky&#x2F;slp3&#x2F;6.pdf</a></p>
]]></content>
      <categories>
        <category>Dive into NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>矢量语义与编码之 TF-IDF 检索</title>
    <url>/3EGCD4S/</url>
    <content><![CDATA[<h1 id="矢量语义与编码之词嵌入"><a href="#矢量语义与编码之词嵌入" class="headerlink" title="矢量语义与编码之词嵌入"></a>矢量语义与编码之词嵌入</h1><h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>上篇笔记中，我们学习了关于文本表征基于词袋模型得到稀疏向量化文本的内容，比如 One-Hot 编码，比如 TF-IDF 编码。本质上来说，词袋模型其实是一种通过统计学方法，将文档中出现的所有词语根据词频等统计指标，<strong>转为稀疏向量</strong>，从而表达一篇文章的方法。但是，这种稀疏向量的表达会存在一些缺点，比如不考虑语序等等。那么有没有办法得到文本向量化的稠密表达呢？这就是本篇笔记要讲的，Word Embedding 词嵌入。</p>
<span id="more"></span>

<h2 id="0x01-什么是词嵌入与词向量"><a href="#0x01-什么是词嵌入与词向量" class="headerlink" title="0x01 什么是词嵌入与词向量"></a>0x01 什么是词嵌入与词向量</h2><p><strong>Word Embedding 词嵌入</strong> 是一种 <strong>考虑词语位置关系</strong> 的文本表征模型。词嵌入将文本中的词转换成<strong>稠密的低维数字向量</strong>的表达。低维且稠密的特点提高了网络的学习效率，并且有利于学习词语之间的相似性。</p>
<p>也就是说，通过学习文本，用词向量的方式表征词的语义信息，即将所有的词，嵌入（Embedding）一个较低维度的连续向量空间中，词嵌入的结果就生成了 <strong>Word Vector 词向量</strong>。Embedding 其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。</p>
<p>之所以说是较低维度，是因为词向量与词袋模型得到的稀疏向量相比，维度是低的多的，对于词袋模型来说，每出现一个词即增加一维向量，假设字典中有 4000 个词，那么就需要 4000 维向量来表达；而词向量所在的连续空间，可以指定维度，可以是 128 维，也可以是 200 维，或者是其他自定义的维度。</p>
<p>在词向量的连续空间中，词嵌入使得语义上相似的单词在该空间内距离很近，语义不同的单词之间距离更远。词与词之间的相似度（关系），或者说距离，可以通过 <code>cosine</code> 求得。</p>
<p>听起来好像很复杂，其实只需要明白，区别于之前的词袋模型那种<strong>稀疏向量表达</strong>：通常都是一个高维的向量，向量里面的元素大部分都是 0，然后通过稀疏矩阵存储，去除没有意义的元素。所以造成每个文档的向量表达的长度是不一样的，也就是他们的向量表达维度是不一样的，不固定的，自然也是不连续的。</p>
<p>而词嵌入也是用一个向量来表示一个词，但是它是使用一个<strong>较低的维度，稠密地表示</strong>。或者说，词嵌入固定了一个维度（规定好一个连续空间），每个词的都有这么多维度，每个维度都会计算出一个数值，然后每一个词，都通过这样一个固定维度的连续向量来表征。</p>
<p>以 <code>hello</code> 这个词语为例。</p>
<p>稀疏向量表示：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203212325688.png"></p>
<p>稠密向量表示（嵌入表示）：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203212326164.png"></p>
<h2 id="0x02-词嵌入模型背景"><a href="#0x02-词嵌入模型背景" class="headerlink" title="0x02 词嵌入模型背景"></a>0x02 词嵌入模型背景</h2><h3 id="1-词嵌入的优势"><a href="#1-词嵌入的优势" class="headerlink" title="1. 词嵌入的优势"></a>1. 词嵌入的优势</h3><p>上篇笔记讲到的文本表征方法——词袋模型有以下缺点：</p>
<ul>
<li>无法表达词语之间的关系，不考虑词语的<strong>有序性</strong>。</li>
<li>对每个词来说，自身是一个独热向量（相互正交），无法编码词之间的<strong>相似性</strong>。</li>
<li>过于稀疏的向量表达，导致计算和存储的效率都不高。可以想象，当我们的词表 Vocab 增大到十万时，每个词都需要 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="7.15ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 3160.4 688"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mn" transform="translate(1444.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g><g data-mml-node="mi" transform="translate(2444.4,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container> 的向量来表达，严重浪费了内存和计算资源。</li>
</ul>
<p>那么本篇笔记要介绍的文本表征方法——词嵌入有哪些优势呢？</p>
<ul>
<li>考虑了词语之间的（位置）关系，即考虑到词语的<strong>有序性</strong>。</li>
<li>语义相似的词在向量空间上也会比较相近，即可学习词语之间的<strong>相似性</strong>。</li>
<li>可以将文本通过一个低维稠密向量来表达，计算和储存效率相比于词袋模型要高。</li>
<li>通用性很强，可以用在不同的任务中。</li>
</ul>
<h3 id="2-两种主流的-Word-Embedding-算法"><a href="#2-两种主流的-Word-Embedding-算法" class="headerlink" title="2. 两种主流的 Word Embedding 算法"></a>2. 两种主流的 Word Embedding 算法</h3><p>目前主流的词嵌入技术有两种，一种是 Word2Vec，另一种是 GloVe。</p>
<ul>
<li>Word2Vec<ul>
<li>这是一种基于统计方法来获得词向量的方法，于 2013 年由谷歌的 Mikolov 提出的一套新的词嵌入方法。</li>
<li>这种算法有 2 种训练模式<ul>
<li><code>CBOW</code> 通过上下文来预测当前词</li>
<li><code>SKip-Gram</code> 通过当前词来预测上下文</li>
</ul>
</li>
</ul>
</li>
<li>GloVe (Globel Vectors)<ul>
<li>GloVe 是<strong>对 Word2Vec 的扩展</strong>，它将全局统计和 Word2vec 的基于上下文的学习结合了起来。</li>
<li>其实就是 SVD 分解与 Word2Vec 的结合。</li>
</ul>
</li>
</ul>
<p>本篇笔记主要学习 Word2Vec。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204280423986.png"></p>
<blockquote>
<p>Word2Vec 是一种可以进行高效率词嵌入学习的预测模型。其两种变体分别为：连续词袋模型（CBOW）及 Skip-Gram 模型。</p>
<p>从算法角度看，这两种方法非常相似，其区别为 CBOW 根据源词上下文词汇（’the cat sits on the’）来预测目标词汇（例如，‘mat’），而 Skip-Gram 模型做法相反，它通过目标词汇（例如，‘mat’）来预测源上下文词汇（’the cat sits on the’）。</p>
<p>Skip-Gram 模型采取 CBOW 的逆过程的动机在于：CBOW 算法对于很多分布式信息进行了平滑处理（例如将一整段上下文信息视为一个单一观察量）。很多情况下，对于小型的数据集，这一处理是有帮助的。相形之下，Skip-Gram 模型将每个“上下文-目标词汇”的组合视为一个新观察量，这种做法在大型数据集中会更为有效。</p>
</blockquote>
<h3 id="3-Word2Vec-的前身"><a href="#3-Word2Vec-的前身" class="headerlink" title="3. Word2Vec 的前身"></a>3. Word2Vec 的前身</h3><p>由于 One-Hot 编码存在两个重要缺陷：<strong>维度灾难</strong>与<strong>语义鸿沟</strong>。因此分布式向量横空出世，将词表示成 short dense vector，弥补了以上两个问题。而后引出了 NNLM (Neural Network Language Model)。</p>
<blockquote>
<p>“You shall know a word by the company it keeps”  (J. R. Firth 1957)</p>
</blockquote>
<h4 id="Neural-Network-Language-Model"><a href="#Neural-Network-Language-Model" class="headerlink" title="Neural Network Language Model"></a>Neural Network Language Model</h4><p>NNLM 出自 Yoshua Bengio 等人于 2003 年发表的《A Neural Probabilistic Language Model》，针对 N-gram 模型的问题进行了解决。这是第一篇提出神经网络语言模型的论文，它在得到语言模型的同时，<strong>也生成了副产品——词向量</strong>。</p>
<blockquote>
<p>什么是语言模型？</p>
<p>通俗解释：<strong>「语言模型就是判断一句话是不是人话，通常用概率来表示一句话是人话的可能性」</strong></p>
<p>标准定义：对于语言序列  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="14.029ex" height="1.441ex" role="img" focusable="false" viewBox="0 -443 6201 637"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1152.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1597.2,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2749.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(3194.4,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(4533.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4977.8,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>，语言模型就是计算该序列出现的概率，即 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="17.489ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7730 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1140,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(2292.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(2737.2,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(3889.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(4334.4,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(5673.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(6117.8,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(7341,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 。</p>
</blockquote>
<p><strong>和 N-gram 类似，NNLM 也假设当前词仅依赖于前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 个词。</strong></p>
<ul>
<li><p>神经网络语言模型（Neural Network Language Model）</p>
<ul>
<li>首次提出了 Word Embedding 的概念（虽然没有叫这个名字），从而奠定了包括 Word2Vec 在内后续研究的基础。</li>
<li>模型结构：<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204281556450.png" alt="|700"></li>
<li>NNLM 学习任务：输入某个句中单词 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="12.387ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 5475 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1560,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2338,0)"><g data-c="2036"><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z"></path><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z" transform="translate(275,0)"></path></g></g><g data-mml-node="mi" transform="translate(2888,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(3647,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(4113,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4564,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(4925,0)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g></g></g></svg></mjx-container> 前面句子的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="4.714ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2083.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(583.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1583.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 个单词，要求网络正确预测单词 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="4.609ex" height="1.57ex" role="img" focusable="false" viewBox="0 -683 2037 694"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(759,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1225,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1676,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></svg></mjx-container>。即最大化 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="4.609ex" height="1.57ex" role="img" focusable="false" viewBox="0 -683 2037 694"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(759,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1225,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1676,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></svg></mjx-container> 出现的条件概率。</li>
<li>NNLM 训练过程<ul>
<li>首先是一个线性的映射层：输入层。<ul>
<li>任意单词 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.875ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1271 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 用 One-Hot 编码（如：001000）作为原始单词输入。</li>
<li>输入 N 个 One-Hot 编码，通过一个共享的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="5.373ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 2375 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(828,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1606,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> 的矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container>，映射为 N 个分布式的词向量 (Distributed Vector)。即乘以矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> 获得词向量。</li>
<li>矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> 的维度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="5.373ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 2375 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(828,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1606,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> ，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> 是词典的大小，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.873ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 828 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g></g></g></svg></mjx-container> 是 Embedding 向量的维度（一个先验参数）。矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> 每一行内容对应着单词的 Word Embedding。该矩阵的内容通过学习获得。</li>
<li>为什么矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> 每一行内容正是每个单词的 Word Embedding 呢？因为当我们把 n 个 One-Hot 表征的词向量输入到神经网络中，单层网络进行的运算等效于查表操作，每个词的 One-Hot 编码将各自的词向量从 Embedding 层中原封不动地取出。如下图所示。<img data-src=" https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204280214399.png" style="zoom: 80%;"></li>
</ul>
</li>
<li>其次是一个简单的前向反馈神经网络。<ul>
<li>由一个激活函数为 tanh 的隐藏层和一个 Softmax 输出（分类）层组成，因此可以计算出在输入 context 的条件下，词典中所有 word 出现的条件概率。</li>
<li>即将上一步映射的词向量拼接，然后接隐藏层，再接 Softmax 预测后面应该接哪个词。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>手绘模型：<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204281649219.png" alt="|700"></p>
<ul>
<li>输入的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1380.6 833"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> 维度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.631ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 2047 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1278,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> 是词典 Vocab 的大小。</li>
<li>输入层：获得词语 Embedding，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 为输入词个数，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 503 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g></g></g></svg></mjx-container> 为 Embedding 的维度。然后将所有 Embedding 拼接到一起。</li>
<li>隐藏层：长度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container>。</li>
<li>输出层：维度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.631ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 2047 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1278,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>。由 Softmax 激活得到归一化的概率。</li>
<li>模型参数：三个转移矩阵：词语 One-Hot 向量 =》输入层 =》隐藏层 =》输出层。其中由输入层到隐藏层的转移矩阵参数量是巨大的。</li>
</ul>
</li>
</ul>
<h2 id="0x03-Word2Vec-的两个模型"><a href="#0x03-Word2Vec-的两个模型" class="headerlink" title="0x03 Word2Vec  的两个模型"></a>0x03 Word2Vec  的两个模型</h2><h3 id="1-Word2Vec-之-CBOW-Continues-Bag-of-Words-Model"><a href="#1-Word2Vec-之-CBOW-Continues-Bag-of-Words-Model" class="headerlink" title="1. Word2Vec 之 CBOW (Continues Bag-of-Words Model)"></a>1. Word2Vec 之 CBOW (Continues Bag-of-Words Model)</h3><p><strong>思路：输入中间词前后的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> 个词，预测中间词。</strong></p>
<blockquote>
<p>既然 NNLM 的复杂度主要来自隐藏层，那么 Word2Vec 提出了 CBOW 与 Skip-Gram，将 NNLM 的隐藏层拿掉，输入层不再对 Embedding 拼接，而是求平均。</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204280404098.png"></p>
<ul>
<li>手绘模型<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204281720104.png"><ul>
<li>对原始 NNLM 做简化，将 NNLM 中的隐藏层拿掉</li>
<li>CBOW 的隐藏层：获得上下文词语的 Embedding 后做平均，直接输出到 Softmax。不需要激活。</li>
</ul>
</li>
</ul>
<h3 id="2-Word2Vec-之-Skip-Gram"><a href="#2-Word2Vec-之-Skip-Gram" class="headerlink" title="2. Word2Vec 之 Skip-Gram"></a>2. Word2Vec 之 Skip-Gram</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204280406149.png"></p>
<p>实际情况中，Skip-Gram 用的较多。CBOW 在小数据集上表现较好，Skip-Gram 在大的数据集上表现更好。</p>
<ul>
<li>手绘模型<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204281820027.png"></li>
</ul>
<h2 id="0x04-Word2Vec-的两个提速手段"><a href="#0x04-Word2Vec-的两个提速手段" class="headerlink" title="0x04 Word2Vec 的两个提速手段"></a>0x04 Word2Vec 的两个提速手段</h2><p>然而，每当计算一个词的概率都要对词典里的 V 个词计算相似度，然后进行归一化，这基本上时不现实的。为此，Mikolov 引入了两个提速手段：层次 Softmax（Hierarchical Softmax）和负采样（Negative Sampling）。普遍认为 Hierarchical Softmax 对低频词效果较好；Negative Sampling 对高频词效果较好，向量维度较低时效果更好。</p>
<ul>
<li>Hierarchical Softmax</li>
<li>Nagative Sampling</li>
</ul>
<h2 id="0x05-Word2Vec-的优缺点"><a href="#0x05-Word2Vec-的优缺点" class="headerlink" title="0x05 Word2Vec 的优缺点"></a>0x05 Word2Vec 的优缺点</h2><ul>
<li>优点<ul>
<li><strong>考虑上下文信息</strong>，效果优于之前的 One-Hot 编码等 Embedding 方法。</li>
<li><strong>由稀疏长向量（基于词袋模型构建）转为稠密短向量。</strong> 较短长度的向量更有利于模型的学习，减少了需要学习的参数量。稠密的特点便于更好的捕捉相同语义。</li>
<li>通用性强，可以用在各种 NLP 任务中。</li>
</ul>
</li>
<li>缺点<ul>
<li>**Word2Vec 是静态编码 (Static Embedding)**，意味着这种方法只能学到一种固定的 Embedding，存在 Vocab 中。也就是说 <strong>Word2Vec 不能解决多义词的问题</strong>，不能联系上下文动态解释当前词语的含义。以后会介绍学习动态上下文嵌入 (Dynamic Contextual Embedding) 的方法，比如当下流行的 BERT 表示，每个词的 Vector 会根据不同的上下文内容而改变。</li>
<li>由于词和向量是一对一的关系，所以<strong>不能学习到词语的多层特性</strong>。一个好的语言表示除了建模一词多义现象以外，还需要能够体现词的复杂特性，包括语法 (syntax)、语义 (semantics) 等。Word2Vec 等词嵌入方法本身不具备这种优点——因为它太简单了。</li>
<li>仅考虑了局部语料，没有考虑到全局信息。</li>
<li>英文语料的分词简单，每个词为独立的个体。但中文语料首先需要解决分词问题，分词效果严重影响词向量的质量。因此 Word2Vec 对中文不是那么友好。</li>
</ul>
</li>
</ul>
<blockquote>
<p>需要说明的是：Word2vec 是上一代的产物（18 年之前）， 18 年之后想要得到最好的效果，已经不使用  Word2vec 这种方法。</p>
</blockquote>
<h2 id="0x06-从-Word-Embedding-到-BERT"><a href="#0x06-从-Word-Embedding-到-BERT" class="headerlink" title="0x06 从 Word Embedding 到 BERT"></a>0x06 从 Word Embedding 到 BERT</h2><blockquote>
<p>Word2Vec 和  BERT 都是语言表示中里程碑式的工作，前者是词嵌入范式的代表，后者是预训练范式的代表。</p>
</blockquote>
<ul>
<li><p>Word Embedding</p>
<ul>
<li>统计语言模型 N-gram<ul>
<li>解决无序性问题，用之前的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 个文本来计算当前文本 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="3.283ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1451.3 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 的条件概率。即<strong>考虑上文，前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 个词</strong>。</li>
<li>未解决文本表征问题，仍使用 One-Hot 向量来表示。由此带来一系列弊端，如维度灾难，无法描述词语语义相似度等。</li>
<li>由于模型复杂度和预测精度的限制，我们很少会考虑 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex;" xmlns="http://www.w3.org/2000/svg" width="6.157ex" height="1.636ex" role="img" focusable="false" viewBox="0 -683 2721.6 723"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(1165.8,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(2221.6,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></svg></mjx-container> 的模型。因此无法处理更长程的 context (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex;" xmlns="http://www.w3.org/2000/svg" width="6.157ex" height="1.636ex" role="img" focusable="false" viewBox="0 -683 2721.6 723"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(1165.8,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(2221.6,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></svg></mjx-container>)。</li>
<li>复杂度<ul>
<li>（需要计算每个词，和其他所有词，共同出现的次数） <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 500 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g></g></g></svg></mjx-container> （该词出现的词频）</li>
<li>如果词典大小为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g></svg></mjx-container>，考虑前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 个单词，则模型参数量级为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.766ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2990.5 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mi" transform="translate(975.3,363) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2601.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>。</li>
</ul>
</li>
</ul>
</li>
<li>分布式表征 (Distribution)<ul>
<li>分布假说 (Distributional Hypothesis)：上下文环境相似的两个词有着相近的语义。</li>
<li>分布式表征是指将数 <strong>据嵌入到低维连续空间</strong>，通常称这种表征为嵌入 (Embedding) 或向量 (Vector)。</li>
</ul>
</li>
<li>基于 SVD 的词向量方法<ul>
<li>词袋假说 (Bag of Words Hypothesis)：一篇文档的词频（而不是词序）代表了文档的主题。</li>
<li>使用奇异值分解 (SVD) 的方法对词袋模型生成的矩阵降维并且分解。</li>
</ul>
</li>
<li>神经网络语言模型（Neural Network Language Model）<ul>
<li>首次提出了 Word Embedding 的概念（虽然没有叫这个名字），从而奠定了包括 Word2Vec 在内后续研究的基础。</li>
<li><strong>解决了 N-gram 模型的两个重要缺陷（由 One-Hot 表示造成的）</strong>：维度灾难，无法描述词语语义相似度。</li>
<li><strong>考虑上文，前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 个词。</strong></li>
</ul>
</li>
<li>Word2Vec<ul>
<li>CBOW：从一个句子里面把一个词抠掉，用这个词的<strong>上文和下文</strong>去预测被抠掉的这个词。</li>
<li>Skip-Gram：输入某个单词，要求网络<strong>预测它的上下文单词</strong>。</li>
<li><strong>与 N-gram 的不同：Word2Vec 并不关心相邻单词之前一起出现的频数，而是仅仅关心，这个单词是不是属于另一个单词的上下文</strong>。也就是说，Word2Vec 不关心根据这个词预测出的下一个词语是什么，而是只关心这两个词语之间是不是有上下文关系。</li>
</ul>
</li>
<li>Glove<ul>
<li>对 Word2Vec 的补充和拓展，将全局统计融入 Word2Vec。</li>
</ul>
</li>
</ul>
</li>
<li><p>ELMO (Embedding from Language Models)：<strong>基于上下文</strong>的 Embedding</p>
<ul>
<li>Deep contextualized word representation. 根据当前上下文对 Word Embedding 动态调整。解决了一词多义的问题。</li>
<li>两阶段模型<ul>
<li>预训练阶段<ul>
<li>使用双层双向的 LSTM 作为特征抽取器。</li>
<li>预训练阶段获得三种 Embedding。<ul>
<li>最底层的词向量特征。</li>
<li>第一层双向 LSTM 获得的句法特征。</li>
<li>第二层双向 LSTM 获得的语义特征。</li>
</ul>
</li>
</ul>
</li>
<li>下游任务应用阶段<ul>
<li>做下游应用时作为特征提取器。将三层 Embedding 根据不同的权重拼接起来用于下游任务，作为新特征补充。</li>
<li>权重根据下游任务训练得到。</li>
</ul>
</li>
</ul>
</li>
<li>ELMO 的缺点<ul>
<li>LSTM 抽取特征能力远弱于 Transformer。</li>
<li>通过拼接方式获得融合特征，这种方式对特征融合的能力偏弱。</li>
</ul>
</li>
</ul>
</li>
<li><p>GPT (Generative Pre-Training)：Pretrain + Finetune</p>
<ul>
<li>两阶段模型<ul>
<li>预训练阶段<ul>
<li>使用 Transformer 作为特征抽取器，其特征抽取能力强于 RNN。</li>
<li>采用单向的语言模型，只采用单词的 Context-before 上文来进行预测，抛开了下文（现在来看是不明智的选择）。白白丢失了许多信息。</li>
</ul>
</li>
<li>微调阶段（下游任务应用阶段）<ul>
<li>将预训练模型应用到下游任务时，需保持模型结构和预训练模型一致。借此，通过预训练学到的语言学知识就被引入到当前的任务中来了。</li>
<li>以预训练模型参数作为初始参数，用当前的任务去训练这个网络，对网络参数进行 Fine-Tuning。</li>
</ul>
</li>
</ul>
</li>
<li>GPT 的缺点<ul>
<li>语言模型是单向的。没有考虑下文信息。</li>
</ul>
</li>
</ul>
</li>
<li><p>BERT：新星的诞生</p>
<ul>
<li>两阶段模型<ul>
<li>Pre-Training<ul>
<li>Transformer 作为特征抽取器。</li>
<li>双向的语言模型。</li>
<li>多任务训练<ul>
<li>Masked LM，基于掩码的语言模型。<ul>
<li>类似 CBOW，进行了一定改造。</li>
<li>将 15% 字符进行 mask 操作，其中 80% 进行真正的掩码，10% 随机替换成其他字符，剩下的 10% 保留真正的字符不变。</li>
</ul>
</li>
<li>Next Sentence Prediction<ul>
<li>为了使模型能够理解两个句子之间的关系。</li>
<li>将文档中连续的语句拼接起来作为正样本，其他还会随机选择两句话进行拼接作为负样本用于训练，主要是因为一些句子序列任务会需要。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Fine-Tuning</li>
</ul>
</li>
<li>与 ELMO 的区别<ul>
<li>ELMO 是分别看上文和下文，然后将上文得到的结果和下文得到的结果进行拼接。</li>
<li>BERT 是同时看上下文中的每个词，效果上也比 ELMO 要好。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Word2Vec 的基本理念是构造一个假的学习任务，我们并不关注这个任务的输出结果如何，而是关注它的中间产物。从 Word Embedding 到 BERT 其实都是预训练模型的发展与壮大。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>语言模型：</p>
<p><a href="https://blog.csdn.net/qq_44951759/article/details/120428103">《自然语言处理学习之路》 08 语言模型</a> ——这里解释了为什么 N-gram 模型计算量巨大</p>
<p><a href="https://zhuanlan.zhihu.com/p/195698474">【语言模型】从 N-gram 模型讲起 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/206878986">【语言模型】NNLM(神经网络语言模型) - 知乎</a></p>
<p><a href="https://www.bilibili.com/video/BV1Lb411p7FD?p=1">word2vec：神经语言模型(NNLM), CBOW, skip-gram_bilibili</a> ——手绘模型</p>
<p>Word2Vec:</p>
<p><a href="https://blog.csdn.net/Morganfs/article/details/124417008">基于深度学习的语言模型</a></p>
<p><a href="https://www.cnblogs.com/sandwichnlp/p/11596848.html#word2vec">词向量(one-hot/SVD/NNLM/Word2Vec/GloVe) - 西多士 NLP</a></p>
<p><a href="https://blog.csdn.net/nemoyy/article/details/80603438">word2vec: 理解 nnlm, cbow, skip-gram</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/27234078">理解 Word2Vec 之 Skip-Gram 模型 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/51648675">深度学习第 42 讲：自然语言处理之词嵌入和词向量</a></p>
<p><a href="https://blog.csdn.net/weixin_42468475/article/details/121522295">词袋模型和词嵌入模型</a></p>
<p><a href="https://easyai.tech/ai-definition/word-embedding/">一文看懂词嵌入</a></p>
<p><a href="https://www.jianshu.com/p/2fbd0dde8804">词嵌入</a></p>
<p><a href="https://blog.csdn.net/stupid_3/article/details/83184807">矢量语义——从 TF-IDF 到 Word2Vec 你所需要知道的一切！</a></p>
<p><a href="https://lilianweng.github.io/posts/2017-10-15-word-embedding/">Learning Word Embedding | Lil’Log</a></p>
<p><a href="https://mofanpy.com/tutorials/machine-learning/nlp/cbow/">Continuous Bag of Words (CBOW) - 自然语言处理 | 莫烦 Python</a></p>
<p><a href="https://www.bilibili.com/video/BV1fp4y147Lc?spm_id_from=333.337.search-card.all.click">RNN 模型与 NLP 应用(2/9)：文本处理与词嵌入_bilibili</a></p>
<p><a href="https://blog.csdn.net/itplus/article/details/37969519">word2vec 中的数学原理详解</a></p>
<p>Word2Vec拓展：</p>
<p><a href="https://www.zhihu.com/question/352468546">Bert 比之 Word2Vec,有哪些进步呢？</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/49271699">从 Word Embedding 到 Bert 模型—自然语言处理中的预训练技术发展史</a> ⭐⭐⭐</p>
<p><a href="https://zhuanlan.zhihu.com/p/105989051">广告行业中那些趣事系列 3：NLP 中的巨星 BERT</a> ⭐⭐⭐——分析了 BERT 如何吸取各种模型精华于一身，从而构造出这样一颗新星。</p>
]]></content>
      <categories>
        <category>Dive into NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>1.1 框架式思维——漏斗思维</title>
    <url>/6S0ESX/</url>
    <content><![CDATA[<h1 id="框架式思维——漏斗思维"><a href="#框架式思维——漏斗思维" class="headerlink" title="框架式思维——漏斗思维"></a>框架式思维——漏斗思维</h1><h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>漏斗思维，是一个非常高效的思维方式，是一种线性的思考逻辑，一般按照任务的完成路径，识别出几个关键的行为转化节点；然后分析行为点间的转化与流失情况，进而定位问题，指导决策。</p>
<p>这种思维其实已经渗透在各行各业中，虽然平时并没有很好的觉察，但是，把他单独抽象出来，会让你更加容易洞悉到问题本质</p>
<span id="more"></span>

<h3 id="举一些例子"><a href="#举一些例子" class="headerlink" title="举一些例子"></a>举一些例子</h3><p><strong>以面试为例</strong>（层层筛选）：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205212209588.png"></p>
<p><strong>以咨询为例</strong>：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205221951849.png"></p>
<p><strong>以电商为例</strong>：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205221959021.png"></p>
<p><strong>以广告为例</strong>：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205222118804.png"></p>
<p><strong>以 AISAS 模型为例</strong>（小红书）：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205222002116.png"></p>
<h2 id="0x01-搜索引擎系统漏斗"><a href="#0x01-搜索引擎系统漏斗" class="headerlink" title="0x01 搜索引擎系统漏斗"></a>0x01 搜索引擎系统漏斗</h2><p>当我们在百度或者淘宝这些软件中，输入 Query 搜索自己想要的答案或者商品。搜索引擎如何从上千万乃至上亿的数据中，一步一步，返回满足用户需求的结果？</p>
<p>以百度搜索为例：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205222033968.png"></p>
<p>在这个过程中，关键环节在于其中的：召回 —&gt; 粗排 —&gt; 精排。</p>
<p>以淘宝搜索为例：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205222040679.png"></p>
<blockquote>
<p> ——摘抄自《阿里粗排技术体系与最新进展》<br> <a href="https://zhuanlan.zhihu.com/p/364141120">https://zhuanlan.zhihu.com/p/364141120</a></p>
</blockquote>
<p>在排序的过程中，往往会有一步重排。为什么从 粗排 —&gt; 精排 之后还需要一步重排呢？</p>
<ul>
<li>为什么要重排？<ul>
<li>保证结果的<strong>多样性</strong>：以短视频推荐为例，如果推荐的总是单一类型的视频（比如汽车），那么即使用户喜欢汽车，也会感到疲劳。所以通过重排，推荐一些相似领域，相关领域的视频，丰富推荐结果的多样性。</li>
<li><strong>多轮优选</strong>：Listwise，把所有的排列组合都考虑一遍。</li>
</ul>
</li>
</ul>
<h2 id="0x02-检索系统架构"><a href="#0x02-检索系统架构" class="headerlink" title="0x02 检索系统架构"></a>0x02 检索系统架构</h2><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205291244344.png"></p>
<h2 id="0x03-作业"><a href="#0x03-作业" class="headerlink" title="0x03 作业"></a>0x03 作业</h2><ul>
<li>我们的生活中，还有什么场景会涉及漏斗思维的地方？<ul>
<li>营销、注册转化</li>
</ul>
</li>
<li>在纸上自己画一下今天的图（系统架构图、广告漏斗转化模型，搜索系统漏斗图）<ul>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205241940531.png"></li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205242009540.png"></li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205242010805.png"></li>
</ul>
</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://zhuanlan.zhihu.com/p/200899462">如何理解并应用漏斗模型？ - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/364141120">阿里粗排技术体系与最新进展 - 知乎</a></p>
]]></content>
      <categories>
        <category>FunRec</category>
        <category>01-自然语言文本处理基础</category>
      </categories>
      <tags>
        <tag>FunRec</tag>
      </tags>
  </entry>
  <entry>
    <title>1.2 TF-IDF 实践</title>
    <url>/8CFDNJ/</url>
    <content><![CDATA[<h1 id="矢量语义-TF-IDF-的实践"><a href="#矢量语义-TF-IDF-的实践" class="headerlink" title="矢量语义 TF-IDF 的实践"></a>矢量语义 TF-IDF 的实践</h1><h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>TF-IDF 是 NLP 入门的基础知识。通过对这种编码方式的学习，可以使我们更加容易理解 NLP 工作的本质。这篇笔记重点在实践 TF-IDF。想了解更多相关理论可以参考 <a href="https://1nnoh.top/280EQA3/">矢量语义与嵌入之 TF-IDF 检索</a> 。</p>
<span id="more"></span>

<h2 id="0x01-Count-Vector"><a href="#0x01-Count-Vector" class="headerlink" title="0x01 Count Vector"></a>0x01 Count Vector</h2><ul>
<li>词袋模型 Bow<ul>
<li>目的：最基础的文本特征提取方法，将文本转为计数矩阵</li>
<li>原理：类似 One-Hot</li>
<li>优点：<ul>
<li>简单、直接</li>
<li>易于理解</li>
</ul>
</li>
<li>不足：<ul>
<li>稀疏</li>
<li>相当于只统计了词频<ul>
<li>但是像“你我他，的地得”这种词，每个文章中都会出现</li>
<li>所以需要如何把这种词的权重降下来？</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><ol>
<li>手撸 Count Vector</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">texts = ["He is a boy", "She is a girl, good girl"]

word_set = set()

# 存储词典
for text in texts:
    for word in text.strip().split(' '):
        word_set.add(word.strip(','))

print(word_set)
# 输出：
# {'He', 'girl', 'is', 'good', 'She', 'a', 'boy'}

# 给词典中所有词语赋一个 id 值
word_id_dict = {}
for word in enumerate(word_set):
    word_id_dict[word[1]] = word[0]

print(word_id_dict)
# 输出：
# {'He': 0, 'girl': 1, 'is': 2, 'good': 3, 'She': 4, 'a': 5, 'boy': 6}

# 根据上一步生成的 word_id_dict，将文档 texts 中的词语替换成对应的 id
res_list = []

for text in texts:
    t_list = []
    for word in text.strip().split(' '):
        word = word.strip(',')
        if word in word_id_dict:
            t_list.append(word_id_dict[word])
    res_list.append(t_list)

print(res_list)
# 输出：
# [[0, 2, 5, 6], [4, 2, 5, 1, 3, 1]]

# 将文档 texts 转为向量
# tests 中有两个文档，每个文档的向量长度为词典长度
# 根据 word_id_dict，出现的词语在 id 对应的列表位置 +1
for res in res_list:
    result = [0] * len(word_set)
    for word_id in res:
        result[word_id] += 1

    print(result)
# 输出：
# [1, 0, 1, 0, 0, 1, 1]
# [0, 2, 1, 1, 1, 1, 0]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="2">
<li>调用 Sklearn<br><a href="https://blog.csdn.net/weixin_38278334/article/details/82320307">sklearn——CountVectorizer 详解</a></li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Count Vector 方法只考虑了 TF，很片面！由此引出 TF-IDF（两者相乘）方法。</p>
<h2 id="0x02-TF-IDF-的原理"><a href="#0x02-TF-IDF-的原理" class="headerlink" title="0x02 TF-IDF 的原理"></a>0x02 TF-IDF 的原理</h2><ul>
<li><p>词频 <code>TF</code> ：反应文章的 <strong>局部信息</strong>。</p>
<ul>
<li>在一篇文章中，越重要的内容，出现（强调）的次数越多，那么词频 <code>TF</code> 就会越高。所以这些高词频的词，就可以代表这篇文章。</li>
<li>但伴随而来的问题是，文章中许多的语气词或者“你我他”这种词或者标点符号，同样也会出现很多次，但这些词往往也是高频词，但是没有意义。如何解决这种情况？那就需要 <code>IDF</code>。</li>
</ul>
</li>
<li><p>逆文本频率指数 <code>IDF</code> ：反应系统的 <strong>全局信息</strong>。</p>
<ul>
<li><code>IDF</code> 可以帮助我们判断词语在系统中的 <strong>区分力</strong> 大小。<ul>
<li>比如，如果 <strong>每篇文章</strong> 中都有“我”，那么它在所有文章中的 <strong>区分力都不强</strong>。</li>
<li>也就是说，在 <strong>所有文档</strong> 中都经常出现的词语，区分力小；而不常出现的词，区分力强。</li>
</ul>
</li>
</ul>
</li>
<li><p><code>TF-IDF</code> : <code>TF</code> × <code>IDF</code></p>
<ul>
<li>将两种指数相乘，得到 <code>TF-IDF</code>，表达一篇文档。</li>
<li>降低没有意义的词的重要性，突出文章中真正具有关键意义的内容（词语）。</li>
<li>对于一个 word，在文档出现的频率高，但在语料库里出现频率低，那么这个 word 对该文档的重要性比较高。</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-ad-note" data-language="ad-note"><code class="language-ad-note">TF(词频 - Term Frequency)：指的是某一个给定的词语在该文件中出现的次数，这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。

IDF(逆文本频率指数 - Inverse Document Frequency)：包含指定词语的文档越少，IDF越大。某个词语的IDF，由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到，即：
IDF=log(N/n)
-   N代表语料库中文档的总数
-   n代表某个word在几个文档中出现过；<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="1-前置知识点：文本相似度"><a href="#1-前置知识点：文本相似度" class="headerlink" title="1. 前置知识点：文本相似度"></a>1. 前置知识点：文本相似度</h3><ul>
<li><p>余弦相似度——最常用的相似度计算方法</p>
<ul>
<li>一个向量空间中两个向量夹角的余弦值作为衡量两个个体之间差异的大小</li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205251854838.png"></li>
<li><strong>余弦值接近 1，夹角趋于 0，表明两个向量越相似！</strong></li>
</ul>
</li>
<li><p>Jaccard 相似度——用于比较有限样本集之间的相似性与差异性</p>
<ul>
<li>Jaccard 系数定义为 A 与 B <strong>交集的大小</strong> 同 A 与 B <strong>并集的大小</strong>的比值</li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205251902319.png"></li>
<li>Jaccard Similarity 得分在 0 到 1 的范围内。如果两个文档相同，则 Jaccard Similarity 为 1。如果两个文档之间没有共同词，则 Jaccard 相似度得分为 0。</li>
<li>当 A 和 B 都为空，J(A,B) = 1</li>
</ul>
</li>
<li><p>欧氏距离相似度——最常见的距离度量方法</p>
<ul>
<li>n 维空间中计算两点间距离</li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205251915186.png"></li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>适用场景</strong>：<br>比如向量 A(1,1,1)，向量 B(5,5,5)，两个向量虽然余弦相似度一致（夹角为 0），但是在空间上，两个向量并非一致。类比到用户 A 购买了三个 1 元的物品，用户 B 购买了三个 5 元的物品，衡量这两个用户的消费能力时，显然用余弦相似度是不合适的，欧氏距离更加合适。</p>
<p>欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，比如使用用户行为作为指标分析用户价值的相似情况（比较不同用户的消费能力），这属于价值度量；而余弦相似度对绝对数值不敏感，更多的用于使用用户对内容的评分来分析用户兴趣的相似程度（用户是否喜欢某商品），这属于定性度量。</p>
<p>需要注意的是，欧氏距离和余弦相似度都需要保证各维度处于相同的刻度级别（量纲），所以一般需要对数据先进行标准化处理，否则很可能会引起偏差。比如用户对内容评分，假设为 5 分制，对用户甲来说评分 3 分以上就是自己喜欢的，而对于用户乙，评分 4 分以上才是自己喜欢的，这样就无法很好地衡量两个用户评分之间的相似程度。如果将评分数值减去平均值，那么就可以很好地解决问题。此时，就相当于用皮尔逊相关系数来度量相似程度。</p>
</blockquote>
<h4 id="实践：计算两个字符串的相似度"><a href="#实践：计算两个字符串的相似度" class="headerlink" title="实践：计算两个字符串的相似度"></a>实践：计算两个字符串的相似度</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205251937573.jpg"></p>
<p>首先生成文本的向量化表达：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import jieba
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

texts = ['这只皮靴号码大了，那只号码合适','这只皮靴号码不小，那只更合适']

# 分词
texts_cut = []

def cut_word(sentence):
    return '/'.join(jieba.lcut(sentence))

for text in texts:
    #text = text.replace('，', '')
    texts_cut.append(cut_word(text))
    # strip() 只能移除字符串头尾指定的字符序列，不能删除中间部分的字符
print(texts_cut)
# 输出：分词结果
# ['这/只/皮靴/号码/大/了/，/那/只/号码/合适', '这/只/皮靴/号码/不小/，/那/只/更/合适']

# 创建词袋数据结构
# CountVectorizer 参数中，默认的正则表达式选择 2 个及以上的字母或数字作为 token
# 那么会导致分词为单个字的词语被忽略
# 所以这里需要修改正则表达式参数
cv = CountVectorizer(token_pattern=r"(?u)\b\w+\b")  # 修改正则表达式参数

cv_fit = cv.fit_transform(texts_cut)
# 上行代码等价于下面两行
# cv.fit(texts)
# cv_fit=cv.transform(texts)

# 列表形式呈现文章生成的词典
print(cv.get_feature_names())
# 输出：词典
# ['不小', '了', '只', '号码', '合适', '大', '更', '皮靴', '这', '那']

# 字典形式的词典，带有词语 id
print(cv.vocabulary_)
# 输出：
# {'这': 8, '只': 2, '皮靴': 7, '号码': 3, '大': 5, '了': 1, '那': 9, '合适': 4, '不小': 0, '更': 6}
# 字典形式，key：词，value:该词（特征）的索引

# 所有文本的词频
print(cv_fit)
# 输出：
# (0, 8)  1  第 0 个文档中，词典索引为 8 的元素（这），词频为 1
# (0, 2)  2
# (0, 7)  1
# (0, 3)  2
# (0, 5)  1
# (0, 1)  1
# (0, 9)  1
# (0, 4)  1
# (1, 8)  1
# (1, 2)  2
# (1, 7)  1
# (1, 3)  1
# (1, 9)  1
# (1, 4)  1
# (1, 0)  1
# (1, 6)  1

# toarray() 将结果转为稀疏矩阵的表达
print(cv_fit.toarray())
# 输出：
# [[0 1 2 2 1 1 0 1 1 1]
#  [1 0 2 1 1 0 1 1 1 1]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol>
<li>使用余弦相似度计算<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205251939761.png"></li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 计算余弦相似度
texts_countvector = cv_fit.toarray()
cos_sim = cosine_similarity(texts_countvector[0].reshape(1,-1), texts_countvector[1].reshape(1,-1))

print('Cosine Similarity = %f'%cos_sim)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="2">
<li>使用 Jaccard 相似度计算</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 计算 jaccard 相似度
def jaccard_sim(a,b):
    unions = len(set(a).union(set(b)))
    intersections = len(set(a).intersection(set(b)))
    return intersections / unions

print('Jaccard Similarity = %f'%jaccard_sim(texts_cut[0],texts_cut[1]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="3">
<li>使用欧氏距离相似度计算</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 计算欧氏距离相似度
# calculating Euclidean distance
# using linalg.norm()
point1 = texts_countvector[0]
point2 = texts_countvector[1]
dist = np.linalg.norm(point1 - point2)

print('Euclidean Distance = %f'%dist)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="2-TF-标准计算公式"><a href="#2-TF-标准计算公式" class="headerlink" title="2. TF 标准计算公式"></a>2. TF 标准计算公式</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205261635717.png"></p>
<p>上一节中计算相似度使用的向量化方式其实就是 TF。一开始学习的 Count Vector 其实也是 TF，只是没有做归一化。</p>
<h3 id="3-IDF-标准计算公式"><a href="#3-IDF-标准计算公式" class="headerlink" title="3. IDF 标准计算公式"></a>3. IDF 标准计算公式</h3><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="52.925ex" height="5.285ex" role="img" focusable="false" viewBox="0 -1426 23393 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">逆</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">档</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">频</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">率</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">指</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mo" transform="translate(7000,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7389,0)"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(7893,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(8721,0)"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mo" transform="translate(9470,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(10136.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(11192.6,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(11490.6,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(11975.6,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(12452.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mfrac" transform="translate(12841.6,0)"><g data-mml-node="mrow" transform="translate(1081.2,676)"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">语</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">料</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">库</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">档</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">总</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">包</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">含</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">该</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">词</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">档</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mo" transform="translate(8222.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(9222.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><rect width="9922.4" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(23004,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p>
<p>分母加 1 为了防止式子除 0。<br>对整个式子取 log，为了防止小数过长。</p>
<h3 id="4-TF-IDF-总结"><a href="#4-TF-IDF-总结" class="headerlink" title="4. TF-IDF 总结"></a>4. TF-IDF 总结</h3><ul>
<li>TF-IDF<ul>
<li>目的</li>
<li>原理：<code>TF</code> × <code>IDF</code><ul>
<li>特点：稀疏</li>
</ul>
</li>
<li>优点<ul>
<li>考虑了词的重要性</li>
<li>简单快速，而且容易理解</li>
</ul>
</li>
<li>缺点<ul>
<li>稀疏——计算、存储效率</li>
<li>每个词之间相互独立<ul>
<li>只考虑词频信息，比较片面</li>
<li>不考虑词语的有序性（位置信息）</li>
<li>不能衡量词之间的相似度</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="0x03-TF-IDF-的实践"><a href="#0x03-TF-IDF-的实践" class="headerlink" title="0x03 TF-IDF 的实践"></a>0x03 TF-IDF 的实践</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">import math
import pandas as pd

docA = "The cat sat on my face"
docB = "The dog sat on my bed"

bowA = docA.split(' ')
bowB = docB.split(' ')

wordSet = set(bowA).union(set(bowB))

wordDictA = dict.fromkeys(wordSet, 0)
wordDictB = dict.fromkeys(wordSet, 0)

for word in bowA:
    wordDictA[word] += 1

for word in bowB:
    wordDictB[word] += 1

print(pd.DataFrame([wordDictA, wordDictB]))
# 输出：文本向量化
#    my  The  cat  on  bed  dog  sat  face
# 0   1    1    1   1    0    0    1     1
# 1   1    1    0   1    1    1    1     0

def computeTF(wordDict, bow):
    tfDict = {}
    bowCount = len(bow)
    for word, count in wordDict.items():
        tfDict[word] = count / float(bowCount)

    return tfDict

tfDictA = computeTF(wordDictA, bowA)
tfDictB = computeTF(wordDictB, bowB)

# IDF = log(语料库的文档总数 / (包含该词的文档数 + 1))
def computeIDF(docList):
    idfDict = {}
    N = len(docList)

    idfDict = dict.fromkeys(docList[0].keys(), 0)
    for doc in docList:
        for word, val in doc.items():
            if val &gt; 0:
                idfDict[word] += 1
    print('N:', N)
    print(idfDict)

    for word, val in idfDict.items():
        idfDict[word] = math.log10(N /( float(val)+1))

    return idfDict

idfs = computeIDF([wordDictA, wordDictB])
print(idfs)
# 输出：
# N: 2（总文档数为 2）
# 统计每个词在所有文档中出现的次数：
# {'my': 2, 'The': 2, 'cat': 1, 'on': 2, 'bed': 1, 'dog': 1, 'sat': 2, 'face': 1}
# 计算得到的 idf：
# {'my': -0.17609125905568127, 'The': -0.17609125905568127, 'cat': 0.0, 'on': -0.17609125905568127, 'bed': 0.0, 'dog': 0.0, 'sat': -0.17609125905568127, 'face': 0.0}

# 注意!!! 为什么有的 idf 是 0 ?
# 因为只出现一次的词语通过计算得到 log(2/1+1) = 0

def computeTFIDF(tfDict, idfs):
    tfidfDict = {}
    for word, val in tfDict.items():
        tfidfDict[word] = val * idfs[word]

    return tfidfDict

tfidfDictA = computeTFIDF(tfDictA, idfs)
tfidfDictB = computeTFIDF(tfDictB, idfs)

print(pd.DataFrame([tfidfDictA, tfidfDictB]))
# 输出：所有文档的 TF-IDF
#          my       The  cat        on  bed  dog       sat  face
# 0 -0.029349 -0.029349  0.0 -0.029349  0.0  0.0 -0.029349   0.0
# 1 -0.029349 -0.029349  0.0 -0.029349  0.0  0.0 -0.029349   0.0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<hr>
<h2 id="0x04-项目1：《文章关键信息提取》"><a href="#0x04-项目1：《文章关键信息提取》" class="headerlink" title="0x04 项目1：《文章关键信息提取》"></a>0x04 项目1：《文章关键信息提取》</h2><p>目标：对一个数据集中的所有文章做关键信息提取。</p>
<p>首先将 <code>input_tfidf_dir</code> 中的所有文章整个到一个文件里。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># -----------
# convert.py
# -----------

import os
import sys

# 「argv」是「argument variable」参数变量的简写形式，一般在命令行调用的时候由系统传递给程序。这个变量其实是一个List列表，argv[0] 一般是“被调用的脚本文件名或全路径”，这个与操作系统有关，argv[1]和以后就是传入的系统命令参数。
file_path_dir = sys.argv[1]

def file_handler(file_path):
    f = open(file_path, 'r')
    return f

file_name = 0
for f in os.listdir(file_path_dir):
    if not f.startswith('.'):  # 筛除 以 “.” 开头的隐藏文件
        file_path = file_path_dir + "/" + f
        content_list = []

        file = file_handler(file_path)
        for line in file:
            content_list.append(line.strip())

        print('\t'.join([str(file_name), ' '.join(content_list)]))

        file_name += 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 查看一下输出
python convert.py input_tfidf_dir
# 将输出存入文件
python convert.py input_tfidf_dir &gt; tfidf_input.data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>MapReduce 要做下面的事情：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205291138183.png"></p>
<p>将左边乱序的输出排序，相同的 word 放一起。执行 <code>reduce.py</code> 的时候，要考虑到如右图——最后一个 word 只有一行的情况。</p>
<p>将文档里出现的词打印出来。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># -----------
# map.py
# -----------

import sys

for line in sys.stdin:
    ss = line.strip().split('\t')
    if len(ss) != 2:  # 检查
        continue

    file_name, file_content = ss
    word_list = file_content.strip().split(' ')

    word_set = set(word_list)  # 去除一篇文档中重复的单词

    for word in word_set:
        print('\t'.join([word, '1']))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>检验一下 <code>map.py</code> 文件做了什么，可以做什么。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 输出每个文档中出行啊的词
cat tfidf_input.data | python map.py

# 打印输出的结果
cat tfidf_input.data | python map.py | grep '设计'

# 统计个数
cat tfidf_input.data | python map.py | grep -c '设计'

# 将输出结果做个排序
 cat tfidf_input.data | python map.py | sort -k1 -nr<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>计算 idf。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># -----------
# reduce.py
# -----------

import sys
import math

docs_cnt = 508

current_word = None
count = 0

for line in sys.stdin:
    ss = line.strip().split('\t')
    if len(ss) != 2:

        continue
    word, val = ss
    if current_word == None:
        current_word = word

    if current_word != word:
        idf = math.log10(docs_cnt / (float(count) + 1.0))  # 计算 idf
        print('\t'.join([current_word, str(idf)]))
        count = 0  # 重置 count
        current_word = word  # 换到下一个 word

    count += int(val)

# 防止最后一类只有一行，导致最后一类没有输出
idf = math.log10(docs_cnt / (float(count) + 1.0))
print('\t'.join([current_word, str(idf)]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 将计算的 idf 存储到 result_idf.data 文件里
cat tfidf_input.data | python map.py | sort -k1 -nr | python reduce.py &gt; result_idf.data

# 查看一下 “的” 的 idf
cat result_idf.data | grep '的'
# 输出：
# 的  0.0008557529505832833<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="0x05-项目2：文本相似度计算"><a href="#0x05-项目2：文本相似度计算" class="headerlink" title="0x05 项目2：文本相似度计算"></a>0x05 项目2：文本相似度计算</h2><p><a href="https://github.com/cjymz886/sentence-similarity">GitHub - cjymz886/sentence-similarity: 对四种句子/文本相似度计算方法进行实验与比较</a></p>
<p>使用 Word2Vec 的方法将所有句子生成句向量，然后利用四种方法计算文本相似度。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>配置 Vim：<br> <a href="https://bynss.com/linux/827013.html">怎样在 Ubuntu 上安装最新的 Vim | 月灯依旧</a><br> <a href="https://blog.csdn.net/weixin_36338224/article/details/120985361">保姆级教程！将 Vim 打造一个 IDE （Python 篇）</a><br> <a href="https://zhuanlan.zhihu.com/p/127933244">Vim-Python 环境 - 知乎</a><br> <a href="https://zhuanlan.zhihu.com/p/30022074">Vim - 配置 IDE 一般的 python 环境 - 知乎</a><br> <a href="https://blog.csdn.net/qq_34128332/article/details/115820865?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1-115820865-blog-123357406.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1-115820865-blog-123357406.pc_relevant_default&utm_relevant_index=2">Vim 插件 YouCompleteMe 安装归纳</a></p>
<p>文本相似度：<br> <a href="https://zhuanlan.zhihu.com/p/155960197">推荐算法原理（二）欧几里得距离计算物品间相似度 - 知乎</a><br> <a href="https://www.cnblogs.com/HuZihu/p/10178165.html">相似度度量：欧氏距离与余弦相似度</a><br> <a href="https://blog.csdn.net/steven_ffd/article/details/84881063">sklearn中CountVectorizer里token_pattern默认参数解读</a></p>
<p>相似度计算：<br> <a href="https://www.delftstack.com/zh/howto/python/cosine-similarity-between-lists-python/">Python 中的余弦相似度 | D栈 - Delft Stack</a><br> <a href="https://github.com/cjymz886/sentence-similarity">GitHub - cjymz886/sentence-similarity: 对四种句子/文本相似度计算方法进行实验与比较</a></p>
]]></content>
      <categories>
        <category>FunRec</category>
        <category>01-自然语言文本处理基础</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>FunRec</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫与网络编程基础</title>
    <url>/2FSK6BP/</url>
    <content><![CDATA[<p>跟着 <a href="https://coggle.club/blog/30days-of-ml-202203">Coggle 三月份的学习活动</a> 补充一下这方面的技能。恰好最近收集数据集也比较需要这方面的知识。</p>
<h2 id="0x00-内容介绍"><a href="#0x00-内容介绍" class="headerlink" title="0x00 内容介绍"></a>0x00 内容介绍</h2><p>爬虫与网络编程基础这部分的入门学习的要解决的问题如下：</p>
<ul>
<li>对网络编程了解较少，不会从 HTML 中提取信息；</li>
<li>不会爬虫，不会收集数据，也不会部署模型。</li>
</ul>
<p>而上述问题都是一个合格算法工程师所必备的。因此借助这个活动来入门。</p>
<span id="more"></span>

<h2 id="0x01-学习内容"><a href="#0x01-学习内容" class="headerlink" title="0x01 学习内容"></a>0x01 学习内容</h2><p>当今的世界是一个互联的世界，绝大多数的计算机和人都在通过网络和他人传递信息、沟通互联。我们在网络上学习、游戏、工作，我们提供各种各样的网络服务，又有很多人使用着各种各样的网络服务。网络改变了世界，而程序员“定义”了网络。我们在代码中实现了网络的通信，让一切变得可能。网络爬虫是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。<strong>在本次学习中我们将学习基础的爬虫操作，并学习基础的 HTTP 协议，最后尝试完成基础的网络编程。</strong></p>
<h2 id="0x02-打卡汇总"><a href="#0x02-打卡汇总" class="headerlink" title="0x02 打卡汇总"></a>0x02 打卡汇总</h2><table>
<thead>
<tr>
<th>任务名称</th>
<th>难度</th>
<th>所需技能</th>
</tr>
</thead>
<tbody><tr>
<td>任务 1：计算机网络基础</td>
<td>低、1</td>
<td>json、xml</td>
</tr>
<tr>
<td>任务 2：HTTP 协议与 requests</td>
<td>低、1</td>
<td>requests</td>
</tr>
<tr>
<td>任务 3：bs4 基础使用</td>
<td>中、2</td>
<td>bs4</td>
</tr>
<tr>
<td>任务 4：bs4 高阶使用</td>
<td>高、3</td>
<td>bs4</td>
</tr>
<tr>
<td>任务 5：正则表达式</td>
<td>高、3</td>
<td>re</td>
</tr>
<tr>
<td>任务 6：Python 网络编程基础</td>
<td>高、3</td>
<td>scoket</td>
</tr>
<tr>
<td>任务 7：tornado 基础使用</td>
<td>中、2</td>
<td>tornado</td>
</tr>
<tr>
<td>任务 8：tornado 用户注册&#x2F;登录</td>
<td>高、3</td>
<td>tornado</td>
</tr>
<tr>
<td>任务 9：tornado 部署机器学习模型</td>
<td>中、2</td>
<td>tornado</td>
</tr>
</tbody></table>
<ul>
<li><p><a href="network-spider-programming-task01.md">任务 1：计算机网络基础</a></p>
<ul>
<li>步骤 1：在 Pyhon 中创建一个 list，存储以下个人信息（姓名、年龄、成绩）：[小王、40、50]，[小贾、50、23]。</li>
<li>步骤 2：将步骤 1 的数据存储为 json 格式，并进行读取。</li>
<li>步骤 3：将步骤 1 的数据存储为 xml 格式，并进行读取。</li>
<li>步骤 4：学习 <a href="https://www.runoob.com/w3cnote/summary-of-network.html">计算机网络基础</a> ，思考从打开 <a href="https://coggle.club/">coggle.club</a> 到网页展示，有什么步骤？将你的思考结果写到博客。</li>
</ul>
</li>
<li><p>任务 2：HTTP 协议与 requests</p>
<ul>
<li>步骤 1： <a href="https://www.cnblogs.com/an-wen/p/11180076.html">学习 HTTP 协议</a></li>
<li>步骤 2：HTTP 的 get 和 post 有什么区别？用处在哪儿？将你的思考写到博客。</li>
<li>步骤 3：使用 Python 中 requests 中的 get 访问百度。</li>
</ul>
</li>
<li><p>任务 3：bs4 基础使用</p>
<ul>
<li>学习资料： <a href="https://beautiful-soup-4.readthedocs.io/en/latest/">https://beautiful-soup-4.readthedocs.io/en/latest/</a></li>
<li>步骤 1：使用 requests 和 bs4 爬取 <a href="https://scikit-learn.org/stable/modules/classes.html">sklearn api 页面</a>。</li>
<li>步骤 2：在 api 页面中有多少个模块？有多少个 API？如 sklearn.base.DensityMixin，其中 base 为模块，DensityMixin 为 API。</li>
<li>步骤 3：将模块名作为 key，api 作为 value 存储为字典。</li>
</ul>
</li>
<li><p>任务 4：bs4 高阶使用</p>
<ul>
<li>步骤 1：爬取 <a href="https://scikit-learn.org/stable/glossary.html">sklearn 机器学习名词页面</a> , 爬取所有的名词，如 1d、2d 和 api；</li>
<li>步骤 2：有一些名词在介绍时，会有额外的链接，请将每个名词介绍对应的链接也找出。</li>
<li>步骤 3：原始的名词安装本身有类别，如下所示，你能将爬取的结果进行分类吗？</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">General Concepts
Class APIs and Estimator Types
Target Types
Methods
Parameters
Attributes
Data and sample properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><p>任务 5：正则表达式</p>
<ul>
<li>步骤 1： <a href="https://www.runoob.com/python/python-reg-expressions.html">学习正则表达式 re 模块使用</a> 。</li>
<li>步骤 2：使用 re 筛选出机器学习名词，只包含字母的名词；</li>
<li>步骤 3：使用 re 筛选出机器学习名词，首字母为 A 或 首字母为 B 的名词。</li>
</ul>
</li>
<li><p>任务 6：Python 网络编程基础</p>
<ul>
<li>步骤 1： <a href="https://www.runoob.com/python/python-socket.html">学习 Socket 编程</a></li>
<li>步骤 2：使用编写一个 Socket 聊天机器人，程序 A 发送数据给程序 B，程序 B 也可以发送信息给程序 A；</li>
<li>步骤 3：使用编写一个 Socket 聊天机器人，程序 A 发送文件内容给 程序 B，程序 B 将文件进行存储。</li>
</ul>
</li>
<li><p>任务 7：tornado 基础使用</p>
<ul>
<li>步骤 1：学习 tornado 基础使用<ul>
<li>tornado 官网： <a href="https://www.tornadoweb.org/en/stable/">https://www.tornadoweb.org/en/stable/</a></li>
<li>tornado 教程：<ul>
<li><a href="http://www.ttlsa.com/docs/tornado/">http://www.ttlsa.com/docs/tornado/</a></li>
<li><a href="http://doc.iplaypy.com/tornado/ch1.html">http://doc.iplaypy.com/tornado/ch1.html</a></li>
</ul>
</li>
</ul>
</li>
<li>步骤 2：编写 tornado 的 hello word 程序。</li>
<li>步骤 3：编写 tornado 的 handler，分别接受 post 和 get 请求，请求为两个数字，进行求和，然后返回结果。</li>
</ul>
</li>
<li><p>任务 8：tornado 用户注册&#x2F;登录</p>
<ul>
<li>步骤 1：使用 sqlite 创建用户信息表，表包含 <code>uid</code>，<code>name</code>，<code>passwd</code> 三个字段。</li>
<li>步骤 2：编写 tornado 的用户注册 handler，完成用户注册逻辑，具体需要判断用户名和 passwd 合理性（不包含空格 &amp; 最大长度限制），然后插入数据。</li>
<li>步骤 3：编写 tornado 的用户登录 handler，完成用户登录逻辑，根据 name 和 passwd 判断是否登录成功。</li>
<li>步骤 4：结合 requests 和 tornado 完成上述逻辑。</li>
</ul>
</li>
<li><p>任务 9：tornado 部署机器学习模型</p>
<ul>
<li>步骤 1：读取 <a href="https://mirror.coggle.club/dataset/waimai_10k.csv">外卖评论数据集</a>。</li>
<li>步骤 2：使用 jieba 进行分词，TFIDF 提取特征，并选择分类器进行训练。</li>
<li>步骤 3：将文本分类模型使用 tornado 进行部署，客户端 requests 发送文本进行分类。</li>
</ul>
</li>
</ul>
<h2 id="0x03-学习资料"><a href="#0x03-学习资料" class="headerlink" title="0x03 学习资料"></a>0x03 学习资料</h2><ul>
<li><a href="https://www.jianshu.com/p/f6292d732217">https://www.jianshu.com/p/f6292d732217</a></li>
<li><a href="https://www.bilibili.com/video/av59706997/">https://www.bilibili.com/video/av59706997/</a></li>
<li><a href="https://cuiqingcai.com/1319.html">https://cuiqingcai.com/1319.html</a></li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p> <a href="https://coggle.club/blog/30days-of-ml-202203">Coggle 30 Days of ML（22 年 3 月） - Coggle 数据科学</a></p>
]]></content>
      <categories>
        <category>爬虫与网络编程基础</category>
      </categories>
      <tags>
        <tag>Coggle 30 Days of ML</tag>
        <tag>Spider</tag>
        <tag>Network Programming</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫与网络编程基础_Task01：计算机网络基础</title>
    <url>/3VQT4K6/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>为了使网络上获得的数据干净整洁，并且可以为自己的应用所用，我们得想出一些形式来表示应用与网络之间来来往往的数据。最常使用的两种技术就是：XML 和 JSON。</p>
<span id="more"></span>

<p>但是 Python 中的列表或者字典并不能在 Java 中使用（Java 中使用 HashMap），所以不能直接把 Python 中的字典传给 Java，而是应该发送 Python 与 Java 都能接受的数据。</p>
<p>因此，两者之间需要一种 <em>Wire Format</em> 来作为互相交换数据的媒介。而 XML 和 JSON 是两种比较常用的 <em>Wire Format</em>。</p>
<h2 id="0x01-Python-中创建-List"><a href="#0x01-Python-中创建-List" class="headerlink" title="0x01 Python 中创建 List"></a>0x01 Python 中创建 List</h2><h3 id="1-创建列表"><a href="#1-创建列表" class="headerlink" title="1. 创建列表"></a>1. 创建列表</h3><p>在 Pyhon 中创建一个 list，存储以下个人信息（姓名、年龄、成绩）：[小王、40、50]，[小贾、50、23]</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 定义列表
list &#x3D; [(&quot;小王&quot;,40,50),(&quot;小贾&quot;,50,23)]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[(&#39;小王&#39;, 40,50), (&#39;小贾&#39;, 50,23)]</code></p>
</blockquote>
<h3 id="2-导入列表"><a href="#2-导入列表" class="headerlink" title="2. 导入列表"></a>2. 导入列表</h3><p>如果想把 Numpy 数组转成 Json 格式，可以首先将数组转为列表。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">data &#x3D; data.tolist()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h2 id="0x02-将数据存储为-Json-格式，并进行读取"><a href="#0x02-将数据存储为-Json-格式，并进行读取" class="headerlink" title="0x02 将数据存储为 Json 格式，并进行读取"></a>0x02 将数据存储为 Json 格式，并进行读取</h2><h3 id="JSON-库的一些用法"><a href="#JSON-库的一些用法" class="headerlink" title="JSON 库的一些用法"></a>JSON 库的一些用法</h3><table>
<thead>
<tr>
<th>方法</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>json.dumps ()</td>
<td>将 Python 对象编码成 Json 字符串</td>
</tr>
<tr>
<td>json.loads ()</td>
<td>将 Json 字符串解码成 Python 对象</td>
</tr>
<tr>
<td>json.dump ()</td>
<td>将 Python 中的对象转化成 Json 储存到文件中</td>
</tr>
<tr>
<td>json.load ()</td>
<td>将文件中的 Json 的格式转化成 Python 对象提取</td>
</tr>
</tbody></table>
<p>json.dump () 和 json.dumps () 的区别</p>
<ul>
<li>json.dumps () 是把 Python 对象转换成 Json 对象的一个过程，生成的是字符串。</li>
<li>json.dump () 是把 Python 对象转换成 Json 对象生成一个 fp 的文件流，和文件相关。</li>
</ul>
<p>所以后面多的一个 <code>s</code> 可以理解为 <code>string</code></p>
<p>更详细的内容可以参考 <a href="https://www.w3cschool.cn/article/30808038.html">Python json.dumps()函数使用解析 | w3c 笔记</a></p>
<h3 id="1-存储到-JSON-文件中"><a href="#1-存储到-JSON-文件中" class="headerlink" title="1. 存储到 JSON 文件中"></a>1. 存储到 JSON 文件中</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 导入 json 模块
import json

# 为新建 json 文件命名
filename &#x3D; &#39;info.json&#39;
# 使用json.dump()方法转为json格式数据
with open(filename, &#39;w&#39;) as file:
    json.dump(list, file)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 读取 json 文件
with open(filename, &#39;r&#39;) as file:
    info &#x3D; json.load(file)

print (info)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>printout:<br><code>[[&#39;小王&#39;, 40,50], [&#39;小贾&#39;, 50,23]]</code></p>
</blockquote>
<p>Tips：</p>
<ul>
<li>默认会转为二进制数据，目前存储的 Json 文件内容为：<code>[[&quot;\u5c0f\u738b&quot;, 40,50], [&quot;\u5c0f\u8d3e&quot;, 50,23]]</code>。读取的时候默认又转到了可读的 Python 对象。所以读取并打印的时候看见的是字符。</li>
<li>如果希望文件中不使用二进制存储，通过 <code>ensure_ascii=False</code> 设置不转为二进制，即存储文件时：<code>json.dump(list, file, ensure_ascii=False)</code>。</li>
</ul>
<h3 id="2-编码成-JSON-字符串"><a href="#2-编码成-JSON-字符串" class="headerlink" title="2. 编码成 JSON 字符串"></a>2. 编码成 JSON 字符串</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 使用 json.dumps() 方法转为json格式数据
# 注意：默认会转为二进制数据，使用 ensure_ascii&#x3D;False 设置不转为二进制
json_data &#x3D; json.dumps(list, ensure_ascii&#x3D;False)
print (json_data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[[&quot;小王&quot;, 40,50], [&quot;小贾&quot;, 50,23]]</code></p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">json_data &#x3D; json.dumps(list)
print(json_data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[[&quot;\u5c0f\u738b&quot;, 40,50], [&quot;\u5c0f\u8d3e&quot;, 50,23]]</code></p>
</blockquote>
<p><code>json.dumps()</code> 也是默认编码成二进制数据。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 打印数据类型
print(type(list))
print(type(json_data))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>&lt;class ‘list’&gt;<br>&lt;class ‘str’&gt;</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">py_data &#x3D; json.loads(json_data)
print(type(py_data))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>&lt;class ‘list’&gt;</p>
</blockquote>
<h2 id="0x03-将数据存储为-XML-格式，并进行读取"><a href="#0x03-将数据存储为-XML-格式，并进行读取" class="headerlink" title="0x03 将数据存储为 XML 格式，并进行读取"></a>0x03 将数据存储为 XML 格式，并进行读取</h2><h3 id="XML-eXtensible-Markup-Language"><a href="#XML-eXtensible-Markup-Language" class="headerlink" title="XML (eXtensible Markup Language)"></a>XML (eXtensible Markup Language)</h3><p>主要目的是帮助信息系统交换结构化数据，是一种比 JSON 要老一些的技术。</p>
<p>XML 代码美化工具：Pretty Printer。</p>
<p>XML 是一种树状结构的数据。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203142247567.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203142248500.png"></p>
<p>And you can think of this as, like folders on your computer.</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203142255711.png"></p>
<p>我们要做的，就是从根节点，一层一层向下挖掘，找到这些数据。</p>
<p>更多信息可以参考：<br> <a href="https://www.w3school.com.cn/xml/index.asp">XML 教程</a><br> <a href="https://www.runoob.com/xml/xml-tutorial.html">XML 教程 | 菜鸟教程</a><br> <a href="https://www.coursera.org/learn/python-network-data/lecture/EaR2d/13-2-extensible-markup-language-xml">13.2 eXtensible Markup Language (XML) | Coursera</a></p>
<h3 id="1-存储为-XML-格式"><a href="#1-存储为-XML-格式" class="headerlink" title="1. 存储为 XML 格式"></a>1. 存储为 XML 格式</h3><p>首先将 List 转为字典：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">index &#x3D; [&#39;0&#39;, &#39;1&#39;] # 这里要用 str 型
d &#x3D; dict(zip(index, list))
print(d)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&#123;&#39;0&#39;: (&#39;小王&#39;, 40,50), &#39;1&#39;: (&#39;小贾&#39;, 50,23)&#125;</code></p>
</blockquote>
<p>然后定义一个方法将字典转为 XML。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from xml.etree.ElementTree import Element

# Turn a simple dict of key&#x2F;value pairs into XML
def dict_to_xml(tag, d):
    elem &#x3D; Element(tag)
    for key, val in d.items():
        child &#x3D; Element(key)
        child.text &#x3D; str(val)
        elem.append(child)
    return elem<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">e &#x3D; dict_to_xml(&#39;info&#39;, d)
print(e)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&lt;Element &#39;info&#39; at 0x0000018CA90C69A8&gt;</code></p>
</blockquote>
<p>或者可以使用 <code>pip3 install dicttoxml</code> 来转换。</p>
<h3 id="2-读取-XML-格式"><a href="#2-读取-XML-格式" class="headerlink" title="2. 读取 XML 格式"></a>2. 读取 XML 格式</h3><p>转换结果是一个 <code>Element</code> 实例。对于 I&#x2F;O 操作，使用 <code>xml.etree.ElementTree</code> 中的 <code>tostring()</code> 函数很容易就能将它转换成一个字节字符串。例如：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from xml.etree.ElementTree import tostring
tostring(e)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>b&quot;&lt;info&gt;&lt;0&gt;(&#39;&amp;#23567;&amp;#29579;&#39;, 40,50)&lt;/0&gt;&lt;1&gt;(&#39;&amp;#23567;&amp;#36158;&#39;, 50,23)&lt;/1&gt;&lt;/info&gt;&quot;</code></p>
</blockquote>
<p>目前的 <code>e</code> 就是一个树状结构的示例，探索一下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">print(e.tag)
print(e.text)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>info</code><br><code>None</code></p>
</blockquote>
<p>相当于目前在 <code>root</code> 节点。</p>
<p>把子节点打印出来：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for child in e:
    print(child.tag)
    print (child.text)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>0<br>(‘小王’, 40,50)<br>1<br>(‘小贾’, 50,23)</p>
</blockquote>
<p>或者也可以将节点读取出来后，存成字典或者列表。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">to_dict &#x3D; &#123;&#125;
to_list &#x3D; []
for child in e:
    to_dict[child.tag] &#x3D; child.text
    
to_list.append(to_dict)

print(to_dict)
print (to_list)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&#123;&#39;0&#39;: &quot;(&#39;小王&#39;, 40,50)&quot;, &#39;1&#39;: &quot;(&#39;小贾&#39;, 50,23)&quot;&#125;</code><br><code>[&#123;&#39;0&#39;: &quot;(&#39;小王&#39;, 40,50)&quot;, &#39;1&#39;: &quot;(&#39;小贾&#39;, 50,23)&quot;&#125;]</code></p>
</blockquote>
<p>也可以使用 <code>pip3 install xmltodict</code> 来转换并读取。</p>
<h2 id="0x04-计算机网络基础"><a href="#0x04-计算机网络基础" class="headerlink" title="0x04 计算机网络基础"></a>0x04 计算机网络基础</h2><p>学习 <a href="https://www.runoob.com/w3cnote/summary-of-network.html">计算机网络基础</a> ，思考从打开 <a href="https://coggle.club/">coggle.club</a> 到网页展示，有什么步骤？</p>
<h3 id="1-网络层次分层"><a href="#1-网络层次分层" class="headerlink" title="1. 网络层次分层"></a>1. 网络层次分层</h3><h4 id="OSI-模型"><a href="#OSI-模型" class="headerlink" title="OSI 模型"></a>OSI 模型</h4><ul>
<li><p>OSI 概念：</p>
<ul>
<li>为了使不同计算机厂家生产的计算机能够相互通信，以便在更大的范围内建立计算机网络，国际标准化组织（ISO）在 1978 年提出了”开放系统互联参考模型”，即著名的 OSI&#x2F;RM 模型（Open System Interconnection&#x2F;Reference Model）。“开放性”是指世界上任何地方、任何遵循 OSI 标准的系统，只要连接起来就能互相通信。</li>
<li>注意：OSI 是一种 <strong>模型</strong>，并不是实际投入使用的协议，而是用来了解和设计网络体系结构的。</li>
</ul>
</li>
<li><p>OSI 模型的目的：</p>
<ul>
<li>框架化；SOA。</li>
<li>规范不同系统的互联标准，使两个不同的系统能够较容易的通信，而不需要改变底层的硬件或者软件的逻辑。</li>
</ul>
</li>
<li><p>OSI 模型分为 7 层：</p>
<ul>
<li>将计算机网络体系结构的通信协议划分为七层，自下而上依次为：<ul>
<li>物理层（Physics Layer）</li>
<li>数据链路层（Data Link Layer）</li>
<li>网络层（Network Layer）</li>
<li>传输层（Transport Layer）</li>
<li>会话层（Session Layer）</li>
<li>表示层（Presentation Layer）</li>
<li>应用层（Application Layer）</li>
</ul>
</li>
<li>其中第四层完成数据传输服务，上面三层面向用户（ User Support Layers）。</li>
</ul>
</li>
</ul>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151239418.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151240138.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151240443.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151228935.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151249700.png"></p>
<h4 id="TCP-x2F-IP-协议"><a href="#TCP-x2F-IP-协议" class="headerlink" title="TCP&#x2F;IP 协议"></a>TCP&#x2F;IP 协议</h4><p>目前使用最多的是 TCP&#x2F;IP 四层协议。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151129079.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151301087.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151135178.png"></p>
<h4 id="输入-URL-后执行的全过程"><a href="#输入-URL-后执行的全过程" class="headerlink" title="输入 URL 后执行的全过程"></a>输入 URL 后执行的全过程</h4><p>从打开 <a href="https://coggle.club/">coggle.club</a> 到网页展示，有什么步骤？</p>
<ol>
<li>DNS 域名解析：客户端浏览器通过 DNS 解析得到 Coggle 网站的 IP 地址。</li>
<li>客户端与服务端建立 TCP 连接：TCP 三次握手。</li>
<li>客户端发起 HTTP 请求：建立 TCP 连接后，把客户端信息（携带 cookies）传递给服务端。</li>
<li>服务端响应 HTTP 请求。</li>
<li>释放 TCP 连接：TCP 四次挥手。</li>
<li>浏览器解析 HTML 代码，并请求 HTML 代码中的资源（如图片、音频、视频、CSS、JS 等等）。</li>
<li>浏览器对页面进行渲染呈现给用户。</li>
</ol>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>JSON 格式比较好理解，不多赘述。</p>
<p>XML 格式是一种树状结构的数据表示，数据的写入与读取都是基于这种结构的。所以首先要理解 XML 的这种内在结构，才能明白为什么 XML 的读写操作是这个样子的，否则直接照搬方法也很难清楚为什么这样做。我这里也只是浅尝辄止的了解基础概念，因为工作重心不在这，暂时不需要太深入的研究。</p>
<p>虽然 XML 的格式看起来跟 HTML 十分相似，但这两种格式是负责不一样的工作的。<strong>XML 被设计用来传输和存储数据。HTML 被设计用来显示数据。</strong> 而且 HTML 中使用的标签（以及 HTML 的结构）是预定义好的；而 <strong>XML 仅仅是纯文本</strong>，允许创作者定义自己的标签和自己的文档结构。也就是说是针对自己的应用自行设计标签与结构，使得能够读懂 XML 的应用程序可以有针对性地处理 XML 的标签。标签的功能性意义依赖于应用程序的特性。</p>
<p>之前学过的计算机网络基础大部分都还给老师了，只是模糊地记得一些概念，但要我去说每个层或者每种协议里面传输了哪些数据，数据分为哪几种，各自负责什么功能，或者计算一下 IP 已经是想不起来了。这次先复习一下 OSI 和 TCP&#x2F;IP 的基本结构。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>Json 读写：<br> <a href="https://www.cnblogs.com/tizer/p/11067098.html">Python 中如何将数据存储为 json 格式的文件</a><br> <a href="https://www.w3cschool.cn/article/30808038.html">Python json.dumps()函数使用解析 | w3c 笔记</a><br> <a href="https://cloud.tencent.com/developer/article/1181728">Python json 模块 dumps、dump、loads、load 的使用</a></p>
<p>XML 读写：<br><a href="https://www.w3school.com.cn/xml/xml_intro.asp">XML 简介</a><br> <a href="https://www.coursera.org/learn/python-network-data/lecture/EaR2d/13-2-extensible-markup-language-xml">13.2 eXtensible Markup Language (XML) | Coursera</a><br> <a href="https://zhmou.github.io/2021/10/12/Use-Python-to-Read-and-Write-XML-files/">使用 Python 读写 XML 文件 | Zh 某的备忘录</a><br> <a href="https://www.bilibili.com/video/BV1Sx411Q7zp?from=search&seid=13822148224876736274&spm_id_from=333.337.0.0">【一起学 Python·网络爬虫】XML 简介与解析 | bilibili</a><br> <a href="https://blog.csdn.net/Strive_0902/article/details/119856186">Python 列表转字典</a><br> <a href="https://blog.csdn.net/Jarry_cm/article/details/104925292">字典和列表相互转换</a><br> <a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p05_turning_dictionary_into_xml.html">6.5 将字典转换为 XML — python3-cookbook 3.0.0 文档</a><br> <a href="https://www.jianshu.com/p/ae932d11f059">使用两个三方库来读写</a></p>
<p>计算机网络基础：<br> <a href="https://www.bilibili.com/video/BV1FL4y1M7oM?p=17">OSI、TCP&#x2F;IP、计算机基础、操作系统 | bilibili</a><br> <a href="https://www.bilibili.com/video/BV1ZD4y1R7RT/?spm_id_from=333.788.recommend_more_video.0">计算机基础知识 | bilibili</a><br> <a href="https://www.jianshu.com/p/9b9438dff7a2">图解 OSI 七层模型</a><br> <a href="https://www.runoob.com/w3cnote/summary-of-network.html">计算机网络基础</a></p>
<p>输入 URL 后执行的全过程：<br> <a href="https://juejin.cn/post/6844903757877084174">这里的三次握手，四次挥手讲的很清楚。</a> ⭐⭐⭐<br> <a href="https://blog.csdn.net/qq_41837249/article/details/117001110">输入 URL 后的全部过程 会用到哪些协议？</a><br> <a href="https://blog.csdn.net/TTTZZZTTTZZZ/article/details/87214111">结合协议来讲，每个步骤在哪一层</a></p>
]]></content>
      <categories>
        <category>爬虫与网络编程基础</category>
      </categories>
      <tags>
        <tag>Coggle 30 Days of ML</tag>
        <tag>Spider</tag>
        <tag>Network Programming</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫与网络编程基础_Task02_HTTP协议与Requests</title>
    <url>/4NXBKD/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>Requests 是一个 Python 公认的，优秀的第三方网络爬虫库，可以自动爬取 HTML 页面，自动进行网络请求的提交。所以本次任务首先来了解，学习 Requests 库以及 HTTP 协议。另外，由于Requests 库与 HTTP 协议的方法是一一对应的，所以会穿插着学习，以便更容易理解。</p>
<span id="more"></span>

<h2 id="0x01-初探-Requests-库"><a href="#0x01-初探-Requests-库" class="headerlink" title="0x01 初探 Requests 库"></a>0x01 初探 Requests 库</h2><p> <a href="https://docs.python-requests.org/zh_CN/latest/">Requests: 让 HTTP 服务人类 — Requests 2.18.1 文档</a><br> <a href="https://pypi.org/project/requests/">requests · PyPI</a></p>
<h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><p>安装 <code>requests</code>。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install requests<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="2-Requests-库的七个主要方法"><a href="#2-Requests-库的七个主要方法" class="headerlink" title="2. Requests 库的七个主要方法"></a>2. Requests 库的七个主要方法</h3><table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>requests.request()</td>
<td>构造一个请求，支持以下各方法的基础方法</td>
</tr>
<tr>
<td>requests.get()</td>
<td>获取 HTML 网页的主要方法，对应于 HTTP 的 GET</td>
</tr>
<tr>
<td>requests.head()</td>
<td>获取 HTML 网页头信息的方法，对应于 HTTP 的 HEAD</td>
</tr>
<tr>
<td>requests.post()</td>
<td>向 HTML 网页提交 POST 请求的方法，对应于 HTTP 的 POST</td>
</tr>
<tr>
<td>requests.put()</td>
<td>向 HTML 网页提交 PUT 请求的方法，对应于 HTTP 的 PUT</td>
</tr>
<tr>
<td>requests.patch()</td>
<td>向 HTML 网页提交局部修改请求，对应于 HTTP 的 PATCH</td>
</tr>
<tr>
<td>requests.delete()</td>
<td>向 HTML 页面提交删除请求，对应于 HTTP 的 DELETE</td>
</tr>
</tbody></table>
<p>先来动动手：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
r &#x3D; requests.get(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;) # GET
r.status_code # 检测请求状态码<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>200</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">r.encoding &#x3D; &#39;utf-8&#39; # 修改内容编码
r.text[:500] # 查看响应内容的字符串<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&#39;&lt;!DOCTYPE html&gt;\r\n&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta content=always name=referrer&gt;&lt;link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&gt;&lt;title&gt;百度一下，你就知道&lt;/title&gt;&lt;/head&gt; &lt;body link=#0000cc&gt; &lt;div id=wrapper&gt; &lt;div id=head&gt; &lt;div class=head_wrapper&gt; &lt;div class=s_form&gt; &lt;div class=s_form_wrapper&gt; &lt;div id=lg&gt; &lt;img hidefocus=true src=//www.baidu.com/img/bd_&#39;</code></p>
</blockquote>
<p>下面仅先介绍最常使用的 <code>requests.get()</code> 方法，其余的方法待学习了 HTTP 协议之后，再对照着理解。</p>
<h4 id="requests-get-方法"><a href="#requests-get-方法" class="headerlink" title="requests.get() 方法"></a>requests.get() 方法</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200044542.png"></p>
<p>具体参数：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">r &#x3D; requests.get(url, params, **kwargs)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<ul>
<li><code>url</code> ：拟获取页面的 URL 链接。</li>
<li><code>params</code> ：URL 中的额外参数，字典或者字节流格式，可选。</li>
<li><code>**kwargs</code> : 12 个控制访问的参数</li>
</ul>
<p>通过 <code>requests.get()</code> 方法返回得到的 Response 对象具有以下属性：</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r.status_code</td>
<td>HTTP 请求的返回状态，若为 200 则表示请求成功</td>
</tr>
<tr>
<td>r.text</td>
<td>HTTP 响应内容的字符串形式，即，url 对应的页面内容</td>
</tr>
<tr>
<td>r.encoding</td>
<td>从 HTTP header 中猜测的相应内容编码方式</td>
</tr>
<tr>
<td>r.apparent_encoding</td>
<td>从内容中分析出的响应内容编码方式（备选编码方式）</td>
</tr>
<tr>
<td>r.content</td>
<td>HTTP 响应内容的二进制形式</td>
</tr>
</tbody></table>
<p>这五个属性是最常使用的五个属性。<br><code>r.status_code</code> 获得的状态码，只有返回 200 表示请求成功，其他则为失败，如 404。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200100765.png" alt="|600"></p>
<p>使用 <code>requests.get()</code> 方法获取信息的基本流程：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200102154.png" alt="|500"></p>
<p><strong>Case  通过 get 获取网页信息</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
r &#x3D; requests.get(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)
r.status_code # 返回 200，确认响应成功
r.text[-300:] # 查看响应内容<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&#39;out Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;&amp;copy;2017&amp;nbsp;Baidu&amp;nbsp;&lt;a href=http://www.baidu.com/duty/&gt;ä½¿ç\x94¨ç\x99¾åº¦å\x89\x8då¿\x85è¯»&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;æ\x84\x8fè§\x81å\x8f\x8dé¦\x88&lt;/a&gt;&amp;nbsp;äº¬ICPè¯\x81030173å\x8f·&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;\r\n&#39;</code></p>
</blockquote>
<p>可以发现，返回的内容中有很多乱码。那么来看一下内容的编码方式。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">r.encoding # 从 header 猜测编码<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>‘ISO-8859-1’</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">r.apparent_encoding # 备选编码<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>‘utf-8’</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">r.encoding &#x3D; &#39;utf-8&#39; # 使用备选编码替换当前编码方式
r.text[-300:] # 再次查看响应内容<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&#39; href=http://ir.baidu.com&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;&amp;copy;2017&amp;nbsp;Baidu&amp;nbsp;&lt;a href=http://www.baidu.com/duty/&gt;使用百度前必读&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;意见反馈&lt;/a&gt;&amp;nbsp;京 ICP 证 030173 号&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;\r\n&#39;</code></p>
</blockquote>
<p>经过替换编码方式，原来的一些乱码变成了中文。变得具有可读性了。为什么会这样？那就需要理解 Response 的编码。</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r.encoding</td>
<td>从 HTTP header 中猜测的相应内容编码方式</td>
</tr>
<tr>
<td>r.apparent_encoding</td>
<td>从内容中分析出的响应内容编码方式（备选编码方式）</td>
</tr>
</tbody></table>
<p><code>r.encoding</code>：如果 header 中不存在 charset 字段，则默认编码为 ISO-8859-1。<br>就是当我们刚刚访问百度的时候，默认的编码。但这个编码并不能解析中文，所以查看内容时，会出现乱码。</p>
<p><code>r.apparent_encoding</code>：根据网页内容分析出的编码方式。<br>为了避免上述情况，<code>Requests</code> 库还提供了另一种备选编码方案，从 HTTP 的内容部分（而不是头部份），去分析内容可能的编码形式。严格来说 <code>r.apparent_encoding</code> 比 <code>r.encoding</code> 更加准确，因为前者是实实在在去分析内容，找到可能的编码；而后者只是从 header 的相关字段中提取编码。所以当后者不能正确解码时，需要使用前者——备用编码，来解析返回的信息。这也是为什么示例中替换编码方式后，即可正确解析出中文的原因。</p>
<h3 id="3-爬取网页的通用代码框架"><a href="#3-爬取网页的通用代码框架" class="headerlink" title="3. 爬取网页的通用代码框架"></a>3. 爬取网页的通用代码框架</h3><p>注意 <code>Requests</code> 库有时会产生异常，比如网络连接错误、<code>HTTP</code> 错误异常、重定向异常、请求 <code>URL</code> 超时异常等等。所以我们需要判断 <code>r.status_codes</code> 是否为 200，在这里我们怎么样去捕捉异常呢？</p>
<p>这里我们可以利用 <code>r.raise_for_status()</code> 语句去捕捉异常，该语句在方法内部判断 <code>r.status_code</code> 是否等于 200，如果不等于，则抛出异常。</p>
<p>于是在这里我们有一个爬取网页的通用代码框架：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">try:
    r &#x3D; requests.get(url, timeout&#x3D;30) # 请求超时时间为 30 秒
    r.raise_for_status() # 如果状态不是 200，则引发 HTTPError 异常
    r.encoding &#x3D; r.apparent_encoding # 配置编码
    return r.text 
except:
    return &quot;产生异常&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200150745.png"></p>
<p>可见当 <code>url</code> 链接没有协议，产生错误时，抛出了异常。</p>
<p>通过这种框架，可以使我们的爬虫方法更加可靠，稳定。</p>
<h2 id="0x02-学习-HTTP-协议"><a href="#0x02-学习-HTTP-协议" class="headerlink" title="0x02 学习 HTTP 协议"></a>0x02 学习 HTTP 协议</h2><p>为了更好的理解 Requests 库中的这些方法，我们需要学习并了解 HTTP 协议。</p>
<h3 id="HTTP-协议简介"><a href="#HTTP-协议简介" class="headerlink" title="HTTP 协议简介"></a>HTTP 协议简介</h3><p>HTTP，<strong>H</strong>yper<strong>T</strong>ext <strong>T</strong>ransfer <strong>P</strong>rotocol，超文本传输协议。</p>
<p>HTTP 是万维网的数据通信的基础。</p>
<p>HTTP 是一种基于“请求与响应”模式的，无状态的应用层协议。</p>
<ul>
<li>“请求与响应”：用户发起请求，服务器做相关响应。</li>
<li>无状态：第一次请求与第二次请求之间，没有关联。并且对于发送过的请求或响应都不进行保存。</li>
<li>应用层协议：该协议基于 TCP 协议。</li>
</ul>
<h3 id="HTTP-工作原理"><a href="#HTTP-工作原理" class="headerlink" title="HTTP 工作原理"></a>HTTP 工作原理</h3><p>HTTP 协议定义 Web 客户端如何从 Web 服务器请求 Web 页面，以及服务器如何把 Web 页面传送给客户端。HTTP 协议采用了请求&#x2F;响应模型。客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。服务器以一个状态行作为响应，响应的内容包括协议的版本、成功或者错误代码、服务器信息、响应头部和响应数据。</p>
<p>以下是 HTTP 请求&#x2F;响应的步骤：</p>
<ol>
<li>客户端连接到 Web 服务器。</li>
<li>发送 HTTP 请求。</li>
<li>服务器接受请求并返回 HTTP 响应。</li>
<li>释放连接 TCP 连接。</li>
<li>客户端浏览器解析 HTML 内容。</li>
</ol>
<p>在浏览器地址栏键入 URL，按下回车之后会经历以下流程：</p>
<ol>
<li>浏览器向 DNS 服务器请求解析该 URL 中的域名所对应的 IP 地址。</li>
<li>解析出 IP 地址后，根据该 IP 地址和默认端口 80，和服务器建立 TCP 连接。</li>
<li>浏览器发出读取文件 (URL 中域名后面部分对应的文件) 的 HTTP 请求，该请求报文作为 TCP 三次握手的第三个报文的数据发送给服务器。</li>
<li>服务器对浏览器请求作出响应，并把对应的 HTML 文本发送给浏览器。</li>
<li>释放 TCP 连接。</li>
<li>浏览器将该 HTML 文本并显示内容。</li>
</ol>
<h3 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h3><p>URL，Universal Resource Locator，<strong>统一资源定位符</strong>。</p>
<p>HTTP 协议采用 URL 作为定位网络资源的标识。</p>
<p>URL 遵守一种标准的语法，它由协议、域名、端口、路径名称、查询字符串、以及锚部分这六个部分构成，其中端口可以省略，查询字符串和锚部分为参数。具体语法规则如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">protocol:&#x2F;&#x2F;host: port&#x2F;pathname?query#fragment<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>在上述语法规则中，<code>protocol</code> 表示协议，<code>host</code> 表示主机名（域名或 IP 地址），<code>port</code> 表示端口，<code>pathname</code> 表示路径名称，<code>query</code> 表示查询字符串，<code>fragment</code> 表示锚部分。<code>#</code> 代表网页中的一个位置，作为页面 <code>定位符</code> 出现在 URL 中。其右面的字符，就是该位置的 <code>标识符</code>（锚部分）。</p>
<p><strong>统一资源定位符</strong> 将从因特网获取信息的几种基本元素包含在一个简单的地址中：</p>
<ul>
<li>传输协议 - protocol<ul>
<li>如 http&#x2F;https&#x2F;ftp 等。</li>
</ul>
</li>
<li>服务器 - host<ul>
<li>通常为域名：因为 IP 地址不好记，因此用域名美化，可以理解为 IP 的别名。使用域名 -&gt; 通过 DNS 解析找到对应的 IP。</li>
<li>有时为 IP 地址：主机地址，每台电脑都有自己特定的标识，IPv4&#x2F;IPv6。</li>
</ul>
</li>
<li>端口号 - port （可省略）：可以理解为窗口，交流的端口<ul>
<li>http：默认打开 80 端口</li>
<li>https：默认打开 443 端口</li>
</ul>
</li>
<li>路径 - path：访问主机分享文件的地址<ul>
<li>以 <code>/</code> 字符区别路径中的每一个目录名称</li>
<li>文件路径：用户直接访问主机分享的文件</li>
<li>路由：目前比较流行的形式（后端监听地址访问事件，返回特定的内容）</li>
</ul>
</li>
<li>查询参数 - query：帮助用户访问到特定的资源<ul>
<li>格式 <code>?name=value&amp;name=value...</code></li>
<li>GET 模式的表单参数，以 <code>?</code> 字符为起点，每个参数以 <code>&amp;</code> 隔开，再以 <code>=</code> 分开参数名称与资料，通常以 UTF8 的 URL 编码，避开字符冲突的问题。</li>
</ul>
</li>
<li>锚点 - fragment：<ul>
<li><code>#fragment</code> 锚点，又叫命名锚记，是文档中的一种标记，网页设计者可以用它和 URL“锚”在一起，其作用像一个迅速定位器一样，可快速将访问者带到指定位置。</li>
<li>HTTP 请求不包括 <code>#</code> ：<code>#</code> 是用来指导浏览器动作的，对服务器端完全无用。</li>
</ul>
</li>
</ul>
<p>典型的统一资源定位符看上去是这样的：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">http:&#x2F;&#x2F;zh.wikipedia.org:80&#x2F;w&#x2F;index.php?title&#x3D;Special:%E9%9A%8F%E6%9C%BA%E9%A1%B5%E9%9D%A2&amp;printable&#x3D;yes<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>其中：</p>
<ol>
<li><code>http</code> 是协议；</li>
<li><code>zh.wikipedia.org</code> 是服务器；</li>
<li><code>80</code> 是服务器上的网络端口号；</li>
<li><code>/w/index.php</code> 是路径；</li>
<li><code>?title=Special:%E9%9A%8F%E6%9C%BA%E9%A1%B5%E9%9D%A2&amp;printable=yes</code> 是查询参数。</li>
</ol>
<p>大多数网页浏览器不要求用户输入网页中 <code>http://</code> 的部分，因为绝大多数网页内容是超文本传输协议文件。同样，<code>80</code> 是超文本传输协议文件的常用端口号，因此一般也不必写明。一般来说用户只要键入统一资源定位符的一部分就可以了。<strong>这只是浏览器中可以省略，写爬虫的时候，URL 中的传输协议是不可以省略的。</strong></p>
<p><strong>总结</strong>：URL 是通过 HTTP 协议存取资源的 Internet 路径，一个 URL 对应一个数据资源。</p>
<h3 id="HTTP-协议对资源的操作"><a href="#HTTP-协议对资源的操作" class="headerlink" title="HTTP 协议对资源的操作"></a>HTTP 协议对资源的操作</h3><table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>GET</td>
<td>请求获取 URL 位置的资源</td>
</tr>
<tr>
<td>HEAD</td>
<td>请求获取 URL 位置资源的响应消息报告，即获取该资源的头部信息</td>
</tr>
<tr>
<td>POST</td>
<td>请求向 URL 位置的资源后附加新的数据</td>
</tr>
<tr>
<td>PUT</td>
<td>请求向 URL 位置存储一个资源，覆盖原 URL 位置的资源</td>
</tr>
<tr>
<td>PATCH</td>
<td>请求局部更新 URL 位置的资源，即改变该处资源的部分内容</td>
</tr>
<tr>
<td>DELETE</td>
<td>请求删除 URL 位置存储的资源</td>
</tr>
</tbody></table>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203201707788.png"></p>
<p><strong>GET 与 POST 的区别</strong>：</p>
<ul>
<li>原理区别：GET 请求获取资源，不会产生动作；POST 可能会修改服务器上的资源。因此 POST 用于修改和写入数据，GET 一般用于搜索排序和筛选之类的操作，目的是资源的获取，读取数据。</li>
<li>参数位置区别：GET 把参数包含在 URL 中，POST 通过 Request body 传递参数。所以 POST 可以发送的数据更大，GET 有 URL 长度限制；POST 能发送更多的数据类型（GET 只能发送 ASCII 字符）。</li>
<li>POST 更加安全：不会作为 URL 的一部分，不会被缓存、保存在服务器日志、以及浏览器浏览记录中。</li>
<li>POST 比 GET 慢：POST 在真正接收数据之前会先将请求头发送给服务器进行确认，然后才真正发送数据。<ul>
<li>浏览器请求 TCP 连接（第一次握手）</li>
<li>服务器答应进行 TCP 连接（第二次握手）</li>
<li>浏览器确认，并发送 POST 请求头（第三次握手，这个报文比较小，所以 HTTP 会在此时进行第一次数据发送）</li>
<li>服务器返回 100 Continue 响应</li>
<li>浏览器发送数据</li>
<li>服务器返回 200 OK 响应</li>
</ul>
</li>
</ul>
<p><strong>PATCH 与 PUT 的区别</strong>：<br>假设 URL 位置有一组数据 UserInfo，包括 UserID，UserName 等 20 个字段。<br>需求：修改 UserName，其他不变。</p>
<ul>
<li>采用 PATCH，仅向 URL 提交 UserName 的局部更新请求。</li>
<li>采用 PUT，必须将所有 20 个字段一并提交到 URL，未提交的字段将被删除。</li>
</ul>
<p>因此 PATCH 的最主要好处是：节省网络带宽。</p>
<h2 id="0x03-再探-Requests-库"><a href="#0x03-再探-Requests-库" class="headerlink" title="0x03 再探 Requests 库"></a>0x03 再探 Requests 库</h2><p>HTTP 协议方法与 Requests 库方法是一一对应的。因此有了对 HTTP 协议方法的理解之后，我们再来学习 Requests 库中的其他方法。</p>
<h3 id="Requests-库中的其他方法"><a href="#Requests-库中的其他方法" class="headerlink" title="Requests 库中的其他方法"></a>Requests 库中的其他方法</h3><h4 id="request-head-方法"><a href="#request-head-方法" class="headerlink" title="request.head() 方法"></a>request.head() 方法</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; r &#x3D; requests.head(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;)
&gt;&gt;&gt; r.headers
&#123;&#39;Date&#39;: &#39;Sat, 19 Mar 2022 18:27:08 GMT&#39;, &#39;Content-Type&#39;: &#39;application&#x2F;json&#39;, &#39;Content-Length&#39;: &#39;307&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Server&#39;: &#39;gunicorn&#x2F;19.9.0&#39;, &#39;Access-Control-Allow-Origin&#39;: &#39;*&#39;, &#39;Access-Control-Allow-Credentials&#39;: &#39;true&#39;&#125;
&gt;&gt;&gt; r.text
&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到 <code>r.text</code> 为空，因此通过 <code>head()</code> 方法可以用很少的网络流量，来获得网页的概括信息。</p>
<h4 id="requests-post-方法"><a href="#requests-post-方法" class="headerlink" title="requests.post () 方法"></a>requests.post () 方法</h4><ol>
<li>向 URL POST 一个字典，自动编码为 form。</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; payload &#x3D; &#123;&quot;key1&quot;: &quot;value1&quot;,&quot;key2&quot;: &quot;value2&quot;&#125;
&gt;&gt;&gt; r &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, data &#x3D; payload)
&gt;&gt;&gt; print(r.text)
&#123; ...
  &quot;form&quot;: &#123;
    &quot;key1&quot;: &quot;value1&quot;, 
    &quot;key2&quot;: &quot;value2&quot;
  &#125;, 
...
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可见，当向 URL POST 一个字典或者键值对的时候，会默认存储在表单 form 的字段下。</p>
<ol start="2">
<li>向 URL POST 一个字符串，自动编码为 data。</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt;r &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, data &#x3D; &#39;juanbudongle&#39;)
&gt;&gt;&gt;print(r.text)
&#123; ...
  &quot;data&quot;: &quot;juanbudongle&quot;,
  &quot;files&quot;: &#123;&#125;,
  &quot;form&quot;: &#123;&#125;
&#125;
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="requests-put-方法"><a href="#requests-put-方法" class="headerlink" title="requests.put() 方法"></a>requests.put() 方法</h4><p>与 POST 类似，只是会覆盖原 URL 位置的资源</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; payload &#x3D; &#123;&quot;key1&quot;: &quot;value1&quot;,&quot;key2&quot;: &quot;value2&quot;&#125;
&gt;&gt;&gt; r &#x3D; requests.put(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, data &#x3D; payload)
&gt;&gt;&gt; print(r.text)
&#123; ...
  &quot;form&quot;: &#123;
    &quot;key1&quot;: &quot;value1&quot;, 
    &quot;key2&quot;: &quot;value2&quot;
  &#125;, 
...
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>requests.post ()</code> 与 <code>requests.put()</code> 类似，上节末尾已经分析过两种方法的区别。</p>
<h3 id="Requests-库的基础方法"><a href="#Requests-库的基础方法" class="headerlink" title="Requests 库的基础方法"></a>Requests 库的基础方法</h3><p><code>requests.request()</code> 是 Requests 库所有方法的基础方法，因此将其单列出来。或者说，库中其他方法的实现，都是基于 <code>requests.request()</code> 方法。</p>
<h4 id="requests-request-方法"><a href="#requests-request-方法" class="headerlink" title="requests. request () 方法"></a>requests. request () 方法</h4><p><code>requests.request(method, url, **kwargs)</code></p>
<ul>
<li><code>method</code> ：请求方式，对应 GET&#x2F;PUT&#x2F;POST 等 7 种。</li>
<li><code>url</code> ：拟获取页面的 URL 链接。</li>
<li><code>**kwargs</code> : 控制访问的参数，共 13 个。</li>
</ul>
<p>7 种请求方式如下：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200326757.png" alt="|500"></p>
<p><code>**kwargs</code> 控制访问的参数，一共有 13 个，均为可选项，下面依次讲解：</p>
<ul>
<li>params：字典或字节序列，作为参数增加到 <code>url</code> 中。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200336254.png" alt="|700"></li>
<li>data：字典，字节序或文件对象，作为 Request 的内容。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200340659.png" alt="|700"><br>提交的键值对不直接存在 <code>url</code> 链接中，而是放在 Request 规定的存储位置。参考 <a href="#requests%20post%20%E6%96%B9%E6%B3%95">requests post 方法</a> 。</li>
<li>json：json 格式的数据，作为 Request 的内容。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200347742.png" alt="|700"></li>
<li>headers：字典，HTTP 定制头。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200349208.png"></li>
<li>cookies：字典或 CookieJar，Request 中的 cookie。</li>
<li>auth：元组，支持 HTTP 认证功能。</li>
<li>files：字典类型，传输文件。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200351450.png" alt="|700"></li>
<li>timeout: 设定超时时间，秒为单位。</li>
<li>proxies：字典类型，设定访问代理服务器，可以增加登录认证。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200354523.png"><br>使用这个字段，可以有效隐藏用户爬取网页的源的 IP 地址信息，有效防止对爬虫的逆追踪。</li>
<li>allow_redirects: True&#x2F;False，默认为 True，重定向开关。</li>
<li>stream：True&#x2F;False，默认为 True，获取内容立即下载开关。</li>
<li>verify：True&#x2F;False，默认为 True，认证 SSL 证书开关。</li>
<li>cert： 本地 SSL 证书路径。</li>
</ul>
<h2 id="0x04-Requests-实战"><a href="#0x04-Requests-实战" class="headerlink" title="0x04 Requests 实战"></a>0x04 Requests 实战</h2><h3 id="Case-1-京东商品页面的爬取"><a href="#Case-1-京东商品页面的爬取" class="headerlink" title="Case 1. 京东商品页面的爬取"></a>Case 1. 京东商品页面的爬取</h3><p>对这个 <a href="https://item.jd.com/100029199502.html">手机商品页面</a> 进行爬取。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; import requests
&gt;&gt;&gt; url &#x3D; &#39;https:&#x2F;&#x2F;item.jd.com&#x2F;100029199502.html&#39;
&gt;&gt;&gt; r &#x3D; requests.get(url)
&gt;&gt;&gt; r.status_code
200
&gt;&gt;&gt; r.encoding
&#39;UTF-8&#39;
&gt;&gt;&gt; r.text
&#123;&#39;Date&#39;: &#39;Sat, 19 Mar 2022 18:27:08 GMT&#39;, &#39;Content-Type&#39;: &#39;application&#x2F;json&#39;, &#39;Content-Length&#39;: &#39;307&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Server&#39;: &#39;gunicorn&#x2F;19.9.0&#39;, &#39;Access-Control-Allow-Origin&#39;: &#39;*&#39;, &#39;Access-Control-Allow-Credentials&#39;: &#39;true&#39;&#125;
&gt;&gt;&gt; r.text
&quot;&lt;script&gt;window.location.href&#x3D;&#39;https:&#x2F;&#x2F;passport.jd.com&#x2F;new&#x2F;login.aspx?ReturnUrl&#x3D;http%3A%2F%2Fitem.jd.com%2F100029199502.html&#39;&lt;&#x2F;script&gt;&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>以上都是基本流程操作，我们发现返回的信息并不是商品页面。如果将这个返回的 URL 在浏览器中打开，会呈现一个登录页面。或者说京东阻止了这个爬虫，把我们重定向到了登陆页面。因为我们即使不登陆，在浏览器上还是可以访问的。</p>
<p>一般网页限制爬虫的方式有两种，一种是通过 Robots 协议，告知爬虫，哪些资源可以访问，哪些不可以；另一种是根据 HTTP 的 Header 中的 user-agent 来检测，判断这个 HTTP 访问是不是由爬虫发起的，对于爬虫的请求，网站是可以选择拒绝的。</p>
<p>由于 <code>Requests</code> 库返回的 <code>Response</code> 对象（也就是 <code>requests.get()</code> 返回的 <code>r</code> ），包含发出的 <code>request</code> 请求，因此我们可以查看之前对京东发出的请求到底是什么内容。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; r.request.headers  # 查看发出的头信息
&#123;&#39;User-Agent&#39;: &#39;python-requests&#x2F;2.27.1&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*&#x2F;*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;&#125;
# 可以发现，这里标明了我们是使用 &#39;python-requests&#x2F;2.27.1&#39; 发出的请求
# 也就是说我们的爬虫诚实地告诉了服务器，这个请求是由一个爬虫产生的
# 因此，如果京东进行来源审查，并且不支持这种访问的时候，我们的访问会被拒绝
# 那么此时，我们就应该模拟浏览器来进行爬虫请求
&gt;&gt;&gt; kv &#x3D; &#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;7.0&#39;&#125;  # 构造一个键值对，指定 user-agent
&gt;&gt;&gt; url &#x3D; &#39;https:&#x2F;&#x2F;item.jd.com&#x2F;100029199502.html&#39;
&gt;&gt;&gt; r &#x3D; requests.get(url, headers &#x3D; kv)  # 修改 HTTP 头中的 user-agent
&gt;&gt;&gt; r.status_code
200
&gt;&gt;&gt; r.request.headers  # 查看发出请求的头信息
&#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;7.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*&#x2F;*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;&#125;
# 可以看到 user-agent 已经修改为我们键值对中的内容
&gt;&gt;&gt; r.text[:600]  # 查看返回内容
&#39;&lt;!DOCTYPE HTML&gt;\n&lt;html lang&#x3D;&quot;zh-CN&quot;&gt;\n&lt;head&gt;\n    &lt;!-- shouji --&gt;\n    &lt;meta http-equiv&#x3D;&quot;Content-Type&quot; content&#x3D;&quot;text&#x2F;html; charset&#x3D;utf-8&quot; &#x2F;&gt;\n    &lt;title&gt;【华为Mate X2】华为 HUAWEI Mate X2 5G全网通12GB+512GB墨黑素皮款 典藏版 麒麟芯片 超感知徕卡四摄 折叠屏 华为手机【行情 报价 价格 评测】-京东&lt;&#x2F;title&gt;\n    &lt;meta name&#x3D;&quot;keywords&quot; content&#x3D;&quot;HUAWEIMate X2,华为Mate X2,华为Mate X2报价,HUAWEIMate X2报价&quot;&#x2F;&gt;\n    &lt;meta name&#x3D;&quot;description&quot; content&#x3D;&quot;【华为Mate X2】京东JD.COM提供华为Mate X2正品行货，并包括HUAWEIMate X2网购指南，以及华为Mate X2图片、Mate X2参数、Mate X2评论、Mate X2心得、Mate X2技巧等信息，网购华为Mate X2上京东,放心又轻松&quot; &#x2F;&gt;\n    &lt;meta name&#x3D;&quot;format-detection&quot; content&#x3D;&quot;telephone&#x3D;no&quot;&gt;\n    &lt;meta http-equiv&#x3D;&quot;mobile&#39;
# 正常返回商品页面<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>以上是在 Python IDLE 中的测试，下面给出全代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
url &#x3D; &#39;https:&#x2F;&#x2F;item.jd.com&#x2F;100029199502.html&#39;
try:
    kv &#x3D; &#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;7.0&#39;&#125;
    r &#x3D; requests.get(url, headers &#x3D; kv)
    r.raise_for_status()
    r.encoding &#x3D; r.apparent_encoding
    print(r.text[:1000])
except:
    print(&quot;Error&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>总结：有的时候需要修改头信息 ‘user-agent’，模拟浏览器向服务器提交 HTTP 请求。</strong></p>
<h3 id="Case-2-百度搜索结果爬取"><a href="#Case-2-百度搜索结果爬取" class="headerlink" title="Case 2. 百度搜索结果爬取"></a>Case 2. 百度搜索结果爬取</h3><p>百度的搜索关键词接口：<br><code>https://www.baidu.com/s?wd=keyword</code></p>
<p>这里要注意的是，简单的给 HTTP 头信息进行修改是没用的，需要去找一个真实的 User-Agent 修改到头信息中。</p>
<p>在浏览器中按下 F12：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203201605217.png" alt="|800"></p>
<p>随便选取一个 GET 请求，复制请求标头里的用户代理 User-Agent 信息。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; agent &#x3D; &#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko&#x2F;20100101 Firefox&#x2F;84.0&#39;&#125;
&gt;&gt;&gt; kv &#x3D; &#123;&#39;wd&#39;: &#39;Python&#39;&#125;
&gt;&gt;&gt; r &#x3D; requests.get (&#39; https:&#x2F;&#x2F;www.baidu.com&#x2F;s &#39;, params&#x3D;kv, headers&#x3D;agent)
&gt;&gt;&gt; r.request.url  # 查看发出的 HTTP 请求的 URL
&#39;https:&#x2F;&#x2F;www.baidu.com&#x2F;s?wd&#x3D;Python&#39;
# 可以看到，搜素关键词添加到了后面
&gt;&gt;&gt; r.request.headers
&#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko&#x2F;20100101 Firefox&#x2F;84.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*&#x2F;*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;&#125;
&gt;&gt;&gt; r.status_code
200
&gt;&gt;&gt; len(r.text)  # 查看返回内容的长度
786376
# 这就说明返回成功了
# 修改 User-Agent 不正确时，返回的长度只有 227，即代表被百度服务器阻止访问了<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>总结：在实际应用时，可能会涉及各种伪装技巧，反反爬也是要考虑的一方面。</strong></p>
<h3 id="Case-3-网络图片的爬取和存储"><a href="#Case-3-网络图片的爬取和存储" class="headerlink" title="Case 3. 网络图片的爬取和存储"></a>Case 3. 网络图片的爬取和存储</h3><p>在国家地理的网站上随便找 <a href="http://www.ngchina.com.cn/news/detail?newsId=47344">一幅图</a>，通过查看网页源码找到这幅图的 URL。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; import requests
&gt;&gt;&gt; path &#x3D; &quot;city.jpg&quot;  # 给图片一个存储位置，存在当前目录下，命名为 city
&gt;&gt;&gt; url &#x3D; &#39;https:&#x2F;&#x2F;ngimages.oss-cn-beijing.aliyuncs.com&#x2F;2022&#x2F;03&#x2F;16&#x2F;fabe20d0-01cc-400d-ae07-8f56516a556a.jpg&#39;  # 源码中找到的图的 URL
&gt;&gt;&gt; r &#x3D; requests.get(url)
&gt;&gt;&gt; r.status_code
200  # 成功
&gt;&gt;&gt; with open(path, &#39;wb&#39;) as f:  # 将图片以二进制形式写入文件
	    f.write(r.content)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>完整代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
import os

url &#x3D; &#39;https:&#x2F;&#x2F;ngimages.oss-cn-beijing.aliyuncs.com&#x2F;2022&#x2F;03&#x2F;16&#x2F;fabe20d0-01cc-400d-ae07-8f56516a556a.jpg&#39;
root &#x3D; &quot;E:&#x2F;&#x2F;pics&#x2F;&#x2F;&quot;
path &#x3D; root + url.split(&#39;&#x2F;&#39;)[-1]  # .spilt 返回⼀个将字符串以 &#x2F; 划分的列表，取 [-1]，即最后一个（或者说倒数第一个），也就是 URL 中图片的名字
try:
    kv &#x3D; &#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;5.0&#39;&#125;
    if not os.path.exists(root):  # 如果根目录不存在，则新建根目录
        os.mkdir(root)
    if not os.path.exists(path):  # 如果文件不存在，则从网上爬取并存储
        r &#x3D; requests.get(url)
        with open(path, &#39;wb&#39;) as f:
            f.write(r.content)
            print(&quot;文件保存成功&quot;)
    else:
        print(&quot;文件保存成功&quot;)
except:
    print(&quot;爬取失败&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>总结：在工程要求上，代码的可靠性与稳定性是非常重要的。因此哪怕是简单的代码，也要考虑可能出现的问题，并对这些问题做相关处理。</strong></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p> <a href="https://www.cnblogs.com/an-wen/p/11180076.html">学习 HTTP 协议</a> ⭐⭐⭐<br> <a href="http://www.dedenotes.com/html/url-fragment.html">URL 锚点、fragment 锚部分、锚点定位的九点知识 </a> ⭐⭐<br> <a href="https://blog.csdn.net/caryee89/article/details/7398088">统一资源定位符 URL</a><br> <a href="https://blog.csdn.net/Selina_lxh/article/details/121840190">URL(统一资源定位符)各部分详解</a><br> <a href="https://www.bilibili.com/video/BV1pt41137qK?p=7">Python 爬虫视频教程全集 | bilibili</a> ⭐⭐⭐<br> <a href="https://blog.csdn.net/qq_29339467/article/details/105342399">Python 爬虫教程中转站</a><br> <a href="https://blog.csdn.net/pittpakk/article/details/81218566">python3 requests 详解</a><br> <a href="https://zhuanlan.zhihu.com/p/275695831">http 请求中 get 和 post 方法的区别</a><br> <a href="https://zhuanlan.zhihu.com/p/370768951">python 爬虫-百度搜索结果爬取</a></p>
]]></content>
      <categories>
        <category>爬虫与网络编程基础</category>
      </categories>
      <tags>
        <tag>Coggle 30 Days of ML</tag>
        <tag>Spider</tag>
        <tag>Network Programming</tag>
      </tags>
  </entry>
  <entry>
    <title>搭建Hexo博客&amp;在GitHub+Gitee双重部署</title>
    <url>/2RYYQJ0/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>大致记录了如何快速搭建一个简单的个人博客，并且部署到 GitHub 与 Gitee。中间也碰到了一些坑，记录一下，帮助后面可能会碰到这些问题的同学。</p>
<span id="more"></span>

<h2 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1. 环境准备"></a>1. 环境准备</h2><p>这里使用的是 WSL2 的 Ubuntu18.04 系统<br>首先设置代理，使得可以使用 windows 的 clash 代理。详见 <a href="wsl2%20%E5%AD%90%E7%B3%BB%E7%BB%9F%E4%BB%A3%E7%90%86.md">wsl2 子系统代理</a></p>
<h3 id="安装-node-js"><a href="#安装-node-js" class="headerlink" title="- 安装 node.js"></a>- 安装 node.js</h3><p><a href="https://docs.microsoft.com/zh-cn/windows/dev-environment/javascript/nodejs-on-wsl">在 WSL 2 上设置 Node.js | Microsoft Docs</a><br><a href="https://hexo.io/zh-cn/docs/#%E5%AE%89%E8%A3%85-Hexo">文档 | Hexo</a><br><a href="https://dkvirus.gitbooks.io/-npm/content/di-sanzhang-npm-chuang-jian-xiang-mu/31-npm-init-shi-yong.html">3.1 npm init 使用 · 通俗易懂的 npm 入门教程</a></p>
<p>我使用的是微软子系统官方文档里的方法安装 <code>nvm</code>，照着做就行。</p>
<h3 id="安装-git"><a href="#安装-git" class="headerlink" title="- 安装 git"></a>- 安装 git</h3><p><a href="https://git-scm.com/downloads">Git - Downloads</a></p>
<h3 id="安装-Hexo"><a href="#安装-Hexo" class="headerlink" title="- 安装 Hexo"></a>- 安装 Hexo</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">npm install -g hexo-cli

# 验证安装成功
hexo -v<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Tips："><a href="#Tips：" class="headerlink" title="Tips："></a>Tips：</h4><p>这里有个坑，<code>nvm</code> 是用来管理 <code>node.js</code> 版本的。但是当我用 <code>nvm</code> 下载了另一个更新版本的 <code>node.js</code> 并且切换之后，<code>hexo</code> 这个命令就找不到了（应该是因为当时 <code>hexo</code> 安装在旧版的 <code>node.js</code> ），切回之前使用的版本才找得到。所以安装后尽量就不要换 <code>node.js</code> 的版本了。</p>
<p>比如我安装了一个新版的，之前有一个老版的，如下：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">➜  inno:[&#x2F;mnt&#x2F;w&#x2F;github_blog] nvm ls
-&gt;     v16.13.2
       v16.14.0
default -&gt; v16.13.2
iojs -&gt; N&#x2F;A (default)
unstable -&gt; N&#x2F;A (default)
node -&gt; stable (-&gt; v16.14.0) (default)
stable -&gt; 16.14 (-&gt; v16.14.0) (default)
lts&#x2F;* -&gt; lts&#x2F;gallium (-&gt; v16.14.0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>安装新版之后，每次默认启动的都是新版的，所以 <code>hexo</code> 是找不到的，要切回之前版本：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">nvm use v16.13.2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>切回后可以使用。</p>
<p>为了避免每次都要切换  <code>node.js</code>，更改默认版本：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">nvm alias default v16.13.2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>



<h2 id="2-搭建仓库"><a href="#2-搭建仓库" class="headerlink" title="2. 搭建仓库"></a>2. 搭建仓库</h2><h3 id="生成-SSH-Keys"><a href="#生成-SSH-Keys" class="headerlink" title="- 生成 SSH Keys"></a>- 生成 SSH Keys</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ssh-keygen -t rsa -C &quot;你的邮箱账号&quot; # 不带双引号<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>全部默认，四次回车，显示以下则建立成功。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Generating public&#x2F;private rsa key pair.
Enter file in which to save the key (&#x2F;home&#x2F;inno&#x2F;.ssh&#x2F;id_rsa):
Created directory &#39;&#x2F;home&#x2F;inno&#x2F;.ssh&#39;.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in &#x2F;home&#x2F;inno&#x2F;.ssh&#x2F;id_rsa.
Your public key has been saved in &#x2F;home&#x2F;inno&#x2F;.ssh&#x2F;id_rsa.pub.
The key fingerprint is:
SHA256:llz0nPfP************Ua0&#x2F;as4********I9g4 jh.hour@gmail.com
The key&#39;s randomart image is:
+---[RSA 2048]----+
|         .&#x3D;      |
|         *****   |
|        &#x3D; * B .  |
|       ***** + . |
|        E *.o . o|
|       . * *o. +o|
|          O. +o o|
|         *****+ .|
|        ..S**+.o |
+----[SHA256]-----+<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>查看 ssh 密钥并且复制到 Github：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd ~&#x2F;.ssh

ll
# 目录文件如下
	drwx------ 2 inno inno 4096 Jan 24 16:44 .
	drwxr-xr-x 9 inno inno 4096 Jan 24 16:49 ..
	-rw------- 1 inno inno 1675 Jan 24 16:44 id_rsa
	-rw-r--r-- 1 inno inno  399 Jan 24 16:44 id_rsa.pub

# vim 打开 id_rsa.pub 文件
vim  id_rsa.pub

# 全选复制 然后粘贴到 GitHub 的 [SSH keys](https:&#x2F;&#x2F;github.com&#x2F;settings&#x2F;keys) &#x2F; Add new<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>检查 ssh 是否绑定成功：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ssh -T git@github.com<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>Success：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">The authenticity of host &#39;github.com (20.205.243.166)&#39; can&#39;t be established.
ECDSA key fingerprint is SHA256:p2QAM********trVc98&#x2F;R1*****3&#x2F;L*******M.
Are you sure you want to continue connecting (yes&#x2F;no)? yes

Warning: Permanently added &#39;github.com,20.205.243.166&#39; (ECDSA) to the list of known hosts.
Hi 1nnoh! You&#39;ve successfully authenticated, but GitHub does not provide shell access.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="新建仓库"><a href="#新建仓库" class="headerlink" title="- 新建仓库"></a>- 新建仓库</h3><h4 id="Github"><a href="#Github" class="headerlink" title="- Github"></a>- Github</h4><p>在 GitHub 新建一个仓库（public），命名如下：</p>
<ul>
<li>我的仓库地址是: <code>1nnoh/1nnoh.github.io</code></li>
<li>仓库命名规则为 前面是用户名（GitHub用户名，点击右上角头像即可看到: signed as xxx），后面为 <code>.github.io</code></li>
</ul>
<p>更详细的内容可以看 <a href="#Reference">Reference</a> 里初级建站那个视频。</p>
<h4 id="Gitee"><a href="#Gitee" class="headerlink" title="- Gitee"></a>- Gitee</h4><p>在 Gitee 新建仓库（开源或私有都可以），命名如下：</p>
<ul>
<li>仓库名称：可以随便填，比如 <code>blog</code></li>
<li>仓库路径：填入用户名，跟 GitHub 一样，只是后面不用加 <code>.github.io</code>。至于为什么这样设置，可以自己试试其他的路径，然后看看 服务选项中 Gitee Pages 服务 生成的网站地址的区别。</li>
<li>开启 Gitee Pages 服务：进入仓库选择 服务，选择 Gitee Pages。第一次使用需要身份验证，一般半天可以通过人工审核。选取 强制使用 HTTPS，然后生成网站。</li>
<li>Tips：Github 的 Pages 服务可以自动更新，但Gitee 的 Pages 服务不能自动更新（开启的话需要付费）。所以每次从本地部署网站到线上之后，需要在 Gitee Pages 服务 中更新网站。</li>
</ul>
<h2 id="3-本地生成博客内容"><a href="#3-本地生成博客内容" class="headerlink" title="3. 本地生成博客内容"></a>3. 本地生成博客内容</h2><p>进入想要生成博客内容的目录，然后生成：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 初始化 hexo 博客
mkdir hexo-blog
hexo init

# Output
	INFO  Cloning hexo-starter https:&#x2F;&#x2F;github.com&#x2F;hexojs&#x2F;hexo-starter.git
	INFO  Install dependencies
	INFO  Start blogging with Hexo!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>随后本地生成 hexo 界面：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo s # hexo server 启动本地服务器

# Output 点击链接可以进入
# INFO  Validating config
# INFO  Start processing
# INFO  Hexo is running at http:&#x2F;&#x2F;localhost:4000&#x2F; . Press Ctrl+C to stop.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Press Ctrl+C 点击生成的本地地址即可访问初步建好的 Hexo博客。或者将 <code>http://localhost:4000/</code> 复制到浏览器地址栏打开。</p>
<h2 id="4-发布到-GitHub-与-Gitee"><a href="#4-发布到-GitHub-与-Gitee" class="headerlink" title="4. 发布到 GitHub 与 Gitee"></a>4. 发布到 GitHub 与 Gitee</h2><h3 id="修改-config-yml"><a href="#修改-config-yml" class="headerlink" title="- 修改  _config.yml"></a>- 修改  _config.yml</h3><p>进入上节生成博客的目录，打开 _config.yml  文件（或者在上节中生成博客的目录中找到该文件，用记事本或 VS Code 等代码编辑器打开）：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">vim _config.yml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>修改文档末尾的 deploy 设置：</p>
<pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"># 只配置 Github
# Deployment
# Docs: https:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;one-command-deployment
deploy:
  type: git
  repository: https:&#x2F;&#x2F;github.com&#x2F;1nnoh&#x2F;1nnoh.github.io.git
  branch: main # GitHub 的分支改成 main 了， Gitee 是 master

# Github 与 Gitee 一起部署
deploy:
 type: git
 repository:
	 gitee: git@gitee.com:innoh&#x2F;innoh.git,master
	 github: https:&#x2F;&#x2F;github.com&#x2F;1nnoh&#x2F;1nnoh.github.io.git,main<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Tips：</p>
<ul>
<li>repository 是 GitHub 上建立的博客仓库</li>
<li>进入仓库后，点击 Code 即可看到 https 地址（就是 clone 的地方）。</li>
</ul>
<p>更改完成后保存。</p>
<h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="- 安装依赖"></a>- 安装依赖</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">npm install hexo-deployer-git --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>部署：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># hexo generate
# hexo deploy 部署到服务器
hexo clean &amp;&amp; hexo g &amp;&amp; hexo d

# 输入 GitHub用户名 与 token（是用户名，不是邮箱；是token，不是登录密码！）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>重点！！！：输入密码的时候，去 GitHub 生成 token，把 token 作为部署时登录密码</strong></p>
<p>&#x3D;&#x3D;<strong>以后每次推送的时候，如果需要输入密码，记得也是输入令牌！！！</strong>&#x3D;&#x3D;</p>
<p>进入 GitHub 的 Settings &#x2F; Developer settings: 选择 Personal access tokens</p>
<p>生成 tokens，权限全选。生成令牌后复制，并粘贴输入密码。</p>
<p><strong>token 备份</strong></p>
<pre class="line-numbers language-none"><code class="language-none">ghp_DIvVJ*******************83i<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<p>&#x3D;&#x3D;<strong>这个 token令牌 是有一个有效期的，默认为 30 天，所以每三十天记得更新一下令牌权限，否则推送的时候会出现错误！！！另外，每次更新之后，令牌也会变，记得重新复制保存！</strong>&#x3D;&#x3D;</p>
<h3 id="检验发布"><a href="#检验发布" class="headerlink" title="- 检验发布"></a>- 检验发布</h3><p>到博客仓库 1nnoh &#x2F; 1nnoh.github.io 进入 Settings。</p>
<p>最下面有一个 GitHub Pages：Pages settings now has its own dedicated tab! Check it out here!</p>
<p>点击即可进入博客网站。</p>
<p>Gitee 则是从 Gitee Pages 服务 中生成的网站地址进入。</p>
<h2 id="5-修改主题"><a href="#5-修改主题" class="headerlink" title="5. 修改主题"></a>5. 修改主题</h2><p>我使用的是一款简约整洁的主题：</p>
<ul>
<li><a href="https://github.com/next-theme/hexo-theme-next">GitHub - next-theme&#x2F;hexo-theme-next: 🎉 Elegant and powerful theme for Hexo.</a></li>
</ul>
<p>还有其他比较优秀的主题比如：</p>
<ul>
<li><a href="https://github.com/blinkfox/hexo-theme-matery">GitHub - blinkfox&#x2F;hexo-theme-matery: A beautiful hexo blog theme with material design and responsive design.一个基于材料设计和响应式设计而成的全面、美观的Hexo主题。国内访问：http://blinkfox.com</a></li>
<li><a href="https://github.com/fluid-dev/hexo-theme-fluid">GitHub - fluid-dev&#x2F;hexo-theme-fluid: 一款 Material Design 风格的 Hexo 主题 &#x2F; An elegant Material-Design theme for Hexo</a></li>
<li><a href="https://github.com/jerryc127/hexo-theme-butterfly">GitHub - jerryc127&#x2F;hexo-theme-butterfly: 🦋 A Hexo Theme: Butterfly</a></li>
</ul>
<p>后面这三款会有更加丰富的一些外观设计。</p>
<h2 id="6-后记"><a href="#6-后记" class="headerlink" title="6. 后记"></a>6. 后记</h2><p>最近两天打算在 Gitee 和 Github 同时推送，但 Github 一直推送不成功，还以为是 <code>_config.yml</code> 里 deploy 那一块儿的格式问题。</p>
<p>报错：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">fatal: Authentication failed for &#39;https:&#x2F;&#x2F;github.com&#x2F;1nnoh&#x2F;1nnoh.github.io.git&#x2F;&#39;
FATAL &#123;
  err: Error: Spawn failed
      at ChildProcess.&lt;anonymous&gt; (&#x2F;mnt&#x2F;w&#x2F;github_blog&#x2F;node_modules&#x2F;hexo-util&#x2F;lib&#x2F;spawn.js:51:21)
      at ChildProcess.emit (node:events:390:28)
      at Process.ChildProcess._handle.onexit (node:internal&#x2F;child_process:290:12) &#123;
    code: 128
  &#125;
&#125; Something&#39;s wrong. Maybe you can find the solution here: %s https:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;troubleshooting.html<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>找了两天原因，最后才发现，推送的时候输入的是 GitHub 的用户名与 token，而 token 默认是只有 30 天的有效期，过期了之后要重新生成，并且 token 也会变，需要重新复制保存。</p>
<p>当时美化 Next 主题也是花了很多时间，因为 Hexo 和 Next 经历好多版本的更新，很多老一点的教程都是不适用的。调各种细节，增加一些统计或者评论的模块，都还蛮繁琐的。我就不重复造轮子了，在 References 里贴出一些我参考的教程。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>初级建站：<a href="https://www.bilibili.com/video/BV1mU4y1j72n?p=4">【2021最新版】保姆级Hexo+github搭建个人博客_哔哩哔哩_bilibili</a></p>
<p>后续更改主题可参考：<a href="https://voyage-li.github.io/2021/12/26/experience/">建站经历 | Twilight</a> <a href="https://www.bilibili.com/read/cv4499195/#:~:text=Hexo%E4%B8%BB%E9%A2%98%E6%8E%A8%E8%8D%90%201%20%E3%80%81%20Butterfly%202%20%E3%80%81NexT%203%20%E3%80%81,5%20%E3%80%81Material%20X%206%20%E3%80%81%20Pure%207%20%E3%80%81Icarus">Hexo主题推荐 - 哔哩哔哩</a></p>
<p>查找适合主题：<a href="https://hexo.io/themes/">Themes | Hexo</a></p>
<p>Hexo 官网：<a href="https://hexo.io/zh-cn/docs/#%E5%AE%89%E8%A3%85-Hexo">文档 | Hexo</a></p>
<p>主题美化：</p>
<p><a href="https://theme-next.js.org/docs/theme-settings/">https://theme-next.js.org/docs/theme-settings/</a> </p>
<p><a href="https://www.techgrow.cn/posts/755ff30d.html">https://www.techgrow.cn/posts/755ff30d.html</a></p>
<p><a href="https://iitii.github.io/2021/05/28/1/">https://iitii.github.io/2021/05/28/1/</a></p>
<p><a href="https://happyseashell.gitee.io/2021/09/26/hexo3/#%E4%B8%BB%E9%A2%98%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE">https://happyseashell.gitee.io/2021/09/26/hexo3/#%E4%B8%BB%E9%A2%98%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Gitee</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫与网络编程基础_Task03_BS4基础使用</title>
    <url>/2K8XAGY/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p><a href="http://www.crummy.com/software/BeautifulSoup/">Beautiful Soup</a> 是一个可以从 HTML 或 XML 文件中提取数据的 Python 库。它能够通过你喜欢的转换器实现惯用的文档导航，查找，修改文档的方式。Beautiful Soup 会帮你节省数小时甚至数天的工作时间。</p>
<span id="more"></span>

<ul>
<li>学习资料： <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/">Beautiful Soup 4.2.0 documentation</a> </li>
<li>步骤 1：使用 requests 和 bs4 爬取 <a href="https://scikit-learn.org/stable/modules/classes.html">sklearn api 页面</a> </li>
<li>步骤 2：在 api 页面中有多少个模块？有多少个 API？如 sklearn.base.DensityMixin，其中 base 为模块，DensityMixin 为 API。</li>
<li>步骤 3：将模块名作为 key，api 作为 value 存储为字典。</li>
</ul>
<h2 id="0x01-爬取-sklearn-api-页面"><a href="#0x01-爬取-sklearn-api-页面" class="headerlink" title="0x01 爬取 sklearn api 页面"></a>0x01 爬取 <a href="https://scikit-learn.org/stable/modules/classes.html">sklearn api 页面</a></h2><h3 id="Requests-获得网页-HTML-源代码"><a href="#Requests-获得网页-HTML-源代码" class="headerlink" title="Requests 获得网页 HTML 源代码"></a>Requests 获得网页 HTML 源代码</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests

r &#x3D; requests.get(&quot;https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;classes.html&quot;)
print(r.text)
demo &#x3D; r.text<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="BS4-制作-Soup"><a href="#BS4-制作-Soup" class="headerlink" title="BS4 制作 Soup"></a>BS4 制作 Soup</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">from bs4 import BeautifulSoup

soup &#x3D; BeautifulSoup(demo, &quot;html.parser&quot;)
print(soup.prettify())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>成功解析网页：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292151305.png"></p>
<p>PS： <code>.prettify()</code> 方法可以帮助我们对 HTML 格式化与编码，为 HTML 源码添加一些换行符 <code>\n</code>，来增强 HTML 的可读性。</p>
<h2 id="0x02-BS4-解析库用法详解"><a href="#0x02-BS4-解析库用法详解" class="headerlink" title="0x02 BS4 解析库用法详解"></a>0x02 BS4 解析库用法详解</h2><p><strong>Beautiful Soup 库的理解</strong><br>Beautiful Soup 库是解析、遍历、维护“标签树”的功能库。</p>
<p><strong>Beautiful Soup 类的理解</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292140235.png"></p>
<p><strong>Beautiful Soup 库解析器</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292202930.png"></p>
<p>官方推荐使用 lxml 作为解析器，因为效率更高。</p>
<h3 id="BS4-常用语法"><a href="#BS4-常用语法" class="headerlink" title="BS4 常用语法"></a>BS4 常用语法</h3><p>Beautiful Soup 将 HTML 文档转换成一个树形结构，该结构有利于快速地遍历和搜索 HTML 文档。下面使用树状结构来描述一段 HTML 文档：</p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>html</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>head</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>title</span><span class="token punctuation">></span></span>c语言中文网<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>title</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>head</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span><span class="token punctuation">></span></span>c.biancheng.net<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>b</span><span class="token punctuation">></span></span>一个学习编程的网站<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>b</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>body</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>html</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>树状图如下：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292213495.png" alt="HTML 文档树结构图"></p>
<p>文档树中的每个节点都是 Python 对象，这些对象大致分为四类：Tag , NavigableString , BeautifulSoup , Comment 。其中使用最多的是 Tag 和 NavigableString。</p>
<ul>
<li>Tag：标签类，<strong>HTML 文档中所有的标签都可以看做 Tag 对象。</strong></li>
<li>NavigableString：字符串类，指的是标签中的文本内容，使用 text、string、strings 来获取文本内容。</li>
<li>BeautifulSoup：表示一个 HTML 文档的全部内容，您可以把它当作一个人特殊的 Tag 对象。</li>
<li>Comment：表示 HTML 文档中的注释内容以及特殊字符串，它是一个特殊的 NavigableString。（不常用）</li>
</ul>
<h4 id="1-Tag-节点"><a href="#1-Tag-节点" class="headerlink" title="1. Tag 节点"></a>1. Tag 节点</h4><p>标签（Tag）是组成 HTML 文档的基本元素。在 BS4 中，通过标签名和标签属性可以提取出想要的内容。看一组简单的示例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from bs4 import BeautifulSoup

soup &#x3D; BeautifulSoup(&#39;&lt;p class&#x3D;&quot;Web site url&quot;&gt;&lt;b&gt;c.biancheng.net&lt;&#x2F;b&gt;&lt;&#x2F;p&gt;&#39;, &#39;html.parser&#39;)

#获取整个p标签的html代码
print(soup.p)

#获取b标签
print(soup.p.b)

#获取p标签内容，使用NavigableString类中的string、text、get_text()
print(soup.p.text)

#返回一个字典，里面是属性和值
print(soup.p.attrs)

#查看返回的数据类型
print(type(soup.p))

#根据属性，获取标签的属性值，返回值为列表
print(soup.p[&#39;class&#39;])

#给class属性赋值,此时属性值由列表转换为字符串
soup.p[&#39;class&#39;]&#x3D;[&#39;Web&#39;,&#39;Site&#39;]
print(soup.p)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text">soup.p输出结果:
&lt;p class="Web site url">&lt;b>c.biancheng.net&lt;/b>&lt;/p>

soup.p.b输出结果：
&lt;b>c.biancheng.net&lt;/b>

soup.p.text输出结果：
c.biancheng.net

soup.p.attrs输出结果：
&#123;'class': ['Web', 'site', 'url']&#125;

type(soup.p)输出结果：
&lt;class 'bs4.element.Tag'>

soup.p['class']输出结果：
['Web', 'site', 'url']

class属性重新赋值：
&lt;p class="Web Site">&lt;b>c.biancheng.net&lt;/b>&lt;/p><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>小结：Beautiful Soup 类的五种基本元素</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292240002.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292242273.png"></p>
<h4 id="2-遍历节点"><a href="#2-遍历节点" class="headerlink" title="2. 遍历节点"></a>2. 遍历节点</h4><p>由于 HTML 本身是一种树形结构，所以我们有以下几种遍历方式。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292250137.png"></p>
<p>Tag 对象提供了许多遍历 tag 节点的属性，比如 contents、children 用来遍历子节点；parent 与 parents 用来遍历父节点；而 next_sibling 与 previous_sibling 则用来遍历兄弟节点。</p>
<p><strong>2.1. 下行遍历（子节点）</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300017220.png"></p>
<p>Tag 的 <code>.contents</code> 属性可以将 tag 的子节点以列表的方式输出:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from bs4 import BeautifulSoup

html_doc &#x3D; &quot;&quot;&quot;
&lt;html&gt;&lt;head&gt;&lt;title&gt;&quot;c语言中文网&quot;&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;
&lt;body&gt;
&lt;p class&#x3D;&quot;title&quot;&gt;&lt;b&gt;c.biancheng.net&lt;&#x2F;b&gt;&lt;&#x2F;p&gt;
&lt;p class&#x3D;&quot;website&quot;&gt;一个学习编程的网站&lt;&#x2F;p&gt;
&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;c.biancheng.net&#x2F;python&#x2F;&quot; id&#x3D;&quot;link1&quot;&gt;python教程&lt;&#x2F;a&gt;,
&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;c.biancheng.net&#x2F;c&#x2F;&quot; id&#x3D;&quot;link2&quot;&gt;c语言教程&lt;&#x2F;a&gt; and
&quot;&quot;&quot;
soup &#x3D; BeautifulSoup(html_doc, &#39;html.parser&#39;)
body_tag&#x3D;soup.body

#打印原&lt;body&gt;标签内容
print(body_tag)

#以列表的形式输出，所有子节点
print(body_tag.contents)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">&lt;body>
&lt;p class="title">&lt;b>c.biancheng.net&lt;/b>&lt;/p>
&lt;p class="website">一个学习编程的网站&lt;/p>
&lt;a href="http://c.biancheng.net/python/" id="link1">python教程&lt;/a>,
&lt;a href="http://c.biancheng.net/c/" id="link2">c语言教程&lt;/a> and
&lt;/body>

#以列表的形式输出 可以看见换行符也在内
['\n', &lt;p class="title">&lt;b>c.biancheng.net&lt;/b>&lt;/p>, '\n', &lt;p class="website">一个学习编程的网站&lt;/p>, '\n', &lt;a href=" http://c.biancheng.net/python/" id="link1">python教程&lt;/a>, '\n', &lt;a href=" http://c.biancheng.net/c/" id="link2">c语言教程&lt;/a>, '\n']<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>因为 <code>.contents</code> 返回的是一个列表，所以可以有以下操作：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">#打印列表的长度
print(len(body_tag.contents))

#打印列表中第二个元素（第一个元素是换行符）
print(body_tag.contents[1])
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">#打印列表的长度
9

#打印列表中第二个元素
&lt;p class="title">&lt;b>c.biancheng.net&lt;/b>&lt;/p><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>除了 <code>.contents</code> 之外，Tag 的 <code>.children</code> 属性会生成一个<strong>可迭代对象</strong>，可以用来遍历子节点，示例如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for child in body_tag.children:
    print(child)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">#注意此处已将换行符"\n"省略
&lt;p class="title">&lt;b>c.biancheng.net&lt;/b>&lt;/p>
&lt;p class="website">一个学习编程的网站&lt;/p>
&lt;a href=" http://c.biancheng.net/python/" id="link1">python教程&lt;/a>
&lt;a href=" http://c.biancheng.net/c/" id="link2">c语言教程&lt;/a><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>.contents</code> 和 <code>.children</code> 属性仅包含 tag 的直接子节点。例如，当前的 &lt;head&gt; 标签只有一个直接子节点 &lt;title&gt;：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">head_tag.contents

&#39;&#39;&#39;输出
[&lt;title&gt;The Dormouse&#39;s story&lt;&#x2F;title&gt;]
&#39;&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>但是 &lt;title&gt; 标签也包含一个子节点:字符串 “The Dormouse’s story”，这种情况下字符串“The Dormouse’s story”也属于 &lt;head&gt; 标签的子孙节点。<code>.descendants</code> 属性可以对所有 tag 的子孙节点进行递归循环：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for child in head_tag.descendants:
    print(child)

&#39;&#39;&#39;输出
&lt;title&gt;The Dormouse&#39;s story&lt;&#x2F;title&gt;
The Dormouse&#39;s story
&#39;&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>2.2. 上行遍历（父节点）</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300020443.png"></p>
<p>打印 <code>soup.a</code> 的所有父节点：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300029975.png"></p>
<p>由于 <code>soup.a</code> 的父节点遍历会包含 <code>soup</code> 本身，但是当遍历到 <code>soup.parent</code> 时，由于 <code>soup</code> 不存在先辈节点，所以为空。因此这里要加一个判断 <code>parent</code> 是否为空。即 <code>BeautifulSoup</code> 对象的 <code>.parent</code> 是 None。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">print(soup.parent)
# None<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><strong>2.2. 平行遍历（兄弟节点）</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300034762.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300040532.png"></p>
<p><strong>小结：</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300047393.jpg"></p>
<h4 id="3-基于-BS4-库的-HTML-内容查找"><a href="#3-基于-BS4-库的-HTML-内容查找" class="headerlink" title="3. 基于 BS4 库的 HTML 内容查找"></a>3. 基于 BS4 库的 HTML 内容查找</h4><p>了解 BS4 的基本元素之后，我们就需要利用 BS4 来对 HTML 做信息提取，从中找到我们需要的信息。信息提取的一般方法如下。</p>
<ul>
<li>方法一：完整解析信息的标记形式，再提取关键信息。<ul>
<li>信息标记的三种形式：<strong>XML JSON YAML</strong></li>
<li>需要标记解析器，如：bs4 库的标签树遍历</li>
<li>优点：信息解析准确</li>
<li>缺点：提取过程繁琐，速度慢</li>
</ul>
</li>
<li>方法二：无视标记形式，直接搜索关键信息<ul>
<li><strong>搜索</strong>：对信息的文本使用查找函数即可</li>
<li>优点：提取过程简洁，速度较快</li>
<li>缺点：提取结果的准确性与信息内容直接相关（也就是如果信息质量不好，提取的准确性差）</li>
</ul>
</li>
</ul>
<p><strong>3.1. find_all() 与 find()</strong><br><code>find_all()</code> 与 <code>find()</code> 是解析 HTML 文档的常用方法，它们可以在 HTML 文档中按照一定的条件（相当于过滤器）查找所需内容。<code>find()</code> 与 <code>find_all()</code> 的语法格式相似，希望大家在学习的时候，可以举一反三。</p>
<blockquote>
<p>BS4 库中定义了许多用于搜索的方法，find() 与 find_all() 是最为关键的两个方法，其余方法的参数和使用与其类似。</p>
</blockquote>
<p><strong>3.1.1 find_all()</strong><br><code>find_all()</code> 方法用来搜索当前 tag 的所有子节点，并判断这些节点是否符合过滤条件，最后以列表形式将符合条件的内容返回，语法格式如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">find_all(name, attrs, recursive, text, limit)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>参数说明：</p>
<ul>
<li>name：查找所有名字为 name 的 tag 标签，字符串对象会被自动忽略。</li>
<li>attrs：按照属性名和属性值搜索 tag 标签，**注意由于 class 是 Python 的关键字码，所以要使用 “class_”**。</li>
<li>recursive：find_all() 会搜索 tag 的所有子孙节点，设置 recursive&#x3D;False 可以只搜索 tag 的直接子节点。</li>
<li>text：用来搜文档中的字符串内容，该参数可以接受字符串 、正则表达式 、列表、True。</li>
<li>limit：由于 find_all() 会返回所有的搜索结果，这样会影响执行效率，通过 limit 参数可以限制返回结果的数量。</li>
</ul>
<p><strong>3.1.2 find_all()</strong><br><code>find()</code> 方法与 <code>find_all()</code> 类似，不同之处在于 <code>find_all()</code> 会将文档中所有符合条件的结果返回，而 <code>find()</code> 仅返回一个符合条件的结果，所以 find() 方法没有 <code>limit</code> 参数。</p>
<p>两种方法可以结合 <code>re</code> 使用，官方文档中说明的很详细，给的应用样例也很好，这里就不赘述了。<br><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/">Beautiful Soup 4.4.0 文档 — Beautiful Soup 4.2.0 documentation</a></p>
<h3 id="BS4-入门方法总结"><a href="#BS4-入门方法总结" class="headerlink" title="BS4 入门方法总结"></a>BS4 入门方法总结</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300053492.png"></p>
<p>与信息提取的方法 <code>find_all</code> 和 <code>find()</code>。</p>
<h2 id="0x03-提取-sklearn-api-页面-的信息"><a href="#0x03-提取-sklearn-api-页面-的信息" class="headerlink" title="0x03 提取 sklearn api 页面 的信息"></a>0x03 提取 <a href="https://scikit-learn.org/stable/modules/classes.html">sklearn api 页面</a> 的信息</h2><p>具体任务：在 api 页面中有多少个模块？有多少个 API？如 sklearn.base.DensityMixin，其中 base 为模块，DensityMixin 为 API。</p>
<h3 id="1-查找-Module"><a href="#1-查找-Module" class="headerlink" title="1. 查找 Module"></a>1. 查找 Module</h3><p>首先查看网页的源代码。观察一下网页与源代码。<br>由于我们已知 sklearn.base 为模块，那我们先来在源代码中搜索一下。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300231498.png"></p>
<p>可以很轻松地发现有这么一块，API Reference，结合网页发现是左边的一个索引栏（或者说目录）。检查后确认所有 Module 在这个索引栏的 <code>&lt;li&gt;</code> 列表里有依次列出。因此我们只需要从这一块代码来找 Module 就好了。</p>
<p>以第一个 Module 为例：</p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>reference internal<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>[#module-sklearn.base](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.base)<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>code</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>xref py py-mod docutils literal notranslate<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>pre<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>sklearn.base<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>code</span><span class="token punctuation">></span></span>: Base classes and utility functions
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到内容写在 <code>&lt;span&gt;</code> 这个 Tag 里，然后外面还嵌套了好几层其他的标签。<br>上面的是我手工美化的，源码如下：</p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>reference internal<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>[#module-sklearn.base](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.base)<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>code</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>xref py py-mod docutils literal notranslate<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>pre<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>sklearn.base<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>code</span><span class="token punctuation">></span></span>: Base classes and utility functions<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ul</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>reference internal<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>[#base-classes](https://scikit-learn.org/stable/modules/classes.html#base-classes)<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Base classes<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>reference internal<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>[#functions](https://scikit-learn.org/stable/modules/classes.html#functions)<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Functions<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ul</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>如果你观察了网页源码会发现，这样一个 <code>&lt;li&gt;</code> 标签包裹的列表是第一个 Module <code>sklearn.base</code>的全部内容（在网页左侧 API Reference 这一栏中）。<strong>然后所有的 Module 又一起嵌套在一个更大的 <code>&lt;li&gt;</code> 中。</strong> 为什么这里要重点提一下，因为我直到发现这些信息，才明白我直接做搜索的时候，那些一开始想不通的事情。</p>
<p>那么很简单，直接来把 API Reference 这一栏的所有 Module 检索出来就好了。既然都嵌套在 <code>&lt;li&gt;</code> 中，在源码中再搜索一下这个标签，发现有一大部分集中在一起（API Reference 的位置），但还有零散的一些分布在其他位置。但没关系直接从这些所有的 <code>&lt;li&gt;</code> 标签中找符合我们条件的就好。</p>
<p>思路如下：先找 <code>&lt;li&gt;</code>，然后找其中嵌套着 <code>class=&quot;pre&quot;</code> 的 <code>&lt;span&gt;</code> 标签。<code>&lt;span&gt;</code> 标签的 <code>string</code> 正是我们要搜索的信息，用一个正则表达式搜索 <code>sklearn.</code> 就好咯。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 嵌套查询
# 不能直接 soup.find_all(&#39;li&#39;).find_all()，会报错
li &#x3D; soup.find_all(&#39;li&#39;)
module &#x3D; []  # 设置一个列表来存储读到的数据
# 我第一次搜素的时候发现有很多空列表值，所以加一个 if条件
# 另外一个重点是，对 class 属性设置条件时，应该用 class_ []
for i in li:
    if i.find_all(&#39;span&#39;, class_ &#x3D; &#39;pre&#39;, string &#x3D; re.compile(&#39;sklearn.&#39;)) !&#x3D; []:
        module.append(i.find_all(&#39;span&#39;, class_ &#x3D; &#39;pre&#39;, string &#x3D; re.compile(&#39;sklearn.&#39;)))

print(module[0])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>来看一下获得的 <code>module</code> 列表：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">module[0]

[&lt;span class="pre">sklearn.base&lt;/span>,
 &lt;span class="pre">sklearn.calibration&lt;/span>,
 &lt;span class="pre">sklearn.cluster&lt;/span>,
 &lt;span class="pre">sklearn.compose&lt;/span>,
 ...
 &lt;span class="pre">sklearn.svm&lt;/span>,
 &lt;span class="pre">sklearn.tree&lt;/span>,
 &lt;span class="pre">sklearn.utils&lt;/span>]

module[1]:

[&lt;span class="pre">sklearn.base&lt;/span>]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到 <code>module[0]</code> 存储了所有的 Module，<code>module[1]</code> 存储了第一个 Module。后面的也都是存储单个的 Module。</p>
<p>我感觉是因为 <code>api[0]</code> 存的是，从最大的 <code>&lt;li&gt;</code> 找的里面的所有 <code>&lt;span&gt;</code> 标签。后面的是从最大的 <code>&lt;li&gt;</code> 里面嵌套的 <code>&lt;li&gt;</code> 中的 <code>&lt;span&gt;</code> 找到的信息，因为最小级别的 <code>&lt;li&gt;</code> 里面只有一个 <code>&lt;span&gt;</code>。</p>
<p>再看一下 <code>sklearn.base</code> Module 的源码，整体关系是这样的：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">&lt;li>
	&lt;li>
		&lt;span class="pre">sklearn.base&lt;/span>
		&lt;li>&lt;/li>
		&lt;li>&lt;/li>
	&lt;/li>
	...
&lt;/li><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>理清了关系，来看一下有多少个 Module：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">print(len(api[0]))
print(len(api))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>38<br>39</p>
</blockquote>
<p>因此，有 38 个 Module。后面多的那一个，不用问是什么了吧？</p>
<p>将 Module 存入 <code>module</code> 列表：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">li &#x3D; soup.find_all(&#39;li&#39;)
module &#x3D; []

# 首先在所有&lt;li&gt;中遍历
# 然后找到我们需要的&lt;li&gt;
# 接着只用第一个最大的&lt;li&gt;，所以是 find()，只找第一个
# 最后找到这个最大&lt;li&gt;中所有的&lt;span&gt;，并返回他们的 string
# 存储后跳出循环，否则会继续遍历最大&lt;li&gt;中的其余子&lt;li&gt;标签
for i in li:
    if i.find(&#39;a&#39;, class_ &#x3D; &#39;reference internal&#39;) !&#x3D; None:
        if i.find(&#39;span&#39;, class_ &#x3D; &#39;pre&#39;, string &#x3D; re.compile(&#39;sklearn.&#39;)) !&#x3D; None:
            span &#x3D; i.find_all(&#39;span&#39;, class_ &#x3D; &#39;pre&#39;, string &#x3D; re.compile(&#39;sklearn.&#39;))
            module &#x3D; [x.string for x in span]
            break<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>这里有一个细节需要注意。<code>find()</code> 如果找不到，返回 <code>None</code>； <code>find_all()</code> 如果找不到，返回的是空列表 <code>[]</code>。也就是说前者返回一串字符，后者返回一个包含所有结果的列表。</p>
<h3 id="2-查找-API"><a href="#2-查找-API" class="headerlink" title="2. 查找 API"></a>2. 查找 API</h3><p>一样的步骤，以 <code>base</code> 的第一个 API 为例，先找一下  <code>base.BaseEstimator</code> 的位置。可以发现一个 Module 的 API 都在 <code>&lt;tbody&gt;</code> 中（一个 Module 的所有 API 在两个 <code>&lt;tbody&gt;</code> 里面。一个 <code>&lt;section id=&quot;base-classes&quot;&gt;</code>，一个 <code>&lt;section id=&quot;functions&quot;&gt;</code> ）。然后每一个 API 在其中的一个个 <code>&lt;tr&gt;</code> 里面。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203301712851.png"></p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tr</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>row-odd<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>reference internal<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>[generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator](https://scikit-learn.org****mator)<span class="token punctuation">"</span></span> <span class="token attr-name">title</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>sklearn.base.BaseEstimator<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>code</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>xref py py-obj docutils literal notranslate<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>pre<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>base.BaseEstimator<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>code</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>

	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>Base class for all estimators in scikit-learn.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tr</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>我们要找的 API 是在 <code>&lt;tr&gt;</code> 中的第一个 <code>&lt;td&gt;</code> 的 <code>.string</code>。</p>
<p>思路：首先找到 <code>&lt;tbody&gt;</code> 获得所有 API 的信息，然后在 <code>&lt;tbody&gt;</code>  中解析 <code>&lt;tr&gt;</code> 获得每一个 API 的信息，再把 <code>&lt;tr&gt;</code> 标签中的 <code>&lt;td&gt;</code> 标签找到，把 API 的名字写入列表。通过遍历节点已经查找方法获得。</p>
<h4 id="2-1-首先试试对第一个-lt-tbody-gt-爬取："><a href="#2-1-首先试试对第一个-lt-tbody-gt-爬取：" class="headerlink" title="2.1 首先试试对第一个 &lt;tbody&gt; 爬取："></a>2.1 首先试试对第一个 <code>&lt;tbody&gt;</code> 爬取：</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203301756250.png"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import bs4

# 因为每一个标签的子标签可能存在字符串类型
# 而所有的 API 信息封装在 &lt;tr&gt; 标签中，&lt;tr&gt; 标签是标签类型
# 所以要过滤掉非标签类型的其他信息
for tr in soup.find(&#39;tbody&#39;).children:
    if isinstance(tr, bs4.element.Tag):  # 检测 tr 是否为标签类型
        tds &#x3D; tr(&#39;td&#39;)
        print(tds[0].string)  # 打印第一个 &lt;td&gt;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text"># 输出了第一个 &lt;tbody> 里面的 API
base.BaseEstimator
base.BiclusterMixin
base.ClassifierMixin
base.ClusterMixin
base.DensityMixin
base.RegressorMixin
base.TransformerMixin
feature_selection.SelectorMixin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="2-2-爬取所有-lt-tbody-gt"><a href="#2-2-爬取所有-lt-tbody-gt" class="headerlink" title="2.2 爬取所有 &lt;tbody&gt;"></a>2.2 爬取所有 <code>&lt;tbody&gt;</code></h4><p><strong>正解</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">api &#x3D; []

for tbody in soup.find_all(&#39;tbody&#39;):  # 找到所有 &lt;tbody&gt;
    for tr in tbody.find_all(&#39;tr&#39;):  # 对每一个 &lt;tbody&gt; 的子孙节点遍历
        if isinstance(tr, bs4.element.Tag):  # 检测 tr 是否为标签类型
            tds &#x3D; tr(&#39;td&#39;)
            api.append(tds[0](&#39;span&#39;)[0].string)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>代码中的最后一步是，取 <code>&lt;tr&gt;</code> 中的第一个 <code>&lt;td&gt;</code>，然后找 <code>&lt;td&gt;</code> 中的第一个 <code>&lt;span&gt;</code> 并且读取 <code>.string</code>。</p>
<p><strong>思考</strong><br>正解是经过尝试，发现如果直接用之前的 <code>tds[0].string</code>，会导致一些 API 读不到，所以取到更精确的那一级 <code>&lt;span&gt;</code> 标签。还没想明白为什么。情况如下（不正确的读取）：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">api &#x3D; []

for tbody in soup.find_all(&#39;tbody&#39;):  # 找到所有 &lt;tbody&gt;
    for tr in tbody.find_all(&#39;tr&#39;):  # 对每一个 &lt;tbody&gt; 的子孙节点遍历
        if isinstance(tr, bs4.element.Tag):  # 检测 tr 是否为标签类型
            tds &#x3D; tr(&#39;td&#39;)  # 查找所有 &lt;td&gt;
			api.append(tds[0]) # 只保存第一个 &lt;td&gt;
            print(tds[0].string)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text"># 输出如下（省略了一些）
...
base.RegressorMixin
base.TransformerMixin
feature_selection.SelectorMixin
None
None
None
None
...
experimental.enable_iterative_imputer
experimental.enable_halving_search_cv
None
None
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>长度跟正解是一样的，只是有的 <code>.string</code> 读不出来。</p>
<p>再对存为列表的所有 <code>&lt;td&gt;</code> 研究一下：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203302141074.png"></p>
<p>这里是从中选了两个 <code>&lt;td&gt;</code> 标签的内容。</p>
<p>第一个能读出来，第二个读不出来：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203302148928.png"></p>
<p>进一步精细搜索：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203302146810.png"></p>
<p>原因未知。我猜可能是因为那些读不出来的是解码器识别不到，看看源码：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text"># api[50]
&lt;td>&lt;p>&lt;a class="reference internal" href="generated/sklearn.covariance.OAS.html#sklearn.covariance.OAS" title="sklearn.covariance.OAS">&lt;code class="xref py py-obj docutils literal notranslate">&lt;span class="pre">covariance.OAS&lt;/span>&lt;/code>&lt;/a>(*[, store_precision, ...])&lt;/p>&lt;/td><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><code>&lt;td&gt;</code> 中确实没有 <code>string</code>（不知道 <code>&lt;/p&gt;</code> 前面那个算不算），包含 <code>string</code> 的是其中的 <code>&lt;span&gt;</code> 标签，可能是这个原因吧。</p>
<p>后来经过群友的努力，大概是这个原因：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">Type:        property
String form: &lt;property object at 0x000002036EFD32C8&gt;
Source:     
# text.string.fget 
@property 
def string(self):     
	&quot;&quot;&quot;Convenience property to get the single string within this
    PageElement.

    TODO It might make sense to have NavigableString.string return
    itself.

    :return: If this element has a single string child, return
     value is that string. If this element has one child tag,
     return value is the &#39;string&#39; attribute of the child tag,
     recursively. If this element is itself a string, has no
     children, or has more than one child, return value is None.
    &quot;&quot;&quot;     
	if len(self.contents) !&#x3D; 1:
		return None     
	child &#x3D; self.contents[0]     
	if isinstance(child, NavigableString):
		return child     
	return child.string  
# text.string.fset 
@string.setter 
def string(self, string):
	&quot;&quot;&quot;Replace this PageElement&#39;s contents with &#96;string&#96;.&quot;&quot;&quot;
	self.clear()
	self.append(string.__class__(string))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>return</code> 那里解释说到：<code>If this element is itself a string, has no children, or has more than one child, return value is None.</code> 所以大概是还有一个 <code>(*[, store_precision, ...])</code> 算是 <code>have more than one child</code> 吧。</p>
<p><code>api[50].text</code> 的输出为 <code>&#39;covariance.OAS(*[,\xa0store_precision,\xa0...])&#39;</code>。</p>
<h2 id="0x04-存储爬取的信息"><a href="#0x04-存储爬取的信息" class="headerlink" title="0x04 存储爬取的信息"></a>0x04 存储爬取的信息</h2><p>将爬取到的模块名作为 Key，API 作为 Value 存储为字典</p>
<p>其实跟上面的两个任务大同小异，关键是怎么搜索到每个 Moudule 对应的 API。</p>
<p>因为每个 Module 里面有多少 <code>&lt;tbody&gt;</code> 是不固定的，所以要找到每个 Module 外面包围的那个标签，不能像上一节一样只对 <code>&lt;tbody&gt;</code> 做处理。</p>
<p>看了源码可以发现每个 Module 对应了一个 <code>&lt;section&gt;</code> 标签，并且里面的 <code>id</code> 描述了这一个 <code>&lt;section&gt;</code>。</p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>section</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>module-sklearn.base<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>先对第一个 Module 试试：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sec &#x3D; soup.find(&#39;section&#39;, id &#x3D; &#39;module-sklearn.base&#39;)
api &#x3D; []

for tbody in sec.find_all(&#39;tbody&#39;):  # 找到所有 &lt;tbody&gt;
    for tr in tbody.find_all(&#39;tr&#39;):  # 对每一个 &lt;tbody&gt; 的子孙节点遍历
        if isinstance(tr, bs4.element.Tag):  # 检测 tr 是否为标签类型
            tds &#x3D; tr(&#39;td&#39;)
            api.append(tds[0](&#39;span&#39;)[0].string)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text"># api
['base.BaseEstimator',
 'base.BiclusterMixin',
 'base.ClassifierMixin',
 'base.ClusterMixin',
 'base.DensityMixin',
 'base.RegressorMixin',
 'base.TransformerMixin',
 'feature_selection.SelectorMixin',
 'base.clone',
 'base.is_classifier',
 'base.is_regressor',
 'config_context',
 'get_config',
 'set_config',
 'show_versions']<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>使用我们之前存储的 <code>module</code> 列表，推广到所有的 <code>&lt;section&gt;</code> ：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def api_find(sec):
    for tbody in sec.find_all(&#39;tbody&#39;):  # 找到所有 &lt;tbody&gt;
        for tr in tbody.find_all(&#39;tr&#39;):  # 对每一个 &lt;tbody&gt; 的子孙节点遍历
            if isinstance(tr, bs4.element.Tag):  # 检测 tr 是否为标签类型
                tds &#x3D; tr(&#39;td&#39;)
                print(tds[0](&#39;span&#39;)[0].string)
    print(&#39;------------------&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">for mo in module:
    print(mo)
    print(&#39;------------------&#39;)
    sec &#x3D; soup.find(&#39;section&#39;, id &#x3D; &#39;module-&#39;+mo)
    api_find(sec)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text"># 输出
sklearn.base
------------------
base.BaseEstimator
...
show_versions
------------------
sklearn.calibration
------------------
calibration.CalibratedClassifierCV
calibration.calibration_curve
------------------
sklearn.cluster
------------------
cluster.AffinityPropagation
cluster.AgglomerativeClustering
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>存为字典</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def api_find(sec, dic, mo:str):
    dic[mo] &#x3D; []  # 创建字典当前 key 的 value list
    if sec:  # 添加这个是因为有的 sec 为 None，不加会报错，不知原因
        for tbody in sec.find_all(&#39;tbody&#39;):
            for tr in tbody.find_all(&#39;tr&#39;):
                if isinstance(tr, bs4.element.Tag):
                    tds &#x3D; tr(&#39;td&#39;)
                    dic[mo].append(tds[0](&#39;span&#39;)[0].string)

sklearn_dic &#x3D; &#123;&#125;

for mo in module:
    sec &#x3D; soup.find(&#39;section&#39;, id &#x3D; &#39;module-&#39; + mo)
    api_find(sec, sklearn_dic, mo)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text"># sklearn_dic
&#123;'sklearn.base': ['base.BaseEstimator',
  'base.BiclusterMixin',
  ...
  'show_versions'],
 'sklearn.calibration': ['calibration.CalibratedClassifierCV',
  'calibration.calibration_curve'],
 'sklearn.cluster': ['cluster.AffinityPropagation',
  'cluster.AgglomerativeClustering',
  'cluster.Birch',
  ...
  'cluster.spectral_clustering',
  'cluster.ward_tree'],
 'sklearn.compose': ['compose.ColumnTransformer',
  'compose.TransformedTargetRegressor',
  'compose.make_column_transformer',
  'compose.make_column_selector'],
 'sklearn.covariance': ['covariance.EmpiricalCovariance',
  'covariance.EllipticEnvelope',
  'covariance.GraphicalLasso',
  ...&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>也可以用 <code>from collections import defaultdict</code>，免得创建空列表了。</p>
<h2 id="0x05-Conclusion"><a href="#0x05-Conclusion" class="headerlink" title="0x05 Conclusion"></a>0x05 Conclusion</h2><p>终于写完这一章了，感觉写的快要了老命了。后面的内容，如果有总结的很好的基础知识，考虑是不是不用重复造轮子。但写一遍好像又会帮我掌握的更好一些。基础知识这块尽量还是写的更加精简，把自己的思考写出来就好。</p>
<p>这章写完体会就是，重点还是在动手，实战过程中会遇到各种问题，可以引发更多的思考，或者对知识查缺补漏。</p>
<p>通过学习，掌握了 BS4 的基本用法。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p> <a href="http://c.biancheng.net/python_spider/bs4.html">Python BS4 解析库用法详解</a><br> <a href="https://www.bilibili.com/video/BV1pt41137qK?p=23&spm_id_from=pageDriver">Python 爬虫视频教程全集（62P）| bilibili</a><br> <a href="https://blog.csdn.net/learner_syj/article/details/120590574">BeautifulSoup 利用 find_all() 多级标签索引和获取标签中的属性内容</a><br> <a href="https://zhuanlan.zhihu.com/p/344114093">Python 中的 x for y in z for x in y语法详解</a><br> <a href="https://blog.csdn.net/weixin_44912159/article/details/108457413">python在字典中创建一键多值的几种方法</a><br> <a href="https://zhuanlan.zhihu.com/p/349471029">Python 中3种创建字典数据的方法</a><br> <a href="http://c.biancheng.net/view/2212.html">Python字典及基本操作（超级详细）</a></p>
]]></content>
      <categories>
        <category>爬虫与网络编程基础</category>
      </categories>
      <tags>
        <tag>Coggle 30 Days of ML</tag>
        <tag>Spider</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里灵杰_Task01_环境配置与实践数据下载</title>
    <url>/25S7X1E/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><ul>
<li>任务内容：<ul>
<li>从比赛官网下载数据集，并使用 Python 读取数据</li>
<li>使用 <code>jieba</code> 对文本进行分词</li>
<li>使用 <code>TFIDF</code> 对文本进行编码</li>
<li>思考如何使用 TFIDF 计算文本相似度？</li>
</ul>
</li>
<li>学习资料： <a href="https://coggle.club/blog/tianchi-open-search">https://coggle.club/blog/tianchi-open-search</a><span id="more"></span></li>
</ul>
<h2 id="0x01-读取数据"><a href="#0x01-读取数据" class="headerlink" title="0x01 读取数据"></a>0x01 读取数据</h2><p>这里只用 <code>train.query.txt</code> 中的前十条数据。同整个数据集的数据处理同理。</p>
<p>首先导入需要用到的包：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import pandas as pd 
import jieba  # 分词
from gensim import corpora, models, similarities  # TF-IDF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>读取数据：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">with open(&#39;data&#x2F;train.query.txt&#39;, encoding&#x3D;&quot;utf-8&quot;) as f:
    train_querys &#x3D; f.readlines()

train_query &#x3D; &#123;&#125;
for line in train_querys:
    line &#x3D; line.strip().split(&#39;\t&#39;)
    query_id &#x3D; line[0]
    query &#x3D; line[1]
    train_query[int(query_id)] &#x3D; query<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>取字典 <code>train_query</code> 的前十条数据：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 字典切片
def dict_slice(adict, start, end):
    keys &#x3D; adict.keys()
    dict_slice &#x3D; &#123;&#125;
    for k in list(keys)[start:end]:
        dict_slice[k] &#x3D; adict[k]
    return dict_slice

wordListTop10 &#x3D; dict_slice (train_query, 0, 10)
wordListTop10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>{1: ‘美赞臣亲舒一段’,<br>2: ‘慱朗手动料理机’,<br>3: ‘電力貓’,<br>4: ‘掏夹缝工具’,<br>5: ‘飞推 vip’,<br>6: ‘多功能托地把’,<br>7: ‘充气浮力袖’,<br>8: ‘盒马花胶鸡汤锅’,<br>9: ‘塞塞乐’,<br>10: ‘广汽传祺 gs5 挡风遮雨条子’}</p>
</blockquote>
<h2 id="0x02-使用-jieba-对文本进行分词"><a href="#0x02-使用-jieba-对文本进行分词" class="headerlink" title="0x02 使用 jieba 对文本进行分词"></a>0x02 使用 <code>jieba</code> 对文本进行分词</h2><p>使用 <code>jieba</code> 中的搜索引擎模式对文本分词。关于 <code>jieba</code> 的更多信息可以参考 <a href="https://github.com/fxsjy/jieba">GitHub - fxsjy&#x2F;jieba: 结巴中文分词</a> 。</p>
<p>有三种分词模式，这里使用默认的精确模式。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">seg_list &#x3D; []
for i in range(1,11):
    seg &#x3D; list(jieba.cut(wordListTop10[i],HMM&#x3D;True))
    seg_list.append(seg)  # 把前10个数据的所有分词放进一个列表中
seg_list<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[[&#39;美赞臣&#39;, &#39;亲舒&#39;, &#39;一段&#39;], [&#39;慱&#39;, &#39;朗&#39;, &#39;手动&#39;, &#39;料理&#39;, &#39;机&#39;], [&#39;電力貓&#39;], [&#39;掏&#39;, &#39;夹缝&#39;, &#39;工具&#39;], [&#39;飞&#39;, &#39;推&#39;, &#39;vip&#39;], [&#39;多功能&#39;, &#39;托地&#39;, &#39;把&#39;], [&#39;充气&#39;, &#39;浮力&#39;, &#39;袖&#39;], [&#39;盒马花胶&#39;, &#39;鸡汤&#39;, &#39;锅&#39;], [&#39;塞塞&#39;, &#39;乐&#39;], [&#39;广汽传祺&#39;, &#39;gs5&#39;, &#39;挡风遮雨&#39;, &#39;条子&#39;]]</code></p>
</blockquote>
<h2 id="0x03-使用-TF-IDF-对文本进行编码"><a href="#0x03-使用-TF-IDF-对文本进行编码" class="headerlink" title="0x03 使用 TF-IDF 对文本进行编码"></a>0x03 使用 <code>TF-IDF</code> 对文本进行编码</h2><h3 id="什么是-TF-IDF-检索"><a href="#什么是-TF-IDF-检索" class="headerlink" title="什么是 TF-IDF 检索"></a>什么是 <code>TF-IDF</code> 检索</h3><blockquote>
<p>可以查看 <a href="https://1nnoh.github.io/280EQA3/">矢量语义与嵌入之 TF-IDF 检索</a> 进行学习。</p>
</blockquote>
<h3 id="使用-TF-IDF-对我们的数据集编码"><a href="#使用-TF-IDF-对我们的数据集编码" class="headerlink" title="使用 TF-IDF 对我们的数据集编码"></a>使用 <code>TF-IDF</code> 对我们的数据集编码</h3><p><a href="https://radimrehurek.com/gensim/models/tfidfmodel.html">models.tfidfmodel – TF-IDF model — gensim</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 制作字典
dictionary &#x3D; corpora.Dictionary(seg_list)

# 可以通过 token2id 得到特征数字
print(dictionary.token2id)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>{‘一段’: 0, ‘亲舒’: 1, ‘美赞臣’: 2, ‘慱’: 3, ‘手动’: 4, ‘料理’: 5, ‘朗’: 6, ‘机’: 7, ‘電力貓’: 8, ‘夹缝’: 9, ‘工具’: 10, ‘掏’: 11, ‘vip’: 12, ‘推’: 13, ‘飞’: 14, ‘多功能’: 15, ‘托地’: 16, ‘把’: 17, ‘充气’: 18, ‘浮力’: 19, ‘袖’: 20, ‘盒马花胶’: 21, ‘锅’: 22, ‘鸡汤’: 23, ‘乐’: 24, ‘塞塞’: 25, ‘gs5’: 26, ‘广汽传祺’: 27, ‘挡风遮雨’: 28, ‘条子’: 29}</p>
</blockquote>
<p>将所有的词存入一个字典，上面打印出了每个词对应的 <code>id</code>。所以上面这一步其实就是将词语都映射为数字，因为机器只能理解数字呀。</p>
<p>然后首先将这十条数据放入词袋模型，之后再对词袋模型中的数据使用 <code>TF-IDF</code> 计算权值，得到 <code>TF-IDF</code> 编码。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># Convert corpus to BoW format
# 制作数字向量类型的语料库（doc2bow）
# ----&gt; 将字符串转换成数字向量类型的词袋模型(稀疏向量)
# 源文件不做处理是一个字符串类型的语料库
corpus &#x3D; [dictionary.doc2bow (doc) for doc in seg_list]  
corpus<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[[(0,1), (1,1), (2,1)], [(3,1), (4,1), (5,1), (6,1), (7,1)], [(8,1)], [(9,1), (10,1), (11,1)], [(12,1), (13,1), (14,1)], [(15,1), (16,1), (17,1)], [(18,1), (19,1), (20,1)], [(21,1), (22,1), (23,1)], [(24,1), (25,1)], [(26,1), (27,1), (28,1), (29,1)]]</code></p>
</blockquote>
<p>这里是将十条数据转为词袋模型，下面贴出之前这十条数据的分词结果：</p>
<blockquote>
<p><code>[[&#39;美赞臣&#39;, &#39;亲舒&#39;, &#39;一段&#39;], [&#39;慱&#39;, &#39;朗&#39;, &#39;手动&#39;, &#39;料理&#39;, &#39;机&#39;], [&#39;電力貓&#39;], [&#39;掏&#39;, &#39;夹缝&#39;, &#39;工具&#39;], [&#39;飞&#39;, &#39;推&#39;, &#39;vip&#39;], [&#39;多功能&#39;, &#39;托地&#39;, &#39;把&#39;], [&#39;充气&#39;, &#39;浮力&#39;, &#39;袖&#39;], [&#39;盒马花胶&#39;, &#39;鸡汤&#39;, &#39;锅&#39;], [&#39;塞塞&#39;, &#39;乐&#39;], [&#39;广汽传祺&#39;, &#39;gs5&#39;, &#39;挡风遮雨&#39;, &#39;条子&#39;]]</code></p>
</blockquote>
<p>以第一条数据为例：<br>第一条数据的词袋模型：<code>[(0,1), (1,1), (2,1)]</code><br>第一条数据的分词结果：<code>[&#39;美赞臣&#39;, &#39;亲舒&#39;, &#39;一段&#39;]</code><br>第一条数据的 <code>token2id</code> ：  <code>&#39;一段&#39;: 0, &#39;亲舒&#39;: 1, &#39;美赞臣&#39;: 2</code><br>词袋中第一个词 <code>（0，1）</code> 对应着：“一段”（编码为 0），数据中出现一次，所以 <code>（0，1）</code>。第一个数字代表哪一个词，第二个数字代表出现了几次（词频）。后面的两个词同理。三个词放一起，组成词袋，来代表整个第一条数据。</p>
<p>可以发现，不存在的词是不放入每条数据的词袋的，通过这样的方式来节约内存。这种技术叫稀疏矩阵（Sparse Matrix）：只存储有内容的值，而忽略无内容的值。</p>
<p>最后得到 <code>TF-IDF</code> 编码（以 <code>corpus</code> 中第一条数据为例 ）：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">tfidf &#x3D; models.TfidfModel(corpus)  # fit model
vector &#x3D; tfidf[corpus[0]]  # apply model to the first corpus document
vector<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]</code></p>
</blockquote>
<p>你也可以自己动手，看看后面的几条数据编码是什么样子的。</p>
<h2 id="0x04-使用-TF-IDF-计算文本相似度"><a href="#0x04-使用-TF-IDF-计算文本相似度" class="headerlink" title="0x04 使用 TF-IDF 计算文本相似度"></a>0x04 使用 <code>TF-IDF</code> 计算文本相似度</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">def semblance(text, corpus):
    # 对测试文本分词
    dic_text_list &#x3D; list(jieba.cut(text))
 
    # 制作测试文本的词袋
    doc_text_vec &#x3D; dictionary.doc2bow(dic_text_list)
 
    # 获取语料库每个文档中每个词的tfidf值，即用tfidf模型训练语料库
    tfidf &#x3D; models.TfidfModel(corpus)
 
    # 对稀疏向量建立索引
    index &#x3D; similarities.SparseMatrixSimilarity(tfidf[corpus], num_features&#x3D;len(dictionary.keys()))
    sim &#x3D; index[tfidf[doc_text_vec]]  # 相当于sim &#x3D; index.get_similarities(tfidf[doc_text_vec])
    print(&quot;相似度评估：&quot;)
    print(sim)
    # 按照相似度来排序
    sim_sorted &#x3D; sorted(enumerate(sim, 1), key&#x3D;lambda x: -x[1])  # enumerate(x, 1) 代表从1开始设立索引
    # 相当于sorted(enumerate(sim), key&#x3D;lambda x: x[1], reverse&#x3D;True
    print(&quot;相似度排序：&quot;)
    print(sim_sorted)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">text &#x3D; &#39;想喝鸡汤&#39;
semblance (text, corpus)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>Output:</p>
</blockquote>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">相似度评估：
[0.         0.         0.         0.         0.         0.
 0.         0.81649655 0.         0.        ]
10
相似度排序：
[(8, 0.81649655), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (9, 0.0), (10, 0.0)]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>没有做过 NLP 的东西，趁着做今天的任务大概了解了分词，文本向量化表示之类的内容。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>jieba 分词：<br> <a href="https://github.com/fxsjy/jieba">GitHub - fxsjy&#x2F;jieba: 结巴中文分词</a><br> <a href="https://alvinntnu.github.io/python-notes/corpus/jieba.html">Chinese Word Segmentation (jieba) — Python Notes for Linguistics</a><br> <a href="https://zhuanlan.zhihu.com/p/361052986">中文分词工具 jieba 的简介｜自然语言处理</a><br> <a href="https://zhuanlan.zhihu.com/p/207057233">jieba 分词-强大的 Python 中文分词库</a></p>
<p>Gensim:<br> <a href="https://gensim.apachecn.org/#/blog/tutorial/README">https://gensim.apachecn.org/#/blog/tutorial/README</a></p>
<p>TF-IDF:<br> <a href="https://blog.csdn.net/weixin_44799217/article/details/116423520">自然语言处理(NLP)之使用 TF-IDF 模型计算文本相似度</a><br> <a href="https://mofanpy.com/tutorials/machine-learning/nlp/intro-search/">你天天用的搜索引擎是怎么工作的 - 自然语言处理 | 莫烦 Python</a><br> <a href="https://mofanpy.com/tutorials/machine-learning/nlp/tfidf/">统计学让搜索速度起飞 - 自然语言处理 | 莫烦 Python</a></p>
]]></content>
      <categories>
        <category>“阿里灵杰”问天引擎电商搜索算法赛</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里灵杰_Task02_词向量介绍与训练</title>
    <url>/CGCSR4/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><ul>
<li>任务内容：<ul>
<li>使用任务 1 得到数据使用 <code>gensim</code> 训练词向量</li>
<li>计算与 <code>格力</code> 相似的 Top10 单词</li>
<li>使用词向量完成句子编码（例如单词编码为 128 维度，一个句子包含十个单词为 10*128）</li>
<li>对句子编码 10*128 进行求均值，转变为 128 维度</li>
<li>扩展：你能使用计算得到的词向量，计算 train.query.txt 和 corpus.tsv 文本的相似度吗（train 选择 100 条文本，corpus 选择 100 条文本）？</li>
</ul>
</li>
<li>学习资料：<ul>
<li><a href="https://coggle.club/blog/tianchi-open-search">https://coggle.club/blog/tianchi-open-search</a></li>
<li><a href="https://radimrehurek.com/gensim/models/word2vec.html">https://radimrehurek.com/gensim/models/word2vec.html</a><span id="more"></span></li>
</ul>
</li>
</ul>
<h2 id="0x01-使用-gensim-训练词向量"><a href="#0x01-使用-gensim-训练词向量" class="headerlink" title="0x01 使用 gensim 训练词向量"></a>0x01 使用 <code>gensim</code> 训练词向量</h2><h3 id="数据集读取"><a href="#数据集读取" class="headerlink" title="数据集读取"></a>数据集读取</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 导入相关库
import numpy as np
import pandas as pd
import os
from tqdm import tqdm_notebook<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 读取数据集
corpus_data &#x3D; pd.read_csv( &quot;.&#x2F;data&#x2F;corpus.tsv&quot;, sep&#x3D;&quot;\t&quot;, names&#x3D;[&quot;doc&quot;, &quot;title&quot;])
train_data &#x3D; pd.read_csv(&quot;.&#x2F;data&#x2F;train.query.txt&quot;, sep&#x3D;&quot;\t&quot;, names&#x3D;[&quot;query&quot;, &quot;title&quot;])
qrels &#x3D; pd.read_csv(&quot;.&#x2F;data&#x2F;qrels.train.tsv&quot;, sep&#x3D;&quot;\t&quot;, names&#x3D;[&quot;query&quot;, &quot;doc&quot;])
dev_data &#x3D; pd.read_csv (&quot;.&#x2F;data&#x2F;dev.query.txt&quot;, sep&#x3D;&quot;\t&quot;, names&#x3D;[&quot;query&quot;, &quot;title&quot;])

# 将原文中 index 设为 df 的 index
corpus_data &#x3D; corpus_data.set_index(&quot;doc&quot;)
train_data &#x3D; train_data.set_index(&quot;query&quot;)
qrels &#x3D; qrels.set_index(&quot;query&quot;)
dev_data &#x3D; dev_data.set_index (&quot;query&quot;)

# 查看一下刚刚导入的数据集
train_data.head()
qrels.head()
qrels.loc[1][&quot;doc&quot;]

# 查看一下前 19 个训练集的 query 与 doc 对应的字段
for idx in range(1,20): 
    print(
        train_data.loc[idx][&quot;title&quot;],
        &quot;\t&quot;,
        corpus_data.loc[qrels.loc[idx][&quot;doc&quot;]][&quot;title&quot;])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>美赞臣亲舒一段        领券满减】美赞臣安婴儿 A+亲舒 婴儿奶粉 1 段 850 克 0-12 个月宝宝<br>慱朗手动料理机        Braun&#x2F;博朗 MQ3035&#x2F;3000&#x2F;5025 料理棒手持小型婴儿辅食家用搅拌机<br>電力貓    小米 WiFi 电力猫无线路由器套装一对 300M 穿墙宝家用信号增强扩展器</p>
</blockquote>
<p>这里只截取一部分展示。</p>
<h3 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 导入 jieba 分词
import jieba

# 先对一个字符串分词看看
&quot; &quot;.join(jieba.cut(&quot;慱朗手动料理机&quot;))

&#39;&#39;&#39;
输出 -&gt; &#39;慱 朗 手动 料理 机&#39;
&#39;&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>上分点：词典（品牌）</strong></p>
<blockquote>
<p>由于 jieba 对品牌的分词效果不一定好，所以可以自行找一些品牌的词典来对这些标题或者 query 分词。</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 对整个数据集分词
def title_cut(x:str):
    return list(jieba.cut(x, HMM&#x3D;True))

from joblib import Parallel, delayed

corpus_title &#x3D; Parallel(n_jobs&#x3D;4)(delayed(title_cut)(title) for title in corpus_data[&quot;title&quot;])
train_title &#x3D; Parallel(n_jobs&#x3D;4)(delayed(title_cut)(title) for title in train_data[&quot;title&quot;])
dev_title &#x3D; Parallel(n_jobs&#x3D;4)(delayed(title_cut)(title) for title in dev_data[&quot;title&quot;])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 使用 gensim 中的 Word2Vec 得到数据集的词向量
from gensim.models import Word2Vec
from gensim.test.utils import common_texts

if os.path.exists(&quot;word2vec.model&quot;):
    model &#x3D; Word2Vec.load(&quot;word2vec.model&quot;)
else: 
    model &#x3D; Word2Vec(
        sentences&#x3D;list(corpus_title) + list(train_title) + list(dev_title),
        vector_size&#x3D;128, # 赛题需要提交的维度
        window&#x3D;5,
        min_count&#x3D;1,
        workers&#x3D;4,
    )
    model.save(&quot;word2vec.model&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="0x02-初探词向量"><a href="#0x02-初探词向量" class="headerlink" title="0x02 初探词向量"></a>0x02 初探词向量</h2><p>经过上面的步骤，我们已经获得了数据集中所有词的词向量。</p>
<h3 id="计算与-格力-相似的-Top10-单词"><a href="#计算与-格力-相似的-Top10-单词" class="headerlink" title="计算与 格力 相似的 Top10 单词"></a>计算与 <code>格力</code> 相似的 Top10 单词</h3><p>下面可以先来看一下一个词的词向量，比如说“格力”：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model.wv[&quot;格力&quot;], model.wv[&quot;格力&quot;].shape<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203221506144.png" alt="|500"></p>
<p>可以看到，输出就是一个 128 维的连续向量。</p>
<p>再来找一下与格力相似的 Top 10 单词：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model.wv.most_similar(&quot;格力&quot;)

&#39;&#39;&#39;
输出：
[(&#39;奥克斯&#39;, 0.8410876989364624),
 (&#39;GREE&#39;, 0.8224707245826721),
 (&#39;柜机&#39;, 0.8208969235420227),
 (&#39;海尔&#39;, 0.8145878911018372),
 (&#39;美的&#39;, 0.8133372664451599),
 (&#39;变频空调&#39;, 0.8063334226608276),
 (&#39;1p1.5&#39;, 0.790867269039154),
 (&#39;挂机&#39;, 0.7852355241775513),
 (&#39;中央空调&#39;, 0.7766402959823608),
 (&#39;新飞&#39;, 0.7650886178016663)]
&#39;&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="获取词向量的-Index"><a href="#获取词向量的-Index" class="headerlink" title="获取词向量的 Index"></a>获取词向量的 Index</h3><p>这是一种 NLP 建模中比较常用的手段，可以得到每个词向量的 ID。因为将这些词（字符串）映射为 ID，由 ID 来进行操作会方便一些。也可以不进行这一步操作。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 通过 index 看一下词向量的前十条是什么
model.wv.index_to_key[0:10]
&#39;&#39;&#39;
输出：
[&#39; &#39;, &#39;新款&#39;, &#39;女&#39;, &#39;&#x2F;&#39;, &#39;2021&#39;, &#39;-&#39;, &#39;加厚&#39;, &#39;儿童&#39;, &#39;秋冬&#39;, &#39;外套&#39;]
&#39;&#39;&#39;

# 找一下“女”这个词的 index
model.wv.key_to_index[&quot;女&quot;]
&#39;&#39;&#39;
输出：
2
&#39;&#39;&#39;
# 可以发现跟之前的是对应的<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>既然 <code>index</code> 可以索引到词，那么我们再来验证一下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 看一下“格力”的 index
model.wv.key_to_index[&quot;格力&quot;]
&#39;&#39;&#39;
输出：
5024
&#39;&#39;&#39;

# 输入“格力”的 index，看看与之前打印的“格力”的词向量是否一样
model.wv[5024], model.wv[5024].shape
# 结果是一样的，可以自行测试<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>获取数据集的 <code>index</code> ：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 从单词 -&gt; id
# 我 爱 阿水
# 2 230 50
train_w2v_ids &#x3D; [[model.wv.key_to_index[xx] for xx in x] for x in train_title]
corpus_w2v_ids &#x3D; [[model.wv.key_to_index[xx] for xx in x] for x in corpus_title]
dev_w2v_ids &#x3D; [[model.wv.key_to_index[xx] for xx in x] for x in dev_title]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="0x03-使用词向量完成句子编码"><a href="#0x03-使用词向量完成句子编码" class="headerlink" title="0x03 使用词向量完成句子编码"></a>0x03 使用词向量完成句子编码</h2><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>先使用 TF-IDF 计算一下词的重要性（区分力），识别 query 与 doc 中哪些词是不重要的。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

idf &#x3D; TfidfVectorizer(analyzer&#x3D;lambda x: x)
idf.fit(train_title + corpus_title)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">idf.idf_, len(idf.idf_)
&#39;&#39;&#39;
输出：
(array([ 2.46292242,  8.5771301 ,  7.7050655 , ..., 14.21903717,
        14.21903717, 14.21903717]),
 640554)
&#39;&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看见已经计算出了每个词的 TF-IDF。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">token &#x3D; np.array(idf.get_feature_names())

# 选出需要去除的词
# 分数为什么设置小于 10，来自水哥的先验经验
drop_token &#x3D; token[np.where(idf.idf_ &lt; 10)[0]]  # 统计分数小于 10 的词
drop_token &#x3D; list(set(drop_token))
drop_token +&#x3D; [&#39;领券&#39;]

# 得到不重要的单词的 index，以便后面过滤
drop_token_ids &#x3D; [model.wv.key_to_index[x] for x in drop_token]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="句子编码"><a href="#句子编码" class="headerlink" title="句子编码"></a>句子编码</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">def unsuper_w2c_encoding(s, pooling&#x3D;&quot;max&quot;):
    feat &#x3D; []
    corpus_query_word &#x3D; [x for x in s if x not in drop_token_ids]  # 将上一步中得到的不重要的（没有区分力的）词过滤
    if len(corpus_query_word) &#x3D;&#x3D; 0:
        return np.zeros(128)
    
    # 获取句子的词向量
    # N * 128 的矩阵
    # N 是每条句子中筛去不重要的词，剩下的词的数量
    # 这一步得到的是一个矩阵，当我们最终需要的是一个句向量
    # 所以下一步就是把矩阵处理成向量
    feat &#x3D; model.wv[corpus_query_word]

    # 通过 pooling 得到句子的词向量
    # 通过池化将矩阵降维到一个 128 维向量
    if pooling &#x3D;&#x3D; &quot;max&quot;:
        return np.array(feat).max(0)  # 对每一列取最大值，即最大池化
    if pooling &#x3D;&#x3D; &quot;avg&quot;:
        return np.array(feat).mean(0)  # 对每一列取均值，即均值池化<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 用第一条数据测试一下
unsuper_w2c_encoding(train_w2v_ids[0]).shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>(128,)</p>
</blockquote>
<p>可以看到已经得到了 128 维的句向量。</p>
<p>那么对整个数据集进行句向量编码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from tqdm import tqdm_notebook
# [corpus_w2v_ids[x] for x in qrels[&#39;doc&#39;].values[:100] - 1]

corpus_mean_feat &#x3D; [
    unsuper_w2c_encoding(s) for s in tqdm_notebook(corpus_w2v_ids[:1000])
]
corpus_mean_feat &#x3D; np.vstack(corpus_mean_feat)

train_mean_feat &#x3D; [
    unsuper_w2c_encoding(s) for s in tqdm_notebook(train_w2v_ids[:100])
]
train_mean_feat &#x3D; np.vstack(train_mean_feat)

dev_mean_feat &#x3D; [
    unsuper_w2c_encoding(s) for s in tqdm_notebook(dev_w2v_ids[:100])
]
dev_mean_feat &#x3D; np.vstack(dev_mean_feat)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img data-src=" https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203221710025.png" style="zoom: 67%;" />

<p>这里只用 <code>corpus</code> 的前 1000 条，<code>train</code> 和 <code>dev</code> 的前 100 条作为演示。对整个数据集编码的话，去掉切片即可。</p>
<h2 id="0x04-检索"><a href="#0x04-检索" class="headerlink" title="0x04 检索"></a>0x04 检索</h2><p>既然已经计算得到的词向量，那么就可以用词向量来计算 <code>train.query.txt</code> 和 <code>corpus.tsv</code> 文本的相似度，然后再做一个相似度排序就可以实现检索。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.preprocessing import normalize

# 归一化
corpus_mean_feat &#x3D; normalize(corpus_mean_feat)
train_mean_feat &#x3D; normalize(train_mean_feat)
dev_mean_feat &#x3D; normalize(dev_mean_feat)

# 计算 Loss
mrr &#x3D; []
for idx in tqdm_notebook(range(1, 10)):
    # 首先计算 train 与 corpus 的相似度，然后排序
    dis &#x3D; np.dot(train_mean_feat[idx - 1], corpus_mean_feat.T)
    #print(dis)
    ids &#x3D; np.argsort(dis)[::1]
    #print(ids)
    
    print(train_title[idx-1], corpus_data.loc[qrels.loc[idx].ravel()[0]][&quot;title&quot;],  dis[qrels.loc[idx].ravel()-1])
    print(corpus_title[ids[0]])

    # 计算每个检索的 MRR Loss（赛题中评价指标是用的 MRR）
    mrr.append(1&#x2F;(np.where(ids &#x3D;&#x3D; qrels.loc[idx].ravel()[0] - 1)[0][0] + 1))
    print(&#39;&#39;)

# 打印平均 MRR
print(np.mean(mrr))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>&#96;[‘美赞臣’, ‘亲舒’, ‘一段’] 领券满减】美赞臣安婴儿A+亲舒婴儿奶粉1段850克 0-12个月宝宝 [0.68474327]<br>[‘现货’, ‘加拿大’, ‘美赞臣’, ‘1’, ‘段’, ‘EnfamilA’, ‘+’, ‘一段’, ‘DHA’, ‘奶粉’, ‘765’, ‘克’, ‘超值’, ‘装金装’]</p>
<p>[‘慱’, ‘朗’, ‘手动’, ‘料理’, ‘机’] Braun&#x2F;博朗 MQ3035&#x2F;3000&#x2F;5025料理棒手持小型婴儿辅食家用搅拌机 [0.71025692]<br>[‘朗’, ‘诗’, ‘LS22A1203’, ‘时尚’, ‘气质’, ‘休闲’, ‘百搭显’, ‘瘦’, ‘拼色’, ‘假’, ‘两件’, ‘衬衣’, ‘小衫’, ‘2022’, ‘春装’]</p>
<p>[‘電力貓’] 小米WiFi电力猫无线路由器套装一对300M穿墙宝家用信号增强扩展器 [0.03024337]<br>[‘[‘, ‘新华书店’, ‘]’, ‘ ‘, ‘党委会’, ‘的’, ‘工作’, ‘方法’, ‘ ‘, ‘毛’]&#96;</p>
</blockquote>
<blockquote>
<p>MRR &#x3D; 0.01528688737011513</p>
</blockquote>
<p>这里只展示部分检索结果，可以看到有些j检索结果并不好，有些品牌的分词也不太对。所以后面还可以考虑去掉一些停用词或者标点符号之类的数据清洗手段，达到更好的效果。毕竟数据为王，数据科学中最重要，最耗时的一步其实就是处理数据的工作。</p>
<p>最后将词向量存储到本地：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">with open(&#39;query_embedding&#39;, &#39;w&#39;) as up :
    for id, feat in zip(dev_data.index, dev_mean_feat):
        up.write(&#39;&#123;0&#125;\t&#123;1&#125;\n&#39;.format(id, &#39;,&#39;.join([str(x)[:6] for x in feat])))
        
with open(&#39;doc_embedding&#39;, &#39;w&#39;) as up :
    for id, feat in zip(corpus_data.index, corpus_mean_feat):
        up.write(&#39;&#123;0&#125;\t&#123;1&#125;\n&#39;.format(id, &#39;,&#39;.join([str(x)[:6] for x in feat])))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>水哥讲的太好了，对我这种刚入门 NLP 的小白帮助很大，学到了 NLP 完整的一套训练流程。</p>
<p>感觉句子编码 <code>pooling</code> 的时候好像也可以做一些技巧。目前是直接用 Word2Vec 得到的词向量，这个比赛本质上是看谁的词向量训练的好，所以后面尝试用 <code>SimCSE</code> 这些深度模型来训练，效果应该会提升一些。当然了，提分的话首先还是从数据入手，比如 <code>corpus</code> 的 <code>title</code> 长度很长，会有一百多个字符，而 <code>query</code> 就是比较短的搜索。所以 <code>corpus</code> 里的那些非核心词（噪声）势必会对训练产生影响。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/ECommerceSearch">team-learning-data-mining&#x2F;ECommerceSearch at master · datawhalechina&#x2F;team-learning-data-mining · GitHub</a></p>
<p><a href="https://www.bilibili.com/video/BV1dU4y1R7a2?spm_id_from=333.999.0.0">天池大神阿水解读：阿里灵杰电商搜索算法赛 | bilibili</a></p>
]]></content>
      <categories>
        <category>“阿里灵杰”问天引擎电商搜索算法赛</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
