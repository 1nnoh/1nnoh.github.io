<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>矢量语义与嵌入之 TF-IDF 检索</title>
    <url>/280EQA3/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>自然语言处理就是研究如何让计算机理解人类语言的一门技术。将计算机不能理解的人类语言编码成可以理解的向量化表示，是 NLP 工作的开始与基础。TF-IDF 是 NLP 入门的基础知识。通过对这种编码方式的学习，可以使我们更加容易理解 NLP 工作的本质。</p>
<span id="more"></span>

<blockquote>
<p>这块内容第一次学的时候比较难理解，后来看到<a href="https://mofanpy.com/tutorials/machine-learning/nlp/intro-search/">莫烦老师</a>讲的才算是彻底明白。补充一下这块内容。</p>
</blockquote>
<p>NLP 的首要任务：Vector Semantics and Embedding，也可以说是文本向量化，或着文本特征提取。想要表达的就是，NLP 首先要做的是，将文本语言变成计算机可以理解的语言——向量，否则后面的工作自然是无法开展的。</p>
<p>在文本向量化的问题中，词袋模型（Bag-of-Words Model）和词嵌入（Word Embedding）是两种最常用的模型。更准确地说，词向量只能表征单个词，如果要表示文本，需要做一些额外的处理。本篇笔记首先学习词袋模型与 TF-IDF。</p>
<h2 id="0x01-TF-IDF-的使用场景"><a href="#0x01-TF-IDF-的使用场景" class="headerlink" title="0x01 TF-IDF 的使用场景"></a>0x01 <code>TF-IDF</code> 的使用场景</h2><p>以搜索引擎为例，有了批量性地召回相对合适的内容后，比如我已经从 1 亿个网页中召回了 100 万个，但 100 万对于我来说，已经够让我看上好几年了。怎么能再继续提升一下精确度，找到我更在乎的内容呢？</p>
<p>所以需要对召回的这 100 万个内容，做一个【问题与内容】的相似度排序，只返回那些头部内容（问题指的是用户搜索的词条，内容就是我们召回的网页）。</p>
<p>因此简单来说，<code>TF-IDF</code> 要做的就是相似度排序，我们取相似度最高的内容返回给用户。从本质上来说，<code>TF-IDF</code> 是一种 <strong>向量表达</strong> 把语言 <strong>向量化</strong>，将词语，句子，文章转为词向量，句向量，文章向量。因为计算机只能理解数字，所以把我们要 <strong>把计算机不能理解的文本转为向量</strong>。等看完后面的内容就会深刻理解为什么 <code>TF-IDF</code> 是 <strong>向量表达</strong>。</p>
<h2 id="0x02-TF-IDF-的原理"><a href="#0x02-TF-IDF-的原理" class="headerlink" title="0x02 TF-IDF 的原理"></a>0x02 <code>TF-IDF</code> 的原理</h2><p><code>TF</code> ：词频（Term Frequency）<br><code>IDF</code> ：逆文本频率指数（Inverse Document Frequency）</p>
<ul>
<li>词频 <code>TF</code> ：反应文章的 <strong>局部信息</strong>。<ul>
<li>在一篇文章中，越重要的内容，出现（强调）的次数越多，那么词频 <code>TF</code> 就会越高。所以这些高词频的词，就可以代表这篇文章。</li>
<li>但伴随而来的问题是，文章中许多的语气词或者“你我他”这种词或者标点符号，同样也会出现很多次，但这些词往往也是高频词，但是没有意义。如何解决这种情况？那就需要 <code>IDF</code>。</li>
</ul>
</li>
<li>逆文本频率指数 <code>IDF</code> ：反应系统的 <strong>全局信息</strong>。<ul>
<li><code>IDF</code> 可以帮助我们判断词语在系统中的 <strong>区分力</strong> 大小。<ul>
<li>比如，如果 <strong>每篇文章</strong> 中都有“我”，那么它在所有文章中的 <strong>区分力都不强</strong>。</li>
<li>如果你搜索的关键词是“莫烦”，<strong>全网都没有几个</strong> 叫“莫烦”的，那么“莫烦” <code>IDF</code> 就会很大，即“莫烦”的 <strong>区分力更强</strong>。</li>
</ul>
</li>
<li>既然 <code>TF</code> 是以单篇文章为中心的局部词信息，但并不知道如果放到全局（所有文章），哪些 <code>TF</code> 高频词是全局垃圾词（中性词），是搜索时没有意义的词； <code>IDF</code> 是统计所有文章的全局词信息，可以分辨 <code>TF</code> 中的哪些高频词是全局垃圾词，但并不知道每个单篇文章中的高频关键词。那么为什么不把两种指标结合，发挥各自的优势？因此通过 <code>TF-IDF</code> 来表达一篇文章。</li>
</ul>
</li>
<li><code>TF-IDF</code> : <code>TF</code> × <code>IDF</code><ul>
<li>将两种指数相乘，得到 <code>TF-IDF</code> 表达一篇文章。</li>
<li>降低没有意义的词的重要性，突出文章中真正具有关键意义的内容（词语）。</li>
</ul>
</li>
</ul>
<p>举个例子：比如有以下三篇文章，我们选取图中的四个词来表达这三篇文章。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203211003678.png" alt="|700"></p>
<p>计算出这四个词的 <code>TF</code> 与 <code>IDF</code> 之后，将两者相乘，得到 <code>TF-IDF</code>。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203211033747.png"></p>
<p>通过这些词的分数高低，可以判断每篇文章的关键词是什么。比如文章 1，即使“爱”的词频最高，是文章 1 中出现最多的词，但是通过 <code>IDF</code> 判断得知是全局垃圾词，反而“莫烦”的全局区分力更强（即使词频不高），所以根据综合评分 <code>TF-IDF</code> 得知——在文章 1 中，“莫烦” 的权值更重，更能代表这篇文章。</p>
<p>至此，我们得到了三篇文章的向量化表示，那么如何在搜索中应用这三个文章向量呢？</p>
<h2 id="0x03-TF-IDF-的应用"><a href="#0x03-TF-IDF-的应用" class="headerlink" title="0x03 TF-IDF 的应用"></a>0x03 <code>TF-IDF</code> 的应用</h2><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203211047534.png"></p>
<p>假设我们搜索“莫烦 Python”，计算机首先通过词表的模式，计算这个搜索问句（“莫烦 Python”）的 <code>TF-IDF</code> 值。然后计算搜索问句与每篇文章的 <code>TF-IDF</code> 值的 <code>cosine</code> 距离。简单来说就是将所有文章，按照词向量的维度放入四维空间（对应四个词），然后将搜索问句向量也放进去，最后查找哪一篇文章离这个搜索问句最近，越近说明相似度越高。由此找到与搜索问句最匹配的文章。图中是用三维空间来说明，一样的意思。</p>
<p>动手实操一下吧： <a href="https://radimrehurek.com/gensim/models/tfidfmodel.html">models.tfidfmodel – TF-IDF model — gensim</a></p>
<h2 id="0x04-向量化表达"><a href="#0x04-向量化表达" class="headerlink" title="0x04 向量化表达"></a>0x04 向量化表达</h2><p>向量化是 NLP 工作的基础。将问句，词语，句子或者文章，通过数字的形式投射到空间中，也就是将这些语言转为计算机可以理解的向量，然后按照向量的模式指向空间中的某个位置。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203211101372.png"></p>
<p>如图，比如 <code>0,23,116.5,21</code> 就是文章 1 的向量表达，下面是文章 2 与搜索问题的向量表达。所以 <code>TF-IDF</code> 本质上是一种向量表达。NLP 中还有许多其他的向量表达与应用，<code>TF-IDF</code> 是其中一种比较基础且好用的一种方式。</p>
<p>在实际应用中，一般不存在的词是不放入每条数据的词袋的，通过这样的方式来节约内存。这种技术叫稀疏矩阵（Sparse Matrix）：只存储有内容的值，而忽略无内容的值。因此词袋模型的文本表征，其实是一种稀疏向量表达，因为向量里面的元素大部分都是 0，只有出现在当前文本的词语才有词频的赋值。</p>
<blockquote>
<p>拓展：<br><code>TF-IDF</code> 就是一张将 <code>词语重要程度</code> 转换成 <code>向量</code> 的文档展示方式，那么在这些向量中，必定会有主导型元素，而这些元素其实就是这篇文档中很重要的关键词了。因此除了搜索匹配之外，<code>TF-IDF</code> 还有很多的应用，比如将挑选文档中的关键词（将主导元素提取出来）。<br>另外，由于 <code>IDF</code> 是所有文档的全局信息，那么带有不同属性的文档集群可能拥有不同性质的 <code>IDF</code> 分布。比如金融领域的 <code>IDF</code> 与生物领域的 <code>IDF</code> ，如果我要搜索金融相关的信息，却是在生物领域的 <code>IDF</code> 下搜索，那么得到的结果必然是不准确的。因此我需要一个带有金融属性的 <code>IDF</code> 表来优化对金融子领域的搜索。这也是 IDF 比较重要的应用方式之一。</p>
</blockquote>
<h2 id="0x05-词袋模型与-TF-IDF"><a href="#0x05-词袋模型与-TF-IDF" class="headerlink" title="0x05 词袋模型与 TF-IDF"></a>0x05 词袋模型与 <code>TF-IDF</code></h2><p>词袋（BoW）模型是数字文本表示的最简单形式。像单词本身一样，我们可以将一个句子表示为一个词向量包（一个数字串）。初入 NLP 可能会对这些概念产生困惑，比如什么是词袋模型，<code>TF-IDF</code> 就是词袋模型吗，下面来解释一下。</p>
<h3 id="词集与词袋模型"><a href="#词集与词袋模型" class="headerlink" title="词集与词袋模型"></a>词集与词袋模型</h3><p>词袋模型将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。这个模型的主要作用也就是对文本做单词切分，有点从一篇文章里提取关键词这种意思，旨在 <strong>用向量来描述文本的主要内容</strong>，其中包含了词集与词袋两种。</p>
<p><strong>词集模型</strong>：单词构成的集合，集合中每个元素只有一个，即词集中的每个元素为一个单词，词集中的每个单词只出现一次，不重复。</p>
<p><strong>词袋模型</strong>：在词集的基础上加入了频率这个维度，即统计单词在文档中出现的次数（令牌化和出现频数统计），通常我们在应用中都选用词袋模型。或者说，在词集的基础上，如果一个单词在文档中出现不止一次，统计其出现的次数。说人话就是，将每篇文章看成一袋子词，并忽略每个词出现的顺序。</p>
<h3 id="词袋模型与-TF-IDF-联合使用"><a href="#词袋模型与-TF-IDF-联合使用" class="headerlink" title="词袋模型与 TF-IDF 联合使用"></a>词袋模型与 <code>TF-IDF</code> 联合使用</h3><p>词袋创建一组向量，其中包含文档中的单词出现次数，而 <code>TF-IDF</code> 编码可以识别其中每个单词的区分力，然后赋予权重。因此通过词袋模型是一种基础模型， <code>TF-IDF</code> 是一种编码方式，基于词袋模型，我们可以采用 <code>TF-IDF</code> 编码，也可以使用 One-Hot 编码或者其他的编码方式。</p>
<p>我理解就是，恰好词袋模型实现了词频统计，恰好这正是 <code>TF</code> 要做的事情，所以再加上 <code>IDF</code> 联合起来，基于词袋模型实现了我们要做的  <code>TF-IDF</code> 编码。</p>
<h2 id="0x06-文本向量化"><a href="#0x06-文本向量化" class="headerlink" title="0x06 文本向量化"></a>0x06 文本向量化</h2><p>刚入门 NLP 想必会对这些专业名词头晕眼花，下面简单列举一下关系：</p>
<ul>
<li>文本向量化（文本表征 Word Representation）<ul>
<li>词袋模型及其编码方法（BoW）<ul>
<li>One-Hot 编码</li>
<li>TF 编码</li>
<li>TF-IDF 编码</li>
<li>N-gram 编码</li>
</ul>
</li>
<li>词嵌入模型（Word Embedding）<ul>
<li>Word2Vec<ul>
<li>Skip-Gram 模型</li>
<li>CBOW 模型</li>
</ul>
</li>
<li>GloVe</li>
</ul>
</li>
<li>主题模型（Topic Model）<ul>
<li>LSA 模型</li>
<li>PLSA 模型</li>
<li>LDA 模型</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Bag-of-Words Model（BoW）和 <code>TF-IDF</code> 编码都是帮助我们将文本句子转换为向量的技术，是使得机器理解文本的技术。</p>
<p>但是 BoW 这样的技术会有一些弊端，比如语序关系丢失（忽略上下文），异常依赖于优秀的词汇库，缺乏相似词之间的表达，并且向量稀疏。为了解决这些问题，下一篇笔记学习另一类向量化的方法——Word Embedding 词嵌入。即自然语言中的词语转化为稠密的向量，相似的词会有相似的向量表示，这样的转化方便挖掘文字中词语和句子之间的特征。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>TF-IDF:<br> <a href="https://blog.csdn.net/weixin_44799217/article/details/116423520">自然语言处理(NLP)之使用 TF-IDF 模型计算文本相似度</a><br> <a href="https://mofanpy.com/tutorials/machine-learning/nlp/intro-search/">你天天用的搜索引擎是怎么工作的 - 自然语言处理 | 莫烦 Python</a><br> <a href="https://mofanpy.com/tutorials/machine-learning/nlp/tfidf/">统计学让搜索速度起飞 - 自然语言处理 | 莫烦 Python</a></p>
<p>词袋模型与 TF-IDF:<br> <a href="https://www.jianshu.com/p/0422853b57a8">词袋模型与 TF-IDF</a><br> <a href="https://blog.csdn.net/fendouaini/article/details/108655680">词袋模型和 TF-IDF</a><br> <a href="https://blog.csdn.net/baidu_41797613/article/details/121268152">词袋模型与 TF-IDF 模型</a><br> <a href="https://www.jiqizhixin.com/graph/technologies/87c62b00-48b2-4e2a-8122-9876a3d3e59e">词袋模型 | 机器之心</a></p>
<p> 词袋模型与词嵌入：<br>  <a href="https://davidchen93.blog.csdn.net/article/details/79993369?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_default&utm_relevant_index=1">词袋模型和词向量模型概念介绍</a><br>  <a href="https://zhuanlan.zhihu.com/p/71065945">从词袋模型 TF-IDF 到词嵌入 Word Embedding</a></p>
<p>斯坦福经典 NLP 教材：<br>  <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">https://web.stanford.edu/~jurafsky&#x2F;slp3&#x2F;6.pdf</a></p>
]]></content>
      <categories>
        <category>Dive into NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>矢量语义与编码之词嵌入</title>
    <url>/374ZH81/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>上篇笔记中，我们学习了关于文本表征基于词袋模型得到稀疏向量化文本的内容，比如 One-Hot 编码，比如 TF-IDF 编码。本质上来说，词袋模型其实是一种通过统计学方法，将文档中出现的所有词语根据词频等统计指标，<strong>转为稀疏向量</strong>，从而表达一篇文章的方法。但是，这种稀疏向量的表达会存在一些缺点，比如不考虑语序等等。那么有没有办法得到文本向量化的稠密表达呢？这就是本篇笔记要讲的，Word Embedding 词嵌入。</p>
<span id="more"></span>

<h2 id="0x01-什么是词嵌入与词向量"><a href="#0x01-什么是词嵌入与词向量" class="headerlink" title="0x01 什么是词嵌入与词向量"></a>0x01 什么是词嵌入与词向量</h2><p><strong>Word Embedding 词嵌入</strong> 是一种 <strong>考虑词语位置关系</strong> 的文本表征模型。词嵌入将文本中的词转换成<strong>稠密的低维数字向量</strong>的表达。低维且稠密的特点提高了网络的学习效率，并且有利于学习词语之间的相似性。</p>
<p>也就是说，通过学习文本，用词向量的方式表征词的语义信息，即将所有的词，嵌入（Embedding）一个较低维度的连续向量空间中，词嵌入的结果就生成了 <strong>Word Vector 词向量</strong>。Embedding 其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。</p>
<p>之所以说是较低维度，是因为词向量与词袋模型得到的稀疏向量相比，维度是低的多的，对于词袋模型来说，每出现一个词即增加一维向量，假设字典中有 4000 个词，那么就需要 4000 维向量来表达；而词向量所在的连续空间，可以指定维度，可以是 128 维，也可以是 200 维，或者是其他自定义的维度。</p>
<p>在词向量的连续空间中，词嵌入使得语义上相似的单词在该空间内距离很近，语义不同的单词之间距离更远。词与词之间的相似度（关系），或者说距离，可以通过 <code>cosine</code> 求得。</p>
<p>听起来好像很复杂，其实只需要明白，区别于之前的词袋模型那种<strong>稀疏向量表达</strong>：通常都是一个高维的向量，向量里面的元素大部分都是 0，然后通过稀疏矩阵存储，去除没有意义的元素。所以造成每个文档的向量表达的长度是不一样的，也就是他们的向量表达维度是不一样的，不固定的，自然也是不连续的。</p>
<p>而词嵌入也是用一个向量来表示一个词，但是它是使用一个<strong>较低的维度，稠密地表示</strong>。或者说，词嵌入固定了一个维度（规定好一个连续空间），每个词的都有这么多维度，每个维度都会计算出一个数值，然后每一个词，都通过这样一个固定维度的连续向量来表征。</p>
<p>以 <code>hello</code> 这个词语为例。</p>
<p>稀疏向量表示：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203212325688.png"></p>
<p>稠密向量表示（嵌入表示）：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203212326164.png"></p>
<h2 id="0x02-词嵌入模型背景"><a href="#0x02-词嵌入模型背景" class="headerlink" title="0x02 词嵌入模型背景"></a>0x02 词嵌入模型背景</h2><h3 id="1-词嵌入的优势"><a href="#1-词嵌入的优势" class="headerlink" title="1. 词嵌入的优势"></a>1. 词嵌入的优势</h3><p>上篇笔记讲到的文本表征方法——词袋模型有以下缺点：</p>
<ul>
<li>无法表达词语之间的关系，不考虑词语的<strong>有序性</strong>。</li>
<li>对每个词来说，自身是一个独热向量（相互正交），无法编码词之间的<strong>相似性</strong>。</li>
<li>过于稀疏的向量表达，导致计算和存储的效率都不高。可以想象，当我们的词表 Vocab 增大到十万时，每个词都需要 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="7.15ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 3160.4 688"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mn" transform="translate(1444.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g><g data-mml-node="mi" transform="translate(2444.4,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container> 的向量来表达，严重浪费了内存和计算资源。</li>
</ul>
<p>那么本篇笔记要介绍的文本表征方法——词嵌入有哪些优势呢？</p>
<ul>
<li>考虑了词语之间的（位置）关系，即考虑到词语的<strong>有序性</strong>。</li>
<li>语义相似的词在向量空间上也会比较相近，即可学习词语之间的<strong>相似性</strong>。</li>
<li>可以将文本通过一个低维稠密向量来表达，计算和储存效率相比于词袋模型要高。</li>
<li>通用性很强，可以用在不同的任务中。</li>
</ul>
<h3 id="2-两种主流的-Word-Embedding-算法"><a href="#2-两种主流的-Word-Embedding-算法" class="headerlink" title="2. 两种主流的 Word Embedding 算法"></a>2. 两种主流的 Word Embedding 算法</h3><p>目前主流的词嵌入技术有两种，一种是 Word2Vec，另一种是 GloVe。</p>
<ul>
<li>Word2Vec<ul>
<li>这是一种基于统计方法来获得词向量的方法，于 2013 年由谷歌的 Mikolov 提出的一套新的词嵌入方法。</li>
<li>这种算法有 2 种训练模式<ul>
<li><code>CBOW</code> 通过上下文来预测当前词</li>
<li><code>SKip-Gram</code> 通过当前词来预测上下文</li>
</ul>
</li>
</ul>
</li>
<li>GloVe (Globel Vectors)<ul>
<li>GloVe 是<strong>对 Word2Vec 的扩展</strong>，它将全局统计和 Word2vec 的基于上下文的学习结合了起来。</li>
<li>其实就是 SVD 分解与 Word2Vec 的结合。</li>
</ul>
</li>
</ul>
<p>本篇笔记主要学习 Word2Vec。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204280423986.png"></p>
<blockquote>
<p>Word2Vec 是一种可以进行高效率词嵌入学习的预测模型。其两种变体分别为：连续词袋模型（CBOW）及 Skip-Gram 模型。</p>
<p>从算法角度看，这两种方法非常相似，其区别为 CBOW 根据源词上下文词汇（’the cat sits on the’）来预测目标词汇（例如，‘mat’），而 Skip-Gram 模型做法相反，它通过目标词汇（例如，‘mat’）来预测源上下文词汇（’the cat sits on the’）。</p>
<p>Skip-Gram 模型采取 CBOW 的逆过程的动机在于：CBOW 算法对于很多分布式信息进行了平滑处理（例如将一整段上下文信息视为一个单一观察量）。很多情况下，对于小型的数据集，这一处理是有帮助的。相形之下，Skip-Gram 模型将每个“上下文-目标词汇”的组合视为一个新观察量，这种做法在大型数据集中会更为有效。</p>
</blockquote>
<h3 id="3-Word2Vec-的前身"><a href="#3-Word2Vec-的前身" class="headerlink" title="3. Word2Vec 的前身"></a>3. Word2Vec 的前身</h3><p>由于 One-Hot 编码存在两个重要缺陷：<strong>维度灾难</strong>与<strong>语义鸿沟</strong>。因此分布式向量横空出世，将词表示成 short dense vector，弥补了以上两个问题。而后引出了 NNLM (Neural Network Language Model)。</p>
<blockquote>
<p>“You shall know a word by the company it keeps”  (J. R. Firth 1957)</p>
</blockquote>
<h4 id="Neural-Network-Language-Model"><a href="#Neural-Network-Language-Model" class="headerlink" title="Neural Network Language Model"></a>Neural Network Language Model</h4><p>NNLM 出自 Yoshua Bengio 等人于 2003 年发表的《A Neural Probabilistic Language Model》，针对 N-gram 模型的问题进行了解决。这是第一篇提出神经网络语言模型的论文，它在得到语言模型的同时，<strong>也生成了副产品——词向量</strong>。</p>
<blockquote>
<p>什么是语言模型？</p>
<p>通俗解释：<strong>「语言模型就是判断一句话是不是人话，通常用概率来表示一句话是人话的可能性」</strong></p>
<p>标准定义：对于语言序列  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="14.029ex" height="1.441ex" role="img" focusable="false" viewBox="0 -443 6201 637"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1152.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1597.2,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2749.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(3194.4,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(4533.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4977.8,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>，语言模型就是计算该序列出现的概率，即 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="17.489ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7730 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1140,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(2292.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(2737.2,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(3889.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(4334.4,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(5673.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(6117.8,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(7341,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 。</p>
</blockquote>
<p><strong>和 N-gram 类似，NNLM 也假设当前词仅依赖于前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 个词。</strong></p>
<ul>
<li><p>神经网络语言模型（Neural Network Language Model）</p>
<ul>
<li>首次提出了 Word Embedding 的概念（虽然没有叫这个名字），从而奠定了包括 Word2Vec 在内后续研究的基础。</li>
<li>模型结构：<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204281556450.png" alt="|700"></li>
<li>NNLM 学习任务：输入某个句中单词 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="12.387ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 5475 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1560,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2338,0)"><g data-c="2036"><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z"></path><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z" transform="translate(275,0)"></path></g></g><g data-mml-node="mi" transform="translate(2888,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(3647,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(4113,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4564,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(4925,0)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g></g></g></svg></mjx-container> 前面句子的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="4.714ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2083.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(583.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1583.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 个单词，要求网络正确预测单词 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="4.609ex" height="1.57ex" role="img" focusable="false" viewBox="0 -683 2037 694"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(759,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1225,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1676,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></svg></mjx-container>。即最大化 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="4.609ex" height="1.57ex" role="img" focusable="false" viewBox="0 -683 2037 694"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(759,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1225,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1676,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></svg></mjx-container> 出现的条件概率。</li>
<li>NNLM 训练过程<ul>
<li>首先是一个线性的映射层：输入层。<ul>
<li>任意单词 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.875ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1271 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 用 One-Hot 编码（如：001000）作为原始单词输入。</li>
<li>输入 N 个 One-Hot 编码，通过一个共享的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="5.373ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 2375 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(828,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1606,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> 的矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container>，映射为 N 个分布式的词向量 (Distributed Vector)。即乘以矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> 获得词向量。</li>
<li>矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> 的维度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="5.373ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 2375 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(828,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1606,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> ，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> 是词典的大小，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.873ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 828 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g></g></g></svg></mjx-container> 是 Embedding 向量的维度（一个先验参数）。矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> 每一行内容对应着单词的 Word Embedding。该矩阵的内容通过学习获得。</li>
<li>为什么矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> 每一行内容正是每个单词的 Word Embedding 呢？因为当我们把 n 个 One-Hot 表征的词向量输入到神经网络中，单层网络进行的运算等效于查表操作，每个词的 One-Hot 编码将各自的词向量从 Embedding 层中原封不动地取出。如下图所示。<img data-src=" https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204280214399.png" style="zoom: 80%;"></li>
</ul>
</li>
<li>其次是一个简单的前向反馈神经网络。<ul>
<li>由一个激活函数为 tanh 的隐藏层和一个 Softmax 输出（分类）层组成，因此可以计算出在输入 context 的条件下，词典中所有 word 出现的条件概率。</li>
<li>即将上一步映射的词向量拼接，然后接隐藏层，再接 Softmax 预测后面应该接哪个词。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>手绘模型：<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204281649219.png" alt="|700"></p>
<ul>
<li>输入的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1380.6 833"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> 维度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.631ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 2047 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1278,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container> 是词典 Vocab 的大小。</li>
<li>输入层：获得词语 Embedding，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 为输入词个数，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.138ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 503 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g></g></g></svg></mjx-container> 为 Embedding 的维度。然后将所有 Embedding 拼接到一起。</li>
<li>隐藏层：长度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container>。</li>
<li>输出层：维度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.631ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 2047 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1278,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>。由 Softmax 激活得到归一化的概率。</li>
<li>模型参数：三个转移矩阵：词语 One-Hot 向量 =》输入层 =》隐藏层 =》输出层。其中由输入层到隐藏层的转移矩阵参数量是巨大的。</li>
</ul>
</li>
</ul>
<h2 id="0x03-Word2Vec-的两个模型"><a href="#0x03-Word2Vec-的两个模型" class="headerlink" title="0x03 Word2Vec  的两个模型"></a>0x03 Word2Vec  的两个模型</h2><h3 id="1-Word2Vec-之-CBOW-Continues-Bag-of-Words-Model"><a href="#1-Word2Vec-之-CBOW-Continues-Bag-of-Words-Model" class="headerlink" title="1. Word2Vec 之 CBOW (Continues Bag-of-Words Model)"></a>1. Word2Vec 之 CBOW (Continues Bag-of-Words Model)</h3><p><strong>思路：输入中间词前后的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></svg></mjx-container> 个词，预测中间词。</strong></p>
<blockquote>
<p>既然 NNLM 的复杂度主要来自隐藏层，那么 Word2Vec 提出了 CBOW 与 Skip-Gram，将 NNLM 的隐藏层拿掉，输入层不再对 Embedding 拼接，而是求平均。</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204280404098.png"></p>
<ul>
<li>手绘模型<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204281720104.png"><ul>
<li>对原始 NNLM 做简化，将 NNLM 中的隐藏层拿掉</li>
<li>CBOW 的隐藏层：获得上下文词语的 Embedding 后做平均，直接输出到 Softmax。不需要激活。</li>
</ul>
</li>
</ul>
<h3 id="2-Word2Vec-之-Skip-Gram"><a href="#2-Word2Vec-之-Skip-Gram" class="headerlink" title="2. Word2Vec 之 Skip-Gram"></a>2. Word2Vec 之 Skip-Gram</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204280406149.png"></p>
<p>实际情况中，Skip-Gram 用的较多。CBOW 在小数据集上表现较好，Skip-Gram 在大的数据集上表现更好。</p>
<ul>
<li>手绘模型<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204281820027.png"></li>
</ul>
<h2 id="0x04-Word2Vec-的两个提速手段"><a href="#0x04-Word2Vec-的两个提速手段" class="headerlink" title="0x04 Word2Vec 的两个提速手段"></a>0x04 Word2Vec 的两个提速手段</h2><p>然而，每当计算一个词的概率都要对词典里的 V 个词计算相似度，然后进行归一化，这基本上时不现实的。为此，Mikolov 引入了两个提速手段：层次 Softmax（Hierarchical Softmax）和负采样（Negative Sampling）。普遍认为 Hierarchical Softmax 对低频词效果较好；Negative Sampling 对高频词效果较好，向量维度较低时效果更好。</p>
<ul>
<li>Hierarchical Softmax</li>
<li>Nagative Sampling</li>
</ul>
<h2 id="0x05-Word2Vec-的优缺点"><a href="#0x05-Word2Vec-的优缺点" class="headerlink" title="0x05 Word2Vec 的优缺点"></a>0x05 Word2Vec 的优缺点</h2><ul>
<li>优点<ul>
<li><strong>考虑上下文信息</strong>，效果优于之前的 One-Hot 编码等 Embedding 方法。</li>
<li><strong>由稀疏长向量（基于词袋模型构建）转为稠密短向量。</strong> 较短长度的向量更有利于模型的学习，减少了需要学习的参数量。稠密的特点便于更好的捕捉相同语义。</li>
<li>通用性强，可以用在各种 NLP 任务中。</li>
</ul>
</li>
<li>缺点<ul>
<li>**Word2Vec 是静态编码 (Static Embedding)**，意味着这种方法只能学到一种固定的 Embedding，存在 Vocab 中。也就是说 <strong>Word2Vec 不能解决多义词的问题</strong>，不能联系上下文动态解释当前词语的含义。以后会介绍学习动态上下文嵌入 (Dynamic Contextual Embedding) 的方法，比如当下流行的 BERT 表示，每个词的 Vector 会根据不同的上下文内容而改变。</li>
<li>由于词和向量是一对一的关系，所以<strong>不能学习到词语的多层特性</strong>。一个好的语言表示除了建模一词多义现象以外，还需要能够体现词的复杂特性，包括语法 (syntax)、语义 (semantics) 等。Word2Vec 等词嵌入方法本身不具备这种优点——因为它太简单了。</li>
<li>仅考虑了局部语料，没有考虑到全局信息。</li>
<li>英文语料的分词简单，每个词为独立的个体。但中文语料首先需要解决分词问题，分词效果严重影响词向量的质量。因此 Word2Vec 对中文不是那么友好。</li>
</ul>
</li>
</ul>
<blockquote>
<p>需要说明的是：Word2vec 是上一代的产物（18 年之前）， 18 年之后想要得到最好的效果，已经不使用  Word2vec 这种方法。</p>
</blockquote>
<h2 id="0x06-从-Word-Embedding-到-BERT"><a href="#0x06-从-Word-Embedding-到-BERT" class="headerlink" title="0x06 从 Word Embedding 到 BERT"></a>0x06 从 Word Embedding 到 BERT</h2><blockquote>
<p>Word2Vec 和  BERT 都是语言表示中里程碑式的工作，前者是词嵌入范式的代表，后者是预训练范式的代表。</p>
</blockquote>
<ul>
<li><p>Word Embedding</p>
<ul>
<li>统计语言模型 N-gram<ul>
<li>解决无序性问题，用之前的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 个文本来计算当前文本 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="3.283ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1451.3 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 的条件概率。即<strong>考虑上文，前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 个词</strong>。</li>
<li>未解决文本表征问题，仍使用 One-Hot 向量来表示。由此带来一系列弊端，如维度灾难，无法描述词语语义相似度等。</li>
<li>由于模型复杂度和预测精度的限制，我们很少会考虑 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex;" xmlns="http://www.w3.org/2000/svg" width="6.157ex" height="1.636ex" role="img" focusable="false" viewBox="0 -683 2721.6 723"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(1165.8,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(2221.6,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></svg></mjx-container> 的模型。因此无法处理更长程的 context (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex;" xmlns="http://www.w3.org/2000/svg" width="6.157ex" height="1.636ex" role="img" focusable="false" viewBox="0 -683 2721.6 723"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(1165.8,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(2221.6,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></svg></mjx-container>)。</li>
<li>复杂度<ul>
<li>（需要计算每个词，和其他所有词，共同出现的次数） <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 500 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g></g></g></svg></mjx-container> （该词出现的词频）</li>
<li>如果词典大小为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g></svg></mjx-container>，考虑前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 个单词，则模型参数量级为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.766ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2990.5 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mi" transform="translate(975.3,363) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2601.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>。</li>
</ul>
</li>
</ul>
</li>
<li>分布式表征 (Distribution)<ul>
<li>分布假说 (Distributional Hypothesis)：上下文环境相似的两个词有着相近的语义。</li>
<li>分布式表征是指将数 <strong>据嵌入到低维连续空间</strong>，通常称这种表征为嵌入 (Embedding) 或向量 (Vector)。</li>
</ul>
</li>
<li>基于 SVD 的词向量方法<ul>
<li>词袋假说 (Bag of Words Hypothesis)：一篇文档的词频（而不是词序）代表了文档的主题。</li>
<li>使用奇异值分解 (SVD) 的方法对词袋模型生成的矩阵降维并且分解。</li>
</ul>
</li>
<li>神经网络语言模型（Neural Network Language Model）<ul>
<li>首次提出了 Word Embedding 的概念（虽然没有叫这个名字），从而奠定了包括 Word2Vec 在内后续研究的基础。</li>
<li><strong>解决了 N-gram 模型的两个重要缺陷（由 One-Hot 表示造成的）</strong>：维度灾难，无法描述词语语义相似度。</li>
<li><strong>考虑上文，前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2322.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 个词。</strong></li>
</ul>
</li>
<li>Word2Vec<ul>
<li>CBOW：从一个句子里面把一个词抠掉，用这个词的<strong>上文和下文</strong>去预测被抠掉的这个词。</li>
<li>Skip-Gram：输入某个单词，要求网络<strong>预测它的上下文单词</strong>。</li>
<li><strong>与 N-gram 的不同：Word2Vec 并不关心相邻单词之前一起出现的频数，而是仅仅关心，这个单词是不是属于另一个单词的上下文</strong>。也就是说，Word2Vec 不关心根据这个词预测出的下一个词语是什么，而是只关心这两个词语之间是不是有上下文关系。</li>
</ul>
</li>
<li>Glove<ul>
<li>对 Word2Vec 的补充和拓展，将全局统计融入 Word2Vec。</li>
</ul>
</li>
</ul>
</li>
<li><p>ELMO (Embedding from Language Models)：<strong>基于上下文</strong>的 Embedding</p>
<ul>
<li>Deep contextualized word representation. 根据当前上下文对 Word Embedding 动态调整。解决了一词多义的问题。</li>
<li>两阶段模型<ul>
<li>预训练阶段<ul>
<li>使用双层双向的 LSTM 作为特征抽取器。</li>
<li>预训练阶段获得三种 Embedding。<ul>
<li>最底层的词向量特征。</li>
<li>第一层双向 LSTM 获得的句法特征。</li>
<li>第二层双向 LSTM 获得的语义特征。</li>
</ul>
</li>
</ul>
</li>
<li>下游任务应用阶段<ul>
<li>做下游应用时作为特征提取器。将三层 Embedding 根据不同的权重拼接起来用于下游任务，作为新特征补充。</li>
<li>权重根据下游任务训练得到。</li>
</ul>
</li>
</ul>
</li>
<li>ELMO 的缺点<ul>
<li>LSTM 抽取特征能力远弱于 Transformer。</li>
<li>通过拼接方式获得融合特征，这种方式对特征融合的能力偏弱。</li>
</ul>
</li>
</ul>
</li>
<li><p>GPT (Generative Pre-Training)：Pretrain + Finetune</p>
<ul>
<li>两阶段模型<ul>
<li>预训练阶段<ul>
<li>使用 Transformer 作为特征抽取器，其特征抽取能力强于 RNN。</li>
<li>采用单向的语言模型，只采用单词的 Context-before 上文来进行预测，抛开了下文（现在来看是不明智的选择）。白白丢失了许多信息。</li>
</ul>
</li>
<li>微调阶段（下游任务应用阶段）<ul>
<li>将预训练模型应用到下游任务时，需保持模型结构和预训练模型一致。借此，通过预训练学到的语言学知识就被引入到当前的任务中来了。</li>
<li>以预训练模型参数作为初始参数，用当前的任务去训练这个网络，对网络参数进行 Fine-Tuning。</li>
</ul>
</li>
</ul>
</li>
<li>GPT 的缺点<ul>
<li>语言模型是单向的。没有考虑下文信息。</li>
</ul>
</li>
</ul>
</li>
<li><p>BERT：新星的诞生</p>
<ul>
<li>两阶段模型<ul>
<li>Pre-Training<ul>
<li>Transformer 作为特征抽取器。</li>
<li>双向的语言模型。</li>
<li>多任务训练<ul>
<li>Masked LM，基于掩码的语言模型。<ul>
<li>类似 CBOW，进行了一定改造。</li>
<li>将 15% 字符进行 mask 操作，其中 80% 进行真正的掩码，10% 随机替换成其他字符，剩下的 10% 保留真正的字符不变。</li>
</ul>
</li>
<li>Next Sentence Prediction<ul>
<li>为了使模型能够理解两个句子之间的关系。</li>
<li>将文档中连续的语句拼接起来作为正样本，其他还会随机选择两句话进行拼接作为负样本用于训练，主要是因为一些句子序列任务会需要。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Fine-Tuning</li>
</ul>
</li>
<li>与 ELMO 的区别<ul>
<li>ELMO 是分别看上文和下文，然后将上文得到的结果和下文得到的结果进行拼接。</li>
<li>BERT 是同时看上下文中的每个词，效果上也比 ELMO 要好。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Word2Vec 的基本理念是构造一个假的学习任务，我们并不关注这个任务的输出结果如何，而是关注它的中间产物。从 Word Embedding 到 BERT 其实都是预训练模型的发展与壮大。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>语言模型：</p>
<p><a href="https://blog.csdn.net/qq_44951759/article/details/120428103">《自然语言处理学习之路》 08 语言模型</a> ——这里解释了为什么 N-gram 模型计算量巨大</p>
<p><a href="https://zhuanlan.zhihu.com/p/195698474">【语言模型】从 N-gram 模型讲起 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/206878986">【语言模型】NNLM(神经网络语言模型) - 知乎</a></p>
<p><a href="https://www.bilibili.com/video/BV1Lb411p7FD?p=1">word2vec：神经语言模型(NNLM), CBOW, skip-gram_bilibili</a> ——手绘模型</p>
<p>Word2Vec:</p>
<p><a href="https://blog.csdn.net/Morganfs/article/details/124417008">基于深度学习的语言模型</a></p>
<p><a href="https://www.cnblogs.com/sandwichnlp/p/11596848.html#word2vec">词向量(one-hot/SVD/NNLM/Word2Vec/GloVe) - 西多士 NLP</a></p>
<p><a href="https://blog.csdn.net/nemoyy/article/details/80603438">word2vec: 理解 nnlm, cbow, skip-gram</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/27234078">理解 Word2Vec 之 Skip-Gram 模型 - 知乎</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/51648675">深度学习第 42 讲：自然语言处理之词嵌入和词向量</a></p>
<p><a href="https://blog.csdn.net/weixin_42468475/article/details/121522295">词袋模型和词嵌入模型</a></p>
<p><a href="https://easyai.tech/ai-definition/word-embedding/">一文看懂词嵌入</a></p>
<p><a href="https://www.jianshu.com/p/2fbd0dde8804">词嵌入</a></p>
<p><a href="https://blog.csdn.net/stupid_3/article/details/83184807">矢量语义——从 TF-IDF 到 Word2Vec 你所需要知道的一切！</a></p>
<p><a href="https://lilianweng.github.io/posts/2017-10-15-word-embedding/">Learning Word Embedding | Lil’Log</a></p>
<p><a href="https://mofanpy.com/tutorials/machine-learning/nlp/cbow/">Continuous Bag of Words (CBOW) - 自然语言处理 | 莫烦 Python</a></p>
<p><a href="https://www.bilibili.com/video/BV1fp4y147Lc?spm_id_from=333.337.search-card.all.click">RNN 模型与 NLP 应用(2/9)：文本处理与词嵌入_bilibili</a></p>
<p><a href="https://blog.csdn.net/itplus/article/details/37969519">word2vec 中的数学原理详解</a></p>
<p>Word2Vec拓展：</p>
<p><a href="https://www.zhihu.com/question/352468546">Bert 比之 Word2Vec,有哪些进步呢？</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/49271699">从 Word Embedding 到 Bert 模型—自然语言处理中的预训练技术发展史</a> ⭐⭐⭐</p>
<p><a href="https://zhuanlan.zhihu.com/p/105989051">广告行业中那些趣事系列 3：NLP 中的巨星 BERT</a> ⭐⭐⭐——分析了 BERT 如何吸取各种模型精华于一身，从而构造出这样一颗新星。</p>
]]></content>
      <categories>
        <category>Dive into NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>可解释机器学习-Task01导论</title>
    <url>/3W8VFN4/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>从以下角度来认识可解释机器学习：</p>
<ul>
<li>什么是可解释人工智能？</li>
<li>学可解释机器学习有什么用？</li>
<li>如何对传统机器学习、深度学习做可解释性分析？</li>
</ul>
<span id="more"></span>

<h2 id="0x01-什么是可解释机器学习"><a href="#0x01-什么是可解释机器学习" class="headerlink" title="0x01 什么是可解释机器学习"></a>0x01 什么是可解释机器学习</h2><p><strong>是打开人工智能的黑箱子，洞悉人工智能的脑回路与注意力，进而解释它，了解它，改进它，信赖他的一门学科。</strong></p>
<h3 id="1-人工智能黑箱子灵魂之问"><a href="#1-人工智能黑箱子灵魂之问" class="headerlink" title="1. 人工智能黑箱子灵魂之问"></a>1. 人工智能黑箱子灵魂之问</h3><ul>
<li>AI 的脑回路是怎样的？AI 如何做出决策？是否符合人类的直觉和常识？</li>
<li>AI 会重点关注哪些特征，这些特征是不是真的有用？</li>
<li>如何衡量不同特征对 AI 预测结果的不同贡献？</li>
<li>Al 什么时候 work, 什么时候不 work?</li>
<li>AI 有没有过拟合？泛化能力如何？</li>
<li>会不会被黑客误导，让 AI 指鹿为马？</li>
<li>如果样本的某个特征变大 15，会对 AI 预测结果产生什么影响？</li>
<li>如果 AI 误判，为什么会犯错？如何能不犯错？</li>
<li>两个 AI 预测结果不同，该信哪一个？</li>
<li>能让 AI 把学到的特征教给人类吗？</li>
</ul>
<h3 id="2-黑箱子案例："><a href="#2-黑箱子案例：" class="headerlink" title="2. 黑箱子案例："></a>2. 黑箱子案例：</h3><ul>
<li>AI 在关注哪些区域，哪里的特征对于 AI 识别坦克来说最重要<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/20221213230806.png"></li>
<li>将一张大熊猫的照片，加上一些看似随机的噪声后，AI 以较高的置信度将其识别为长臂猿（即使在人类看来没有很多区别，明显仍然是一个熊猫）<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212132315806.png"></li>
<li>AI 绘画的偏差，经不起严谨的推敲：比如要求画出游在溪水中的三文鱼，AI 将日料店里的三文鱼刺身画在了溪水里；或者只能知道大概模样的握手照片。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212132323534.png"></li>
</ul>
<p>因此目前的 AI 大多是黑箱，而且是经常会犯错误的黑箱，如果在医疗、无人驾驶、金融等领域，如何才能放心的将身家性命托付给人工智能？</p>
<h2 id="0x02-为什么要学可解释机器学习"><a href="#0x02-为什么要学可解释机器学习" class="headerlink" title="0x02 为什么要学可解释机器学习"></a>0x02 为什么要学可解释机器学习</h2><h3 id="1-可解释学习是一个很好的研究方向"><a href="#1-可解释学习是一个很好的研究方向" class="headerlink" title="1. 可解释学习是一个很好的研究方向"></a>1. 可解释学习是一个很好的研究方向</h3><ul>
<li>选择人工智能研究方向的建议（同济子豪兄）<ul>
<li>尽可能<strong>通用</strong>，与其它研究方向<strong>交叉</strong></li>
<li>顺应主流发展趋势，<strong>长期存在</strong>且有用</li>
<li>有高质量的<strong>数据集</strong></li>
<li>不过分小众，但也好发 paper, 没有疯狂内卷</li>
<li>能应用到<strong>产业界</strong>垂直细分行业</li>
<li>有商业应用价值，容易“<strong>讲故事</strong>”</li>
</ul>
</li>
</ul>
<p>而可解释机器学习恰好都符合这些要求 ^ ^</p>
<p>总结一下，&#x3D;&#x3D;可解释机器学习的意义&#x3D;&#x3D;：</p>
<ul>
<li>研究 AI 的脑回路，就是研究 AI 的本质。</li>
<li>可解释分析是机器学习和数据挖掘的通用研究方法。</li>
<li>和所有 AI 方向交叉融合：数据挖掘、计算机视觉、自然语言处理、强化学习、知识图谱、联邦学习。</li>
<li>包括但不限于：大模型、弱监督、缺陷异常检测、细粒度分类、决策 AI 和强化学习、图神经网络、AI 纠偏、AI4Science、Machine Teaching、对抗样本、可信计算、联邦学习。</li>
</ul>
<h3 id="2-推荐的细分领域"><a href="#2-推荐的细分领域" class="headerlink" title="2. 推荐的细分领域"></a>2. 推荐的细分领域</h3><h4 id="2-1-Machine-Teaching"><a href="#2-1-Machine-Teaching" class="headerlink" title="2.1 Machine Teaching"></a>2.1 Machine Teaching</h4><blockquote>
<p>从 Machine Learning 到 Machine Teaching，人工智能教人类学习</p>
</blockquote>
<p>这里挺有意思的，先用海量数据训练 AI，使其学习某一项任务并达到较高的标准，随后反过来以可视化的形式，将其学习到的重要特征展示给人类，指导人类的学习、生活与工作。</p>
<ul>
<li>AI 以热力图的形式，将鸟类的区别特征展示给人类，教会人类如何去分辨各种类别的鸟。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140023310.png"></li>
<li>使用神经网络预测激光切割断面的工艺参数（准确度远胜于人类专家），并且以可视化的方法，教会人类切割断面上的哪些特征是对预测起到关键作用的。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140026505.png"></li>
<li>绝艺围棋 AI 指导棋（腾讯）</li>
<li>钢轨伤损智能检测（同济大学轨道系 张子豪）：指导铁路维修工人制定轨道定损的规范与策略</li>
<li>补全、复原未完成或缺损的古画（百度文心）&#x3D;》指导人类书法、绘画技巧</li>
</ul>
<h4 id="2-2-细粒度图像分类"><a href="#2-2-细粒度图像分类" class="headerlink" title="2.2 细粒度图像分类"></a>2.2 细粒度图像分类</h4><blockquote>
<p>什么是细粒度图像分类：对属于同一大类，已经高度相似，做进一步的细分小类。（已经非常像，但又彼此不同）</p>
</blockquote>
<ul>
<li><p>比如荔枝、海洋生物、奥特曼…..</p>
<ul>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140043231.png"></li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140043189.png"></li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140044847.png"></li>
</ul>
</li>
<li><p>肺炎（细菌性肺炎、病毒性肺炎、新冠肺炎……）</p>
<ul>
<li>指导影像科医生应该关注哪些区域，从而做出判断。</li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140045736.png"></li>
</ul>
</li>
<li><p>使用图像分类解决图像定位（甚至图像分割）的问题（这个角度好有意思）</p>
<ul>
<li>分类告诉人类有没有缺陷</li>
<li>进一步通过可解释分析，定位缺陷位置，告诉人类是哪里有缺陷</li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140048540.png"></li>
</ul>
</li>
</ul>
<p>&#x3D;》&#x3D;&#x3D;通过可解释机器学习，验证 AI 关注到了应该关注到的特征（或者区域），甚至进一步指导人类应该去关注哪些特征。&#x3D;&#x3D;</p>
<h4 id="2-3-一些前沿-AI-方向"><a href="#2-3-一些前沿-AI-方向" class="headerlink" title="2.3 一些前沿 AI 方向"></a>2.3 一些前沿 AI 方向</h4><blockquote>
<p>值得使用可解释机器学习去解释、探究的大型的深度学习模型</p>
</blockquote>
<ul>
<li>ChatGPT</li>
<li>AI 绘画</li>
<li>目标检测</li>
<li>关键点检测</li>
</ul>
<h2 id="0x03-怎么学可解释机器学习"><a href="#0x03-怎么学可解释机器学习" class="headerlink" title="0x03 怎么学可解释机器学习"></a>0x03 怎么学可解释机器学习</h2><h3 id="1-本身可解释性好的机器学习算法"><a href="#1-本身可解释性好的机器学习算法" class="headerlink" title="1. 本身可解释性好的机器学习算法"></a>1. 本身可解释性好的机器学习算法</h3><ul>
<li>KNN<ul>
<li>根据距离新样本最近的 K 个样本是什么类别，从而判断新样本的类别（近朱者赤近墨者黑）</li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140102524.png"></li>
</ul>
</li>
<li>逻辑回归<ul>
<li>使用一定的权重，将所有特征加权求和，通过 Sigmoid 函数，获得概率。</li>
</ul>
</li>
<li>线性回归</li>
<li>IF ELSE&#x3D;》决策树（非常接近人类的脑回路）</li>
<li>朴素贝叶斯</li>
</ul>
<p><strong>传统机器学习算法的可解释性分析</strong>（<a href="https://www.bilibili.com/video/BV1Wf4y1U7EL">【子豪兄Kaggle】玩转UCI心脏病二分类数据集</a>）：</p>
<ul>
<li>算法自带的可视化</li>
<li>算法自带的特征权重</li>
<li>Permutation Importance 置换重要度<ul>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140113649.png"></li>
<li>将某一列的特征随机打乱，若打乱后严重降低算法准确度，说明该特征比较重要；</li>
<li>若打乱后没什么影响，说明不怎么重要；</li>
<li>若打乱后反而提升了算法，说明该特征属于噪音，更加不重要。</li>
</ul>
</li>
<li>PDP 图、ICE图<ul>
<li>PDP 图（看一个人）随着年龄的增长，患病几率的变化<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140116470.png"></li>
<li>ICE 图（看多个人）</li>
</ul>
</li>
<li>Shapley 值</li>
<li>Lime</li>
</ul>
<p>&#x3D;&#x3D;总结：目前的一些机器学习算法的可解释性是比较好的，但能力有限，深度网络有较强的能力，但可解释性尚且需要探究，因此很有必要对例如深度学习的模型做可解释性分析。&#x3D;&#x3D;<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140121779.png"></p>
<h3 id="2-对可解释性很差的深度学习做可解释性分析"><a href="#2-对可解释性很差的深度学习做可解释性分析" class="headerlink" title="2. 对可解释性很差的深度学习做可解释性分析"></a>2. 对可解释性很差的深度学习做可解释性分析</h3><p><strong>卷积神经网络的可解释性分析</strong>：</p>
<ul>
<li>可视化卷积核、特征图（最早追溯到 AlexNet）<ul>
<li>卷积核的作用<ul>
<li>每个卷积核提取不同的特征，每个卷积核对输入进行卷积，生成一个 feature map，这个 feature map 即提现了该卷积核从输入中提取的特征，不同的 feature map 显示了图像中不同的特征。</li>
<li>浅层卷积核提取：边缘、颜色、斑块等底层像素特征；中层卷积核提取：条纹、纹路、形状等中层纹理特征；高层卷积核提取：眼睛、轮胎、文字等高层语义特征；最后的分类输出层输出最抽象的分类结果。</li>
</ul>
</li>
<li>但人类只能理解浅层的卷积核，对于深层的卷积核是无法解释的</li>
</ul>
</li>
<li>遮挡、缩放、平移、旋转（ZFNet）<ul>
<li>使用灰色块遮挡图像的不同区域，根据对预测结果与置信度的影响，判断各个区域对于预测的重要性</li>
<li>对图像做缩放、平移、旋转，探究对预测结果的影响</li>
<li>统一遮挡住图像（狗）的右眼，若相比于随机遮挡，产生了一致性影响，那么说明神经网络确实学习到了右眼的特征。</li>
</ul>
</li>
<li>找到能使某个神经元激活的原图像素，或者小图</li>
<li><strong>基于类激活热力图（CAM）的可视化</strong><ul>
<li>把 AI 认为重要的特征高亮出来<ul>
<li>从而可以解释 AI 为什么会犯错，是最终关注在了哪个区域导致判断错误</li>
<li>并且可以根据神经网络关注的区域，判断是不是带有 Bias 的，从而指导人类修改数据集</li>
</ul>
</li>
</ul>
</li>
<li>语义编码降维可视化<ul>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140146257.png"></li>
</ul>
</li>
<li>由语义编码倒推输入的原图</li>
<li>生成满足某些要求的图像（某类别预测概率最大）</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>《深度学习的可解释性研究综述》 ——李凌敏，侯梦然，陈琨，刘军民<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140150461.png"></li>
<li>《深度学习可解释性研究综述》——雷霞，罗雄麟<ul>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140153985.png"></li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212140153198.png"></li>
</ul>
</li>
</ul>
<p><strong>思考题</strong>：</p>
<ul>
<li>为什么要对机器学习、深度学习模型做可解释性分析和显著性分析？<ul>
<li>打开人工智能的黑箱子，探究其脑回路，了解、解释 AI，从而改进它，信赖它。</li>
</ul>
</li>
<li>如何回答〝人工智能黑箱子灵魂之问”？<ul>
<li>对算法做可解释性分析</li>
</ul>
</li>
<li>人工智能的可解释性分析有哪些应用场景？<ul>
<li>Machine Teaching</li>
<li>对大模型做可解释性分析</li>
</ul>
</li>
<li>哪些机器学习算法本身可解释性就好？为什么？<ul>
<li>KNN、LR、决策树等。</li>
<li>这些算法的本质容易被人类理解，比如最近邻的样本点、对特征做加权求和、通过 IF ELSE 判断。</li>
</ul>
</li>
<li>对计算机视觉、自然语言处理、知识图谱、强化学习，分别如何做可解释性分析？<ul>
<li>计算机视觉：可视化卷积核、CAM 等。</li>
</ul>
</li>
<li>在你自己的研究领域和行业，如何使用可解释性分析？<ul>
<li>对三维模型的 Embedding 做降维可视化。</li>
<li>探究神经网络关注到了三维模型的哪些拓扑特征与几何特征。</li>
</ul>
</li>
<li>可以从哪几个角度实现可解释性分析？<ul>
<li>CAM、卷积核可视化、Embedding 降维可视化等。</li>
</ul>
</li>
<li>Machine Teaching 有哪些应用场景？<ul>
<li>AI 指导棋、书法绘画、各种物体或医学图像的细粒度分类等。</li>
</ul>
</li>
</ul>
<p>真的是蛮有意思并且很实用的一个方向，感谢子豪兄的分享 ^ ^</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><p>课程：<a href="https://www.bilibili.com/video/BV1Se4y1T7dG/">可解释机器学习公开课_哔哩哔哩_bilibili</a></p>
</li>
<li><p>实践： </p>
<ul>
<li><a href="https://github.com/TommyZihao/Train_Custom_Dataset/tree/main/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB">Train_Custom_Dataset&#x2F;图像分类 at main · TommyZihao&#x2F;Train_Custom_Dataset · GitHub</a></li>
<li><a href="https://github.com/utkuozbulak/pytorch-cnn-visualizations">GitHub - utkuozbulak&#x2F;pytorch-cnn-visualizations: Pytorch implementation of convolutional neural network visualization techniques</a></li>
</ul>
</li>
<li><p>可解释性分析论文：<a href="https://readpaper.com/user/collect/638623946528292864">可解释性分析-论文集</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Explainable ML</category>
      </categories>
      <tags>
        <tag>Explainable ML</tag>
      </tags>
  </entry>
  <entry>
    <title>可解释机器学习-Task02 ZFNet 深度学习图像分类算法</title>
    <url>/2EQN5PQ/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><ul>
<li>纽约大学 ZFNet，2013 年 ImageNet 图像分类竞赛冠军模型。对 AlexNet 进行改进的基础上，提出了一系列可视化卷积神经网络中间层特征的方法，并巧妙设置了对照消融实验，从各个角度分析卷积神经网络各层提取的特征及对变换的敏感性</li>
<li>论文：<a href="https://arxiv.org/abs/1311.2901">Visualizing and Understanding Convolutional Networks</a>（可视化并理解卷积神经网络）</li>
<li>主要贡献：<strong>提出了一种巧妙的可视化卷积神经网络中间层特征的可视化方法与技巧</strong></li>
</ul>
<span id="more"></span>

<h2 id="0x01-ZFNET-网络结构"><a href="#0x01-ZFNET-网络结构" class="headerlink" title="0x01 ZFNET 网络结构"></a>0x01 ZFNET 网络结构</h2><blockquote>
<p>网络结构：在 AlexNet 的基础上，对卷积核、步长等做了一些调整</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160022667.png"></p>
<p><strong>后文会提到为什么要对 AlexNet 做以上修改，以及修改的根据是什么。</strong></p>
<h2 id="0x02-可视化卷积中间层的方法"><a href="#0x02-可视化卷积中间层的方法" class="headerlink" title="0x02 可视化卷积中间层的方法"></a>0x02 可视化卷积中间层的方法</h2><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160023716.png"><br>如图，对于想要可视化的中间层（由卷积、激活、池化获得的），执行<strong>反池化、反激活、反卷积</strong>操作，重构回原始输入的特征空间，获得人类可以理解的特征（图片）。</p>
<h3 id="1-反池化"><a href="#1-反池化" class="headerlink" title="1. 反池化"></a>1. 反池化</h3><blockquote>
<p>池化操作是用的 MaxPooling，将最大值保留。</p>
</blockquote>
<p>那么如何反池化呢？</p>
<blockquote>
<p>在正向池化的时候将<strong>对最大值的原始像素位置记录</strong>，反池化时，根据各个最大值的原始位置重新放回去。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160044853.png"></p>
</blockquote>
<h3 id="2-反激活"><a href="#2-反激活" class="headerlink" title="2. 反激活"></a>2. 反激活</h3><blockquote>
<p>仍然使用 ReLU 激活函数。</p>
</blockquote>
<h3 id="3-反卷积"><a href="#3-反卷积" class="headerlink" title="3. 反卷积"></a>3. 反卷积</h3><blockquote>
<p>使用原来正向卷积的卷积核的转置（行列互换）。<br>转置卷积没有需要学习的参数，是一个完全<strong>无监督</strong>的过程。</p>
</blockquote>
<p>什么叫卷积核激活最大？</p>
<h2 id="0x03-中间卷积层可视化的实验"><a href="#0x03-中间卷积层可视化的实验" class="headerlink" title="0x03 中间卷积层可视化的实验"></a>0x03 中间卷积层可视化的实验</h2><blockquote>
<p>Feature Map：<br>Feature Map（特征图）是输入图像经过神经网络卷积产生的结果，表征的是神经空间内一种特征；其分辨率大小取决于先前卷积核的步长。</p>
<p>层与层之间会有若干个卷积核（Kernel），上一层中的每个 Feature Map 跟每个卷积核做卷积，对应产生下一层的一个 Feature Map。</p>
<p>Feature Map 的含义在计算机视觉领域基本一致，可以简单译成特征图，例如 RGB 图像，所有像素点的 R 可以认为一个 Feature Map（这个概念与在 CNN 里面概念是一致的）。</p>
</blockquote>
<blockquote>
<p>彩色图：从原始数据集找出的，能使特定层的某些卷积核激活最大的九张图片（九张为一组）。</p>
</blockquote>
<blockquote>
<p>灰色图：将九张图片 feed 到网络中，随后从特定层找到其 Feature Map，然后使用反卷积的技巧重构回原始输入像素空间得到的可视化图片。</p>
</blockquote>
<h3 id="1-第一层"><a href="#1-第一层" class="headerlink" title="1. 第一层"></a>1. 第一层</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160115071.png"></p>
<p>图中上面的 3*3 的图是第一层卷积核，(1, 1) 对应着下图左上角的 9 个 Patch，其他的以此类推。可见 (1, 1) 卷积核主要提取左上角到右下角的边缘特征，(3,2) 卷积核主要提取绿色的特征，可见第一层卷积层主要提取边缘、颜色这种底层特征。</p>
<h3 id="2-第二层（开始使用反卷积做可视化）"><a href="#2-第二层（开始使用反卷积做可视化）" class="headerlink" title="2. 第二层（开始使用反卷积做可视化）"></a>2. 第二层（开始使用反卷积做可视化）</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160124585.png"></p>
<p>可以看到卷积核开始提取晚霞色、圆环、垂直的线等等的形状特征。</p>
<p>左侧灰色图与右侧的彩色图一一对应，灰色图有颜色的位置即代表像素被还原回去的位置，可以发现是可以与彩色图里的特征形成对应，也可视化了该层的 Feature Map 到底是提取到了怎样的特征。与第一层的特征相比好像高级了一些。 </p>
<h3 id="3-第三层"><a href="#3-第三层" class="headerlink" title="3. 第三层"></a>3. 第三层</h3><blockquote>
<p>可以发现，到了更深的层时，提取到的特征更加复杂、高级，或者说更富有语义信息。</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160152647.png"></p>
<p>该层提取到了车轮、人体形状、文字等等的一些特征。</p>
<h3 id="4-第四层和第五层"><a href="#4-第四层和第五层" class="headerlink" title="4. 第四层和第五层"></a>4. 第四层和第五层</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160154228.png"></p>
<p>第四层提取到了比如狗脸、鸟腿、其他动物的腿的一些特征。</p>
<p>第五层甚至提取到了草地的背景特征（而非前景特征），以及眼睛的特征。</p>
<p>而且相比于其他卷积核，他们的彩色图片基本有一定的相似性，但如下<strong>提取草地背景</strong>的卷积核的彩色图片可以说是有较大差异的。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160200571.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160202776.png"></p>
<p>可见随着卷积层的变深，所提取到的特征更加高级，更加富有语义性。</p>
<h2 id="0x04-相关的分析"><a href="#0x04-相关的分析" class="headerlink" title="0x04 相关的分析"></a>0x04 相关的分析</h2><h3 id="1-训练过程中中间层卷积核的变化"><a href="#1-训练过程中中间层卷积核的变化" class="headerlink" title="1. 训练过程中中间层卷积核的变化"></a>1. 训练过程中中间层卷积核的变化</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160217344.png"></p>
<p>灰色图依然从数据集中挑选出来激活最大的 Feature Map，然后使用反卷积重建出在原始像素图片中可视化的结果。图中每一层（Layer 1，Layer 2…）的一列是挑选出的数个卷积核，一行是该卷积核随着训练迭代的变化。<strong>可以看到低层的卷积核很快收敛，高层的卷积核在刚开始的训练过程中提取不到特征，要训练多轮才可以收敛并提取到特征</strong>。</p>
<h3 id="2-不变性分析"><a href="#2-不变性分析" class="headerlink" title="2. 不变性分析"></a>2. 不变性分析</h3><p>对原图做平移、缩放、旋转，对某一层的 Feature Vector（由 Feature Map 拉直获得）的变化，使用变化前与变化后的 Vector 的欧氏距离来度量。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160228871.png"></p>
<p>做平移时，第一层的卷积核比较敏感，Feature Vector 变化较为剧烈，而第七层的卷积核呈现出较为缓慢的类似于线性的变化，<strong>说明该层已经不是那么注重于表面的特征变化，而是更加关注高级的语义信息</strong>。</p>
<p>做旋转时，图像每旋转 90 度，准确率会出现一个峰值，说明了对称性，当某些特征（比如电视机）是对称，或者翻转过去，依然呈现为正的一个长方形时，网络是可以捕获到这种信息的。</p>
<h3 id="3-对-AlexNet-的优化"><a href="#3-对-AlexNet-的优化" class="headerlink" title="3. 对 AlexNet 的优化"></a>3. 对 AlexNet 的优化</h3><blockquote>
<p>通过对 AlexNet 的第一层与第二层的卷积核做可视化，发现存在一些卷积核失效，或者是由于步长过大，出现混淆网格样式的卷积核，基于此，本文对卷积核大小以及步长做了一定调整，并改善了以上发现的问题。</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160239577.png"></p>
<p>对步长以及卷积核大小做了修改之后，<strong>第一层卷积层中失效的卷积核变少了</strong>。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160241682.png"></p>
<p>对步长以及卷积核大小做了修改之后，第二层中混乱网格的卷积核去除了。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160242831.png"></p>
<h3 id="4-局部遮挡敏感性分析"><a href="#4-局部遮挡敏感性分析" class="headerlink" title="4. 局部遮挡敏感性分析"></a>4. 局部遮挡敏感性分析</h3><blockquote>
<p>用一个小灰方块在图像上进行遮挡，并分析该遮挡对 Feature Map 与预测结果造成的影响，<strong>探究了网络对遮挡的敏感性</strong>。</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212160249341.png"></p>
<p>第三列黑框中的是未经遮挡时的第五层 Feature Map 重构的可视化，黑框之外的三个是同样可以使得激活最大的其他三个无关的图片。</p>
<p>第二列是灰色挡板在不同位置时，对各自获得的 Feature Map 叠加起来获得的热力图。当遮挡住重要信息（当前层主要提取的特征，如狗脸、文字、人脸）时，Feature Map 中相应的值自然会降低，在热力图中呈现为蓝色。从而验证了神经网络对遮挡的敏感性。</p>
<h3 id="5-局部遮挡相关性分析"><a href="#5-局部遮挡相关性分析" class="headerlink" title="5. 局部遮挡相关性分析"></a>5. 局部遮挡相关性分析</h3><blockquote>
<p>如果对不同的图片（狗）遮挡住同一位置（左眼、右眼、鼻子等），对网络的影响是一样的，那么说明网络对于各种狗脸图像中的左眼、右眼、鼻子的部位定义了一种隐式相关性。</p>
</blockquote>
<p>如何验证呢？</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212161513878.png"></p>
<p>我们将遮挡前与遮挡后不同层的 Feature Vector 做一个差值 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container>，探究遮住同一部位，与随机遮住某一部位时，该差值 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container> 所反映的不同变化。若五张图（狗脸）都遮住左眼，他们的 FV 差值（对网络造成的影响）是接近的，那么说明遮住左眼对于这些图的影响是近乎一样的（虽然图中的狗脸不一样）。表中数据是用 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.676ex" height="1.977ex" role="img" focusable="false" viewBox="0 -716 1182.7 873.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mo"><path data-c="25B3" d="M75 0L72 2Q69 3 67 5T62 11T59 20Q59 24 62 30Q65 37 245 370T428 707Q428 708 430 710T436 714T444 716Q451 716 455 712Q459 710 644 368L828 27V20Q828 7 814 0H75ZM610 347L444 653Q443 653 278 347T113 40H775Q775 42 610 347Z"></path></g><g data-mml-node="mi" transform="translate(922,-150) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></g></svg></mjx-container> 作为衡量标准，这个指标反映了在不同图像中遮挡同一部位对第 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container> 层网络提取得到的 Feature Map 的影响程度，越小说明影响是相同的。<code>这里会难理解一些，配合论文里的公式、计算方法与解释会更好理解。</code></p>
<p>还有一点要注意的是，随机遮挡时，第五层的影响是较大的，但到第七层时，影响变小了一些，说明更加深层的网络更加关注高级的语义信息，而不仅仅是表面的一些变化。</p>
<h2 id="0x05-模型实验结果"><a href="#0x05-模型实验结果" class="headerlink" title="0x05 模型实验结果"></a>0x05 模型实验结果</h2><h3 id="1-实验对比"><a href="#1-实验对比" class="headerlink" title="1. 实验对比"></a>1. 实验对比</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212161526125.png"></p>
<p>与 AlexNet 相比，改进后的模型获得了较大的提升。</p>
<h3 id="2-消融实验"><a href="#2-消融实验" class="headerlink" title="2. 消融实验"></a>2. 消融实验</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212161532368.png"></p>
<p>上面一栏是对 AlexNet 做的一些实验：</p>
<p>前五行：去掉少量卷积层或者全连接层对于错误率的影响是不大的，但如果同时去掉（3，4，6，7）层的卷积与全连接，会有较大的影响。说明网络的深度对于预测准确性的影响还是较为关键的。</p>
<p>七行：如果将 Layer 6, 7 的神经元个数设置的非常大会造成过拟合，可以看到 Train Top-1 的错误率为 26.8，但 Val Top-1 的错误率却达到了 40.0。</p>
<p>下面一栏有关本文的一些实验，可以看到在验证集达到了一个最低的错误率。</p>
<h3 id="3-迁移学习实验"><a href="#3-迁移学习实验" class="headerlink" title="3. 迁移学习实验"></a>3. 迁移学习实验</h3><blockquote>
<p>探究在 ImageNet 上训练的 ZFNet 能否泛化到其他数据集，即进行迁移学习与微调。</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212161545752.png"></p>
<p>在 ImageNet 上训练的 ZFNet 拿到 Caltech-101 数据集上，<strong>保留前面的层，只对 Softmax 分类层做重新训练</strong>，可以看到取得了比原作要好的效果。但如果只保留 ZFNet 模型结构，清除模型参数，在 Caltech-101 上重新训练网络，可以看到效果并不好。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212161605511.png"></p>
<p>在 Caltech 256 上也取得了远高于原作的效果。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212161606541.png"></p>
<p><strong>有意思的是，每个类别只需要 6 张图片训练，就可以达到原作（每个类别 60 张）最好的效果。</strong><code>也就是说使用迁移学习，可以用很少的数据，就达到较好的性能，如果拥有更多的数据，就可以达到更好的性能。</code></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212161609867.png"></p>
<p>但如果以同样的方式照搬到 PASCAL 数据集，其实效果没那么好，因为这个数据集中每张图片有多个物体，而 ImageNet 与前两个数据集都只有一个物体（是相似的）。因此在不同情况的数据集上，对损失函数等做一些调整，才会达到不错的效果。</p>
<h3 id="4-网络中不同层对于分类的有效性"><a href="#4-网络中不同层对于分类的有效性" class="headerlink" title="4. 网络中不同层对于分类的有效性"></a>4. 网络中不同层对于分类的有效性</h3><blockquote>
<p>全连接层起到线性分类器的作用，SVM 或者 Softmax 同样可以起到分类的作用。在实验中可以看到，越深的层对分类结果起到越发有效的作用。</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202212161544062.png"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>ZFNet 提供了可视化卷积模型中间层的技巧，使用反卷积、反池化、反激活将中间层的 Feature Map 重构到原始输入图像的像素空间，变成人类可以理解的图像。并且提供了很多有趣的实验方法，比如分析局部遮挡的敏感性与相关性，基于可视化结果优化模型结构，做迁移学习的实验方法等等。是很有启发性的一篇优秀论文。也感谢子豪兄的讲解。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li>课程视频<ul>
<li><a href="https://www.bilibili.com/video/BV17b4y1m7x8">【精读AI论文】ZFNet深度学习图像分类算法（反卷积可视化可解释性分析）</a></li>
</ul>
</li>
<li>学习内容链接：<ul>
<li><a href="https://datawhaler.feishu.cn/docx/OTROd2zCIoZlLyxjhSKclxKNnDe">可解释机器学习内容安排</a></li>
<li><a href="https://github.com/TommyZihao/zihao_course/tree/main/XAI">学习 GitHub 内容链接</a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Explainable ML</category>
      </categories>
      <tags>
        <tag>Explainable ML</tag>
      </tags>
  </entry>
  <entry>
    <title>1.1 框架式思维——漏斗思维</title>
    <url>/6S0ESX/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>漏斗思维，是一个非常高效的思维方式，是一种线性的思考逻辑，一般按照任务的完成路径，识别出几个关键的行为转化节点；然后分析行为点间的转化与流失情况，进而定位问题，指导决策。</p>
<p>这种思维其实已经渗透在各行各业中，虽然平时并没有很好的觉察，但是，把他单独抽象出来，会让你更加容易洞悉到问题本质</p>
<span id="more"></span>

<h3 id="举一些例子"><a href="#举一些例子" class="headerlink" title="举一些例子"></a>举一些例子</h3><p><strong>以面试为例</strong>（层层筛选）：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205212209588.png"></p>
<p><strong>以咨询为例</strong>：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205221951849.png"></p>
<p><strong>以电商为例</strong>：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205221959021.png"></p>
<p><strong>以广告为例</strong>：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205222118804.png"></p>
<p><strong>以 AISAS 模型为例</strong>（小红书）：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205222002116.png"></p>
<h2 id="0x01-搜索引擎系统漏斗"><a href="#0x01-搜索引擎系统漏斗" class="headerlink" title="0x01 搜索引擎系统漏斗"></a>0x01 搜索引擎系统漏斗</h2><p>当我们在百度或者淘宝这些软件中，输入 Query 搜索自己想要的答案或者商品。搜索引擎如何从上千万乃至上亿的数据中，一步一步，返回满足用户需求的结果？</p>
<p>以百度搜索为例：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205222033968.png"></p>
<p>在这个过程中，关键环节在于其中的：召回 —&gt; 粗排 —&gt; 精排。</p>
<p>以淘宝搜索为例：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205222040679.png"></p>
<blockquote>
<p> ——摘抄自《阿里粗排技术体系与最新进展》<br> <a href="https://zhuanlan.zhihu.com/p/364141120">https://zhuanlan.zhihu.com/p/364141120</a></p>
</blockquote>
<p>在排序的过程中，往往会有一步重排。为什么从 粗排 —&gt; 精排 之后还需要一步重排呢？</p>
<ul>
<li>为什么要重排？<ul>
<li>保证结果的<strong>多样性</strong>：以短视频推荐为例，如果推荐的总是单一类型的视频（比如汽车），那么即使用户喜欢汽车，也会感到疲劳。所以通过重排，推荐一些相似领域，相关领域的视频，丰富推荐结果的多样性。</li>
<li><strong>多轮优选</strong>：Listwise，把所有的排列组合都考虑一遍。</li>
</ul>
</li>
</ul>
<h2 id="0x02-检索系统架构"><a href="#0x02-检索系统架构" class="headerlink" title="0x02 检索系统架构"></a>0x02 检索系统架构</h2><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205291244344.png"></p>
<h2 id="0x03-作业"><a href="#0x03-作业" class="headerlink" title="0x03 作业"></a>0x03 作业</h2><ul>
<li>我们的生活中，还有什么场景会涉及漏斗思维的地方？<ul>
<li>营销、注册转化</li>
</ul>
</li>
<li>在纸上自己画一下今天的图（系统架构图、广告漏斗转化模型，搜索系统漏斗图）<ul>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205241940531.png"></li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205242009540.png"></li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205242010805.png"></li>
</ul>
</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://zhuanlan.zhihu.com/p/200899462">如何理解并应用漏斗模型？ - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/364141120">阿里粗排技术体系与最新进展 - 知乎</a></p>
]]></content>
      <categories>
        <category>FunRec</category>
        <category>01-自然语言文本处理基础</category>
      </categories>
      <tags>
        <tag>FunRec</tag>
      </tags>
  </entry>
  <entry>
    <title>1.2 TF-IDF 实践</title>
    <url>/8CFDNJ/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>TF-IDF 是 NLP 入门的基础知识。通过对这种编码方式的学习，可以使我们更加容易理解 NLP 工作的本质。这篇笔记重点在实践 TF-IDF。想了解更多相关理论可以参考 <a href="https://1nnoh.top/280EQA3/">矢量语义与嵌入之 TF-IDF 检索</a> 。</p>
<span id="more"></span>

<h2 id="0x01-Count-Vector"><a href="#0x01-Count-Vector" class="headerlink" title="0x01 Count Vector"></a>0x01 Count Vector</h2><ul>
<li>词袋模型 Bow<ul>
<li>目的：最基础的文本特征提取方法，将文本转为计数矩阵</li>
<li>原理：类似 One-Hot</li>
<li>优点：<ul>
<li>简单、直接</li>
<li>易于理解</li>
</ul>
</li>
<li>不足：<ul>
<li>稀疏</li>
<li>相当于只统计了词频<ul>
<li>但是像“你我他，的地得”这种词，每个文章中都会出现</li>
<li>所以需要如何把这种词的权重降下来？</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><ol>
<li>手撸 Count Vector</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">texts = ["He is a boy", "She is a girl, good girl"]

word_set = set()

# 存储词典
for text in texts:
    for word in text.strip().split(' '):
        word_set.add(word.strip(','))

print(word_set)
# 输出：
# {'He', 'girl', 'is', 'good', 'She', 'a', 'boy'}

# 给词典中所有词语赋一个 id 值
word_id_dict = {}
for word in enumerate(word_set):
    word_id_dict[word[1]] = word[0]

print(word_id_dict)
# 输出：
# {'He': 0, 'girl': 1, 'is': 2, 'good': 3, 'She': 4, 'a': 5, 'boy': 6}

# 根据上一步生成的 word_id_dict，将文档 texts 中的词语替换成对应的 id
res_list = []

for text in texts:
    t_list = []
    for word in text.strip().split(' '):
        word = word.strip(',')
        if word in word_id_dict:
            t_list.append(word_id_dict[word])
    res_list.append(t_list)

print(res_list)
# 输出：
# [[0, 2, 5, 6], [4, 2, 5, 1, 3, 1]]

# 将文档 texts 转为向量
# tests 中有两个文档，每个文档的向量长度为词典长度
# 根据 word_id_dict，出现的词语在 id 对应的列表位置 +1
for res in res_list:
    result = [0] * len(word_set)
    for word_id in res:
        result[word_id] += 1

    print(result)
# 输出：
# [1, 0, 1, 0, 0, 1, 1]
# [0, 2, 1, 1, 1, 1, 0]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="2">
<li>调用 Sklearn<br><a href="https://blog.csdn.net/weixin_38278334/article/details/82320307">sklearn——CountVectorizer 详解</a></li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Count Vector 方法只考虑了 TF，很片面！由此引出 TF-IDF（两者相乘）方法。</p>
<h2 id="0x02-TF-IDF-的原理"><a href="#0x02-TF-IDF-的原理" class="headerlink" title="0x02 TF-IDF 的原理"></a>0x02 TF-IDF 的原理</h2><ul>
<li><p>词频 <code>TF</code> ：反应文章的 <strong>局部信息</strong>。</p>
<ul>
<li>在一篇文章中，越重要的内容，出现（强调）的次数越多，那么词频 <code>TF</code> 就会越高。所以这些高词频的词，就可以代表这篇文章。</li>
<li>但伴随而来的问题是，文章中许多的语气词或者“你我他”这种词或者标点符号，同样也会出现很多次，但这些词往往也是高频词，但是没有意义。如何解决这种情况？那就需要 <code>IDF</code>。</li>
</ul>
</li>
<li><p>逆文本频率指数 <code>IDF</code> ：反应系统的 <strong>全局信息</strong>。</p>
<ul>
<li><code>IDF</code> 可以帮助我们判断词语在系统中的 <strong>区分力</strong> 大小。<ul>
<li>比如，如果 <strong>每篇文章</strong> 中都有“我”，那么它在所有文章中的 <strong>区分力都不强</strong>。</li>
<li>也就是说，在 <strong>所有文档</strong> 中都经常出现的词语，区分力小；而不常出现的词，区分力强。</li>
</ul>
</li>
</ul>
</li>
<li><p><code>TF-IDF</code> : <code>TF</code> × <code>IDF</code></p>
<ul>
<li>将两种指数相乘，得到 <code>TF-IDF</code>，表达一篇文档。</li>
<li>降低没有意义的词的重要性，突出文章中真正具有关键意义的内容（词语）。</li>
<li>对于一个 word，在文档出现的频率高，但在语料库里出现频率低，那么这个 word 对该文档的重要性比较高。</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-ad-note" data-language="ad-note"><code class="language-ad-note">TF(词频 - Term Frequency)：指的是某一个给定的词语在该文件中出现的次数，这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。

IDF(逆文本频率指数 - Inverse Document Frequency)：包含指定词语的文档越少，IDF越大。某个词语的IDF，由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到，即：
IDF=log(N/n)
-   N代表语料库中文档的总数
-   n代表某个word在几个文档中出现过；<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="1-前置知识点：文本相似度"><a href="#1-前置知识点：文本相似度" class="headerlink" title="1. 前置知识点：文本相似度"></a>1. 前置知识点：文本相似度</h3><ul>
<li><p>余弦相似度——最常用的相似度计算方法</p>
<ul>
<li>一个向量空间中两个向量夹角的余弦值作为衡量两个个体之间差异的大小</li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205251854838.png"></li>
<li><strong>余弦值接近 1，夹角趋于 0，表明两个向量越相似！</strong></li>
</ul>
</li>
<li><p>Jaccard 相似度——用于比较有限样本集之间的相似性与差异性</p>
<ul>
<li>Jaccard 系数定义为 A 与 B <strong>交集的大小</strong> 同 A 与 B <strong>并集的大小</strong>的比值</li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205251902319.png"></li>
<li>Jaccard Similarity 得分在 0 到 1 的范围内。如果两个文档相同，则 Jaccard Similarity 为 1。如果两个文档之间没有共同词，则 Jaccard 相似度得分为 0。</li>
<li>当 A 和 B 都为空，J(A,B) = 1</li>
</ul>
</li>
<li><p>欧氏距离相似度——最常见的距离度量方法</p>
<ul>
<li>n 维空间中计算两点间距离</li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205251915186.png"></li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>适用场景</strong>：<br>比如向量 A(1,1,1)，向量 B(5,5,5)，两个向量虽然余弦相似度一致（夹角为 0），但是在空间上，两个向量并非一致。类比到用户 A 购买了三个 1 元的物品，用户 B 购买了三个 5 元的物品，衡量这两个用户的消费能力时，显然用余弦相似度是不合适的，欧氏距离更加合适。</p>
<p>欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，比如使用用户行为作为指标分析用户价值的相似情况（比较不同用户的消费能力），这属于价值度量；而余弦相似度对绝对数值不敏感，更多的用于使用用户对内容的评分来分析用户兴趣的相似程度（用户是否喜欢某商品），这属于定性度量。</p>
<p>需要注意的是，欧氏距离和余弦相似度都需要保证各维度处于相同的刻度级别（量纲），所以一般需要对数据先进行标准化处理，否则很可能会引起偏差。比如用户对内容评分，假设为 5 分制，对用户甲来说评分 3 分以上就是自己喜欢的，而对于用户乙，评分 4 分以上才是自己喜欢的，这样就无法很好地衡量两个用户评分之间的相似程度。如果将评分数值减去平均值，那么就可以很好地解决问题。此时，就相当于用皮尔逊相关系数来度量相似程度。</p>
</blockquote>
<h4 id="实践：计算两个字符串的相似度"><a href="#实践：计算两个字符串的相似度" class="headerlink" title="实践：计算两个字符串的相似度"></a>实践：计算两个字符串的相似度</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205251937573.jpg"></p>
<p>首先生成文本的向量化表达：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import jieba
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

texts = ['这只皮靴号码大了，那只号码合适','这只皮靴号码不小，那只更合适']

# 分词
texts_cut = []

def cut_word(sentence):
    return '/'.join(jieba.lcut(sentence))

for text in texts:
    #text = text.replace('，', '')
    texts_cut.append(cut_word(text))
    # strip() 只能移除字符串头尾指定的字符序列，不能删除中间部分的字符
print(texts_cut)
# 输出：分词结果
# ['这/只/皮靴/号码/大/了/，/那/只/号码/合适', '这/只/皮靴/号码/不小/，/那/只/更/合适']

# 创建词袋数据结构
# CountVectorizer 参数中，默认的正则表达式选择 2 个及以上的字母或数字作为 token
# 那么会导致分词为单个字的词语被忽略
# 所以这里需要修改正则表达式参数
cv = CountVectorizer(token_pattern=r"(?u)\b\w+\b")  # 修改正则表达式参数

cv_fit = cv.fit_transform(texts_cut)
# 上行代码等价于下面两行
# cv.fit(texts)
# cv_fit=cv.transform(texts)

# 列表形式呈现文章生成的词典
print(cv.get_feature_names())
# 输出：词典
# ['不小', '了', '只', '号码', '合适', '大', '更', '皮靴', '这', '那']

# 字典形式的词典，带有词语 id
print(cv.vocabulary_)
# 输出：
# {'这': 8, '只': 2, '皮靴': 7, '号码': 3, '大': 5, '了': 1, '那': 9, '合适': 4, '不小': 0, '更': 6}
# 字典形式，key：词，value:该词（特征）的索引

# 所有文本的词频
print(cv_fit)
# 输出：
# (0, 8)  1  第 0 个文档中，词典索引为 8 的元素（这），词频为 1
# (0, 2)  2
# (0, 7)  1
# (0, 3)  2
# (0, 5)  1
# (0, 1)  1
# (0, 9)  1
# (0, 4)  1
# (1, 8)  1
# (1, 2)  2
# (1, 7)  1
# (1, 3)  1
# (1, 9)  1
# (1, 4)  1
# (1, 0)  1
# (1, 6)  1

# toarray() 将结果转为稀疏矩阵的表达
print(cv_fit.toarray())
# 输出：
# [[0 1 2 2 1 1 0 1 1 1]
#  [1 0 2 1 1 0 1 1 1 1]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol>
<li>使用余弦相似度计算<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205251939761.png"></li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 计算余弦相似度
texts_countvector = cv_fit.toarray()
cos_sim = cosine_similarity(texts_countvector[0].reshape(1,-1), texts_countvector[1].reshape(1,-1))

print('Cosine Similarity = %f'%cos_sim)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="2">
<li>使用 Jaccard 相似度计算</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 计算 jaccard 相似度
def jaccard_sim(a,b):
    unions = len(set(a).union(set(b)))
    intersections = len(set(a).intersection(set(b)))
    return intersections / unions

print('Jaccard Similarity = %f'%jaccard_sim(texts_cut[0],texts_cut[1]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="3">
<li>使用欧氏距离相似度计算</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 计算欧氏距离相似度
# calculating Euclidean distance
# using linalg.norm()
point1 = texts_countvector[0]
point2 = texts_countvector[1]
dist = np.linalg.norm(point1 - point2)

print('Euclidean Distance = %f'%dist)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="2-TF-标准计算公式"><a href="#2-TF-标准计算公式" class="headerlink" title="2. TF 标准计算公式"></a>2. TF 标准计算公式</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205261635717.png"></p>
<p>上一节中计算相似度使用的向量化方式其实就是 TF。一开始学习的 Count Vector 其实也是 TF，只是没有做归一化。</p>
<h3 id="3-IDF-标准计算公式"><a href="#3-IDF-标准计算公式" class="headerlink" title="3. IDF 标准计算公式"></a>3. IDF 标准计算公式</h3><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.059ex;" xmlns="http://www.w3.org/2000/svg" width="52.925ex" height="5.285ex" role="img" focusable="false" viewBox="0 -1426 23393 2336"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">逆</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">档</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">频</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">率</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">指</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mo" transform="translate(7000,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7389,0)"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(7893,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(8721,0)"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mo" transform="translate(9470,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(10136.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(11192.6,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(11490.6,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(11975.6,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(12452.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mfrac" transform="translate(12841.6,0)"><g data-mml-node="mrow" transform="translate(1081.2,676)"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">语</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">料</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">库</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">档</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">总</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g></g><g data-mml-node="mrow" transform="translate(220,-710)"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">包</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">含</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">该</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">词</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">文</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">档</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mo" transform="translate(8222.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(9222.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><rect width="9922.4" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(23004,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p>
<p>分母加 1 为了防止式子除 0。<br>对整个式子取 log，为了防止小数过长。</p>
<h3 id="4-TF-IDF-总结"><a href="#4-TF-IDF-总结" class="headerlink" title="4. TF-IDF 总结"></a>4. TF-IDF 总结</h3><ul>
<li>TF-IDF<ul>
<li>目的</li>
<li>原理：<code>TF</code> × <code>IDF</code><ul>
<li>特点：稀疏</li>
</ul>
</li>
<li>优点<ul>
<li>考虑了词的重要性</li>
<li>简单快速，而且容易理解</li>
</ul>
</li>
<li>缺点<ul>
<li>稀疏——计算、存储效率</li>
<li>每个词之间相互独立<ul>
<li>只考虑词频信息，比较片面</li>
<li>不考虑词语的有序性（位置信息）</li>
<li>不能衡量词之间的相似度</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="0x03-TF-IDF-的实践"><a href="#0x03-TF-IDF-的实践" class="headerlink" title="0x03 TF-IDF 的实践"></a>0x03 TF-IDF 的实践</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">import math
import pandas as pd

docA = "The cat sat on my face"
docB = "The dog sat on my bed"

bowA = docA.split(' ')
bowB = docB.split(' ')

wordSet = set(bowA).union(set(bowB))

wordDictA = dict.fromkeys(wordSet, 0)
wordDictB = dict.fromkeys(wordSet, 0)

for word in bowA:
    wordDictA[word] += 1

for word in bowB:
    wordDictB[word] += 1

print(pd.DataFrame([wordDictA, wordDictB]))
# 输出：文本向量化
#    my  The  cat  on  bed  dog  sat  face
# 0   1    1    1   1    0    0    1     1
# 1   1    1    0   1    1    1    1     0

def computeTF(wordDict, bow):
    tfDict = {}
    bowCount = len(bow)
    for word, count in wordDict.items():
        tfDict[word] = count / float(bowCount)

    return tfDict

tfDictA = computeTF(wordDictA, bowA)
tfDictB = computeTF(wordDictB, bowB)

# IDF = log(语料库的文档总数 / (包含该词的文档数 + 1))
def computeIDF(docList):
    idfDict = {}
    N = len(docList)

    idfDict = dict.fromkeys(docList[0].keys(), 0)
    for doc in docList:
        for word, val in doc.items():
            if val &gt; 0:
                idfDict[word] += 1
    print('N:', N)
    print(idfDict)

    for word, val in idfDict.items():
        idfDict[word] = math.log10(N /( float(val)+1))

    return idfDict

idfs = computeIDF([wordDictA, wordDictB])
print(idfs)
# 输出：
# N: 2（总文档数为 2）
# 统计每个词在所有文档中出现的次数：
# {'my': 2, 'The': 2, 'cat': 1, 'on': 2, 'bed': 1, 'dog': 1, 'sat': 2, 'face': 1}
# 计算得到的 idf：
# {'my': -0.17609125905568127, 'The': -0.17609125905568127, 'cat': 0.0, 'on': -0.17609125905568127, 'bed': 0.0, 'dog': 0.0, 'sat': -0.17609125905568127, 'face': 0.0}

# 注意!!! 为什么有的 idf 是 0 ?
# 因为只出现一次的词语通过计算得到 log(2/1+1) = 0

def computeTFIDF(tfDict, idfs):
    tfidfDict = {}
    for word, val in tfDict.items():
        tfidfDict[word] = val * idfs[word]

    return tfidfDict

tfidfDictA = computeTFIDF(tfDictA, idfs)
tfidfDictB = computeTFIDF(tfDictB, idfs)

print(pd.DataFrame([tfidfDictA, tfidfDictB]))
# 输出：所有文档的 TF-IDF
#          my       The  cat        on  bed  dog       sat  face
# 0 -0.029349 -0.029349  0.0 -0.029349  0.0  0.0 -0.029349   0.0
# 1 -0.029349 -0.029349  0.0 -0.029349  0.0  0.0 -0.029349   0.0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<hr>
<h2 id="0x04-项目1：《文章关键信息提取》"><a href="#0x04-项目1：《文章关键信息提取》" class="headerlink" title="0x04 项目1：《文章关键信息提取》"></a>0x04 项目1：《文章关键信息提取》</h2><p>目标：对一个数据集中的所有文章做关键信息提取。</p>
<p>首先将 <code>input_tfidf_dir</code> 中的所有文章整个到一个文件里。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># -----------
# convert.py
# -----------

import os
import sys

# 「argv」是「argument variable」参数变量的简写形式，一般在命令行调用的时候由系统传递给程序。这个变量其实是一个List列表，argv[0] 一般是“被调用的脚本文件名或全路径”，这个与操作系统有关，argv[1]和以后就是传入的系统命令参数。
file_path_dir = sys.argv[1]

def file_handler(file_path):
    f = open(file_path, 'r')
    return f

file_name = 0
for f in os.listdir(file_path_dir):
    if not f.startswith('.'):  # 筛除 以 “.” 开头的隐藏文件
        file_path = file_path_dir + "/" + f
        content_list = []

        file = file_handler(file_path)
        for line in file:
            content_list.append(line.strip())

        print('\t'.join([str(file_name), ' '.join(content_list)]))

        file_name += 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 查看一下输出
python convert.py input_tfidf_dir
# 将输出存入文件
python convert.py input_tfidf_dir &gt; tfidf_input.data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>MapReduce 要做下面的事情：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202205291138183.png"></p>
<p>将左边乱序的输出排序，相同的 word 放一起。执行 <code>reduce.py</code> 的时候，要考虑到如右图——最后一个 word 只有一行的情况。</p>
<p>将文档里出现的词打印出来。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># -----------
# map.py
# -----------

import sys

for line in sys.stdin:
    ss = line.strip().split('\t')
    if len(ss) != 2:  # 检查
        continue

    file_name, file_content = ss
    word_list = file_content.strip().split(' ')

    word_set = set(word_list)  # 去除一篇文档中重复的单词

    for word in word_set:
        print('\t'.join([word, '1']))
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>检验一下 <code>map.py</code> 文件做了什么，可以做什么。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 输出每个文档中出行啊的词
cat tfidf_input.data | python map.py

# 打印输出的结果
cat tfidf_input.data | python map.py | grep '设计'

# 统计个数
cat tfidf_input.data | python map.py | grep -c '设计'

# 将输出结果做个排序
 cat tfidf_input.data | python map.py | sort -k1 -nr<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>计算 idf。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># -----------
# reduce.py
# -----------

import sys
import math

docs_cnt = 508

current_word = None
count = 0

for line in sys.stdin:
    ss = line.strip().split('\t')
    if len(ss) != 2:

        continue
    word, val = ss
    if current_word == None:
        current_word = word

    if current_word != word:
        idf = math.log10(docs_cnt / (float(count) + 1.0))  # 计算 idf
        print('\t'.join([current_word, str(idf)]))
        count = 0  # 重置 count
        current_word = word  # 换到下一个 word

    count += int(val)

# 防止最后一类只有一行，导致最后一类没有输出
idf = math.log10(docs_cnt / (float(count) + 1.0))
print('\t'.join([current_word, str(idf)]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 将计算的 idf 存储到 result_idf.data 文件里
cat tfidf_input.data | python map.py | sort -k1 -nr | python reduce.py &gt; result_idf.data

# 查看一下 “的” 的 idf
cat result_idf.data | grep '的'
# 输出：
# 的  0.0008557529505832833<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="0x05-项目2：文本相似度计算"><a href="#0x05-项目2：文本相似度计算" class="headerlink" title="0x05 项目2：文本相似度计算"></a>0x05 项目2：文本相似度计算</h2><p><a href="https://github.com/cjymz886/sentence-similarity">GitHub - cjymz886/sentence-similarity: 对四种句子/文本相似度计算方法进行实验与比较</a></p>
<p>使用 Word2Vec 的方法将所有句子生成句向量，然后利用四种方法计算文本相似度。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>配置 Vim：<br> <a href="https://bynss.com/linux/827013.html">怎样在 Ubuntu 上安装最新的 Vim | 月灯依旧</a><br> <a href="https://blog.csdn.net/weixin_36338224/article/details/120985361">保姆级教程！将 Vim 打造一个 IDE （Python 篇）</a><br> <a href="https://zhuanlan.zhihu.com/p/127933244">Vim-Python 环境 - 知乎</a><br> <a href="https://zhuanlan.zhihu.com/p/30022074">Vim - 配置 IDE 一般的 python 环境 - 知乎</a><br> <a href="https://blog.csdn.net/qq_34128332/article/details/115820865?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1-115820865-blog-123357406.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1-115820865-blog-123357406.pc_relevant_default&utm_relevant_index=2">Vim 插件 YouCompleteMe 安装归纳</a></p>
<p>文本相似度：<br> <a href="https://zhuanlan.zhihu.com/p/155960197">推荐算法原理（二）欧几里得距离计算物品间相似度 - 知乎</a><br> <a href="https://www.cnblogs.com/HuZihu/p/10178165.html">相似度度量：欧氏距离与余弦相似度</a><br> <a href="https://blog.csdn.net/steven_ffd/article/details/84881063">sklearn中CountVectorizer里token_pattern默认参数解读</a></p>
<p>相似度计算：<br> <a href="https://www.delftstack.com/zh/howto/python/cosine-similarity-between-lists-python/">Python 中的余弦相似度 | D栈 - Delft Stack</a><br> <a href="https://github.com/cjymz886/sentence-similarity">GitHub - cjymz886/sentence-similarity: 对四种句子/文本相似度计算方法进行实验与比较</a></p>
]]></content>
      <categories>
        <category>FunRec</category>
        <category>01-自然语言文本处理基础</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>FunRec</tag>
      </tags>
  </entry>
  <entry>
    <title>1.3 中文分词</title>
    <url>/2ZZ2RHB/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>中文分词的目的：对中文句子中的词与词之间加上边界标记，本质就是对中文句子做划分词的边界。</p>
<ul>
<li>为什么要做中文分词？<ul>
<li>因为英文单词之间是天然分开的，但中文没有</li>
</ul>
</li>
<li>怎么做中文分词？<ul>
<li>基于统计学习的方法</li>
<li>基于机器学习的方法等</li>
</ul>
</li>
</ul>
<span id="more"></span>

<h2 id="0x01-中文分词的难点"><a href="#0x01-中文分词的难点" class="headerlink" title="0x01 中文分词的难点"></a>0x01 中文分词的难点</h2><p><strong>中文分词面临以下难点</strong>：</p>
<ul>
<li>标准（颗粒度选择，场景多样）<ul>
<li>搜索领域、推荐领域以及其他垂直领域的<strong>需求不同</strong></li>
</ul>
</li>
<li>歧义（一词多义）<ul>
<li>苹果 =&gt; 手机还是水果？</li>
<li>炒鱿鱼？</li>
</ul>
</li>
<li>新词（未登录词）</li>
</ul>
<p>后面两点很容易理解，第一点需要详细阐述一下。在不同的场景中，对于分词颗粒度的选择是不同的。下面举两个例子。</p>
<ul>
<li>切词颗粒度：<ul>
<li>粗颗粒度：适合推荐场景<ul>
<li>例子：分成 =&gt; /环球影城/<ul>
<li>推荐环球影城有关的</li>
<li>而不是跟环球有关，或者其他跟影城有关的</li>
</ul>
</li>
</ul>
</li>
<li>细颗粒度：适合搜索场景<ul>
<li>例子：需要分成 =&gt; /环球/影城/</li>
<li>保证召回质量，出现的词都涉及到</li>
<li>而不是只召回了有关<strong>环球影城</strong>的，其余的召回就全是无关的。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>也就是说对于推荐场景来说，哪怕漏掉一些也没关系，但一定要准；但对于搜索来说，要保证召回的全面。</strong></p>
<h2 id="0x02-中文分词的方法"><a href="#0x02-中文分词的方法" class="headerlink" title="0x02 中文分词的方法"></a>0x02 中文分词的方法</h2><ul>
<li>基于规则字典：<ul>
<li>最大长度匹配（正、逆、双序）<a href="https://zhuanlan.zhihu.com/p/103392455">中文分词算法之–最大匹配法 - 知乎</a></li>
<li>加入前缀树改进最大长度匹配</li>
</ul>
</li>
<li>基于机器学习：<ul>
<li>HMM——双序列标注</li>
<li>CRF——标注训练</li>
</ul>
</li>
</ul>
<h2 id="0x03-案例1：基于字典匹配的方法"><a href="#0x03-案例1：基于字典匹配的方法" class="headerlink" title="0x03 案例1：基于字典匹配的方法"></a>0x03 案例1：基于字典匹配的方法</h2><p>在字典树里寻找逐字寻找匹配的词，如果已经匹配切分好的词，那么结束这个词，下一个字符从新的字符开始。</p>
<p>那么如何实现？首先基于<strong>前缀树的数据结构</strong>构成字典树，然后通过<strong>概率语言模型</strong>预测分词结果。</p>
<h3 id="1-一种重要的数据结构（前缀树）"><a href="#1-一种重要的数据结构（前缀树）" class="headerlink" title="1. 一种重要的数据结构（前缀树）"></a>1. 一种重要的数据结构（前缀树）</h3><p>Trie 树，即字典树，又称单词查找树或者键树，是一种树形结构，是一种哈希树的变种，常被搜索引擎系统用于文本词频统计。</p>
<p><strong>优点</strong>： 最大限度地减少无谓的字符串比较，查询效率高于哈希表。</p>
<p>有了字典树，就可以实现基于最大长度分词，如下。</p>
<p>正向和逆向最大长度匹配（对输入字符串开始匹配方向不同）：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031719270.png"></p>
<p>正向最大长度匹配对字符串从前向后匹配，逆向最大长度匹配从后向前匹配。</p>
<p>但是，仅仅基于最大长度匹配的分词方法<strong>靠谱吗？</strong></p>
<p>正向与反向最大长度匹配都可能出现特例，比如：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031724760.png"></p>
<p>在这个例子中，正向不如反向的效果。一般来讲，因为中文比较复杂以及中文的特殊性，逆向最大匹配大多时候往往会比正向要准确。</p>
<p>由此也可以发现只通过最大长度匹配来分词是不够的。</p>
<p>所以，需要借助<strong>概率语言模型</strong>来完善分词的效果。</p>
<h3 id="2-概率语言模型"><a href="#2-概率语言模型" class="headerlink" title="2. 概率语言模型"></a>2. 概率语言模型</h3><h4 id="基础原理"><a href="#基础原理" class="headerlink" title="基础原理"></a>基础原理</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031731152.png"></p>
<blockquote>
<p>输入=&gt; 对于输入的字符串，以字为单位，根据字典，将所有可能的词都连起来，构成一个有向无环图（DAG）。<br>如何输出？=&gt; 将所有可能的词的组合概率都计算出来，然后取概率最高的组合。</p>
</blockquote>
<p>==输入==：字串 C = c1, c2, …, cn（以字为单位）<br>==输出==：词串 S = w1, w2, …, wn （以词为单位）</p>
<p>从输入到输出，由条件概率 P(S|C) 来衡量——P(S|C)：<strong>字串 C</strong> 产生<strong>切分 S</strong> 的概率。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031753916.png"></p>
<blockquote>
<p>说明：上面式子的变形基于贝叶斯公式。<br>P(A|B) = P(A,B) / P(B) = P(B|A) * P(A) / P(B)<br>P(A, B) = P(B|A) * P(A) = P(A|B) * P(B)</p>
</blockquote>
<p>=&gt; 上面的式子可以继续化简。<br>**因为 P(C) = 1 并且 P(C|S) = 1，上式只剩下 P(S)**。P(C) = 1 很容易理解，那么为什么 P(C|S) = 1？结合下面的例子理解。</p>
<p>==举个栗子：==<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206032350989.png"></p>
<p>以 P(C|S1) 为例，S1: 南京市/长江/大桥，C：南京市长江大桥，所以 S1 去掉那两个分词斜杠就得到了 C，可得根据 S1 必然能得到 C。因此 P(C|S1) = 1 对于其他的词串字串也一样。</p>
<p>对于字串 C 的分词结果 =&gt; seg(C) = argmax P(S) ：令 P(S) 取到最大的 S。</p>
<p>那么在本例就只需要计算 P(S1) 与 P(S2)。</p>
<blockquote>
<p>如何计算 P(S1)？<br>P(S1) = P(南京市)*P(长江)*P(大桥)<br>以 P(南京市) 为例。<br>假设有十篇文章，词语总量为 100，<strong>南京市</strong>总共出现了 2 次。<br>那么 P(南京) = 2 / 100 = 0.02<br>以此类推，算出所有概率。</p>
</blockquote>
<p>==优化：==<br>概率连乘，而且这些概率一般都是小数点后几位，比如 <code>0.00002</code>,那么概率连乘后，最后得到的概率会在小数点后很多位，因此会导致计算机的精度不够。如何解决？</p>
<p>=&gt; 取 log。</p>
<ul>
<li>取 log 的作用<ul>
<li>防止向下溢出</li>
<li>加法比乘法快</li>
</ul>
</li>
</ul>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051658209.png"><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051659302.png"><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051700333.png"></p>
<h4 id="N-元语言模型（N-Gram）"><a href="#N-元语言模型（N-Gram）" class="headerlink" title="N 元语言模型（N-Gram）"></a>N 元语言模型（N-Gram）</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051704065.png"></p>
<ul>
<li>N-Gram<ul>
<li>Unigram model：不考虑前面出现的词</li>
<li>Bigram model：考虑前面的一个词</li>
<li>Trigram model：考虑前面的两个词</li>
</ul>
</li>
</ul>
<p>如何计算 N-Gram 的概率？</p>
<p>以“广州/本田/雅阁/汽车”为例。</p>
<ul>
<li>一元：P（广州，本田，雅阁，汽车）= P(广州) * P(本田) * P(雅阁) * P(汽车) </li>
<li>二元：P（广州，本田，雅阁，汽车）= P(广州) * P(本田|广州) * P(雅阁|本田) * P(汽车|雅阁) </li>
<li>三元：P（广州，本田，雅阁，汽车）= P(广州) * P(本田|广州) * P(雅阁|本田，广州) * P(汽车|雅阁，本田)</li>
</ul>
<p>以 P(本田|广州) 为例，怎么计算？</p>
<p>P(本田|广州) = <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.238ex;" xmlns="http://www.w3.org/2000/svg" width="14.16ex" height="3.607ex" role="img" focusable="false" viewBox="0 -1047.1 6258.8 1594.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">本</text></g><g data-mml-node="mi" transform="translate(3840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">田</text></g><g data-mml-node="mi" transform="translate(4840,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(5840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">广</text></g><g data-mml-node="mi" transform="translate(6840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">州</text></g><g data-mml-node="mo" transform="translate(7840,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(1280.7,-370.3) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">广</text></g><g data-mml-node="mi" transform="translate(3840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">州</text></g><g data-mml-node="mo" transform="translate(4840,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="6018.8" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></p>
<p>即计算<strong>本田和广州</strong>同时出现的次数（一定要广州在前，本田在后），以及<strong>广州</strong>出现的次数。</p>
<h3 id="3-实践"><a href="#3-实践" class="headerlink" title="3. 实践"></a>3. 实践</h3><h4 id="方法一：最大长度匹配（反向）"><a href="#方法一：最大长度匹配（反向）" class="headerlink" title="方法一：最大长度匹配（反向）"></a>方法一：最大长度匹配（反向）</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">import sys

WordDic = {}
MaxWordLen = 1

# 写入字典
# 并且找到词语最大长度
def LoadLexicon(lexiconFile):
    global MaxWordLen
    infile = open(lexiconFile, 'r', encoding='gb2312')
    s = infile.readline().strip()  # 每次读一行
    while len(s) &gt; 0:  # 通过判断每行的长度，放在 while 循环里读完整个文件
        #s = s.decode("gb2312")
        WordDic[s] = 1  # 读到的每行的词，放入字典
        if len(s) &gt; MaxWordLen:
            MaxWordLen = len(s)  # 记录词语最大长度
        s = infile.readline().strip()
    infile.close()

def BMM(s):
    global MaxWordLen
    wordlist = []
    i = len(s)  # 句子长度为 i，代表有 i 个字
    while i &gt; 0:
        start = i - MaxWordLen  # 从后向前，以最大长度匹配
        if start &lt; 0:
            start = 0
        while start &lt; i:
            tmpWord = s[start:i]  # 框起来 start 到 i 的字
            if tmpWord in WordDic:
                wordlist.insert(0, tmpWord)  # 在字典中找到匹配的词，那么放入 wordlist
                break
            else:
                start += 1
        # 如果 start 一直向后移动，直到 i（句尾）还没有匹配的词
        # 那么将最后的一个字作为一个分词结果
        if start &gt;= i:
            wordlist.insert(0, s[i-1:i])
            start = i - 1
        i = start
        # 因为 start~i的字已经构成了分词结果
        # 所以下一次循环中，句尾坐标 i 重新赋值，左闭右开，所以不包括 start
    return wordlist

def PrintSegResult(wordlist):
    print("After word-seg:")
    for i in range(len(wordlist)-1):
        print(wordlist[i])
    print(wordlist[len(wordlist)-1])

LoadLexicon("./lexicon.txt")

inputStr = u"南京市长江大桥"

wordlist = BMM(inputStr)
PrintSegResult(wordlist)
# 输出：
# After word-seg:
# 南京
# 市
# 长江
# 大
# 桥<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="方法二：最大概率-Bigram"><a href="#方法二：最大概率-Bigram" class="headerlink" title="方法二：最大概率(Bigram)"></a>方法二：最大概率(Bigram)</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Step 1.
# convert token to id
python CreateLexicon.py

Step 2.
# gen lang model
python BiLMTrain.py

Step 3.
# viterbi
python ViterbiCWS.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h3><ul>
<li>中文分词<ul>
<li>容易区分的——登陆词（词表中已有的）=&gt; 字典树（Trie）</li>
<li>不容易区分的——未登录词（新词）=&gt; 单字</li>
</ul>
</li>
</ul>
<p>通过上面的学习，我们知道中文分词是这样处理的：对于登录词，使用字典树，对于未登录词，分为单字。</p>
<p>那么如何才能更合理的处理未登录词呢？比如 <code>广州/本田/雅/阁/汽车</code>，如何将雅阁分为一个词呢？</p>
<p>=&gt; <strong>隐马尔可夫模型</strong></p>
<blockquote>
<p>隐马模型可以像胶水一样，将他认为合适的词粘到一起，合并为一个词。</p>
</blockquote>
<p>最后重新总结一下中文分词的技术点，并且引出后面要学习的内容——动态规划。</p>
<ul>
<li>中文分词的技术<ul>
<li>字典树：处理登录词</li>
<li>隐马模型：处理未登录词</li>
<li>动态规划（Viterbi 算法）：支撑以上两个技术</li>
</ul>
</li>
</ul>
<h2 id="0x04-动态规划——Viterbi-算法"><a href="#0x04-动态规划——Viterbi-算法" class="headerlink" title="0x04 动态规划——Viterbi 算法"></a>0x04 动态规划——Viterbi 算法</h2><p>多步骤，每步多选择模型的最优选择问题。</p>
<p>每一步的所有选择都保存了前序步骤到当前步骤的最优选择。</p>
<p>依次计算完所有步骤后，通过回溯的方法找到最优选择路径。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206112109344.png"></p>
<p>这里不是很容易理解，需要好好思考一下。<a href="https://www.zhihu.com/question/20136144">如何通俗地讲解 viterbi 算法？ - 知乎</a></p>
<p>Viterbi 与枚举的区别到底在哪？以上图为例，枚举的话需要 <code>3×3×3</code> 次计算，Viterbi 需要 <code>3×3×2</code> 次计算。假设每一步有 N 种选择，一共有 L 步。那么枚举的时间复杂度为 O(N^L)，Viterbi 的时间复杂度为 O(N^2*L)，也就是说当这个序列越长（步数 L 越大）时，两种方法的复杂度差距会越大。这时，Viterbi 的优势就体现出来了。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206112117841.png"></p>
<p>实现参考上一节的实践方法二中 <code>ViterbiCWS.py</code>。</p>
<h2 id="0x05-案例2：复用轮子——jieba"><a href="#0x05-案例2：复用轮子——jieba" class="headerlink" title="0x05 案例2：复用轮子——jieba"></a>0x05 案例2：复用轮子——jieba</h2><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">方式1：

pip install jieba

方式2：

先下载 http://pypi.python.org/pypi/jieba/

然后解压，运行 python setup.py install

// 具体信息可参考github文档：[https://github.com/fxsjy/jieba](https://link.zhihu.com/?target=https://github.com/fxsjy/jieba)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="2-整体逻辑"><a href="#2-整体逻辑" class="headerlink" title="2. 整体逻辑"></a>2. 整体逻辑</h3><p>梳理一下 <code>jieba</code> 分词的逻辑，也是重新复习一遍中文分词的相关技术。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122110155.png"></p>
<p>整体逻辑如上，对于登录词通过前两步处理，对于未登录词，使用 HMM。</p>
<p>进一步拆分：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122111277.png"></p>
<blockquote>
<p>对于剩下的未登录词，不知道这些字应不应该粘到一起，使用 HMM 判断。</p>
</blockquote>
<h3 id="3-举例子"><a href="#3-举例子" class="headerlink" title="3. 举例子"></a>3. 举例子</h3><p>这里用一个例子带入上面的分词整体逻辑，理解分词的细节。</p>
<blockquote>
<p>句子：<strong>广州本田雅阁汽车</strong></p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122117079.png"></p>
<p>构成 ==DAG 表示==：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122119587.png"></p>
<p>如图可见，每个字用一个 id 表示。<code>0:[0,1,3]</code> 代表通过查字典，0 到 0：广，可以作为一个词；0 到 1：广州，可以作为一个词；0 到 3：广州本田，可以作为一个词。以此类推。</p>
<p>每种分词结果，比如 <code>广州、广州本田、汽车</code> 都有自己的概率。下一步就是找出概率最大的路径组合。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122124565.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122124567.png"></p>
<p>以上的概率是怎么算的？是根据词表（字典）算的。</p>
<p>以 <code>汽车</code> 为例。比如汽车在词表中的词频为 10，然后词表中所有词的词频相加为 1000，那么 p(汽车)= log(10/1000)，为了防止下溢，取个 log。</p>
<h2 id="0x06-案例3：基于统计机器学习的方法"><a href="#0x06-案例3：基于统计机器学习的方法" class="headerlink" title="0x06 案例3：基于统计机器学习的方法"></a>0x06 案例3：基于统计机器学习的方法</h2><h3 id="1-马尔科夫模型"><a href="#1-马尔科夫模型" class="headerlink" title="1. 马尔科夫模型"></a>1. 马尔科夫模型</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p><strong>每个状态只依赖之前有限个状态。</strong></p>
<ul>
<li>N 阶马尔科夫：依赖之前 n 个状态</li>
<li>1 阶马尔科夫：仅仅依赖前一个状态</li>
</ul>
<blockquote>
<p>所以二元语言模型相当于 1 阶马尔科夫。</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122154778.png"></p>
<ul>
<li>参数：<ul>
<li>状态（S1, S2, …）——其实在这里就相当于词语</li>
<li>初始概率<ul>
<li>比如共 1000 篇文章，其中 100 篇文章以 <code>今天</code> 作为开头</li>
<li>那么 <code>今天</code> 的初始概率就为 100/1000</li>
</ul>
</li>
<li>状态转移概率<ul>
<li>其实就是 N-Gram 中，在前 n 个词出现的条件下，当前词出现的概率</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>==最重要的两类概率：==<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122157720.png"></p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>p(w1=今天，w2=我，w3=写，w4=了，w5=一个，w6=程序)<br>= p(w1=今天)p(w2=我|w1=今天)p(w3=写|w2=我)……p(w6=程序|w5=一个)</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122202063.png"></p>
<h4 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h4><p>马尔科夫是单序列模型，可以解决的业务问题有限。比如机器翻译、语音识别这种工作，马尔科夫是做不了的。</p>
<ul>
<li>机器翻译：源语言序列 &lt;–&gt; 目标语言序列</li>
<li>语音识别：语音信号序列 &lt;–&gt; 文字序列</li>
</ul>
<h3 id="2-隐马尔科夫模型-HMM"><a href="#2-隐马尔科夫模型-HMM" class="headerlink" title="2. 隐马尔科夫模型 (HMM)"></a>2. 隐马尔科夫模型 (HMM)</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p>马尔科夫是单序列建模，隐马是双序列建模。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122213974.png"></p>
<ul>
<li>参数：<ul>
<li>状态序列（未知）</li>
<li>初始概率</li>
<li>状态转移概率</li>
<li>观察序列（已知）</li>
<li>发射概率</li>
</ul>
</li>
</ul>
<p>以机器翻译为例，解释一下上面的五个参数。比如翻译 <code>我爱你 —&gt; I love you</code>。</p>
<ul>
<li><code>我爱你</code>，是已知的观察序列；<code>I love you</code> 是未知的状态序列（想要得到的）。</li>
<li>初始概率和状态转移概率同之前的马尔科夫模型一样，即代表 <code>I</code> 作为开头的概率，以及在前面一个词为 <code>I</code> 的条件下，下一个状态（词）为 <code>love</code> 的概率，等等。</li>
<li>发射概率：从状态序列到观察序列的概率。也就是说，<code>I</code> 翻译为 <code>我</code> 的概率，<code>love</code> 翻译为 <code>爱</code> 的概率，以此类推。</li>
</ul>
<blockquote>
<p>梳理一下整个模型的过程：<br>首先完成第一状态，然后依次由当前状态生成下一状态，最后每个状态发射出一个观测值。</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206132248906.png"></p>
<p>因此，最后就是寻找一个最大的联合概率。</p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>那么在 <code>jieba 分词</code> 中，是如何运用隐马模型将那些未登录词的单个字符粘到一起的呢？</p>
<p>以 <code>广州塔</code> 为例，假设这个词是一个未登录词。那么输入模型就是以单字输入。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206132304514.png"></p>
<p>如上图，以单字输入模型。</p>
<p>那么每个字的状态有以下几种：</p>
<ol>
<li>B —— 词语开头</li>
<li>M —— 词语中间</li>
<li>E —— 词语结尾</li>
<li>S —— 单字</li>
</ol>
<blockquote>
<p>举例：<br>汽车——BE<br>面包车——BME<br>天天向上——BMME<br>哦——S</p>
</blockquote>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206132321801.png"></p>
<p>因此通过这四种状态，预测出一个状态序列，就可以得到分词结果。</p>
<p>那么对应着 HMM 的五个参数，依次分析如何实现分词的隐马模型。</p>
<ul>
<li><p>观察序列（已知）：广 州 塔</p>
</li>
<li><p>状态序列（未知）：需要预测的，预测每个位置到底是哪种状态（BMES其中之一）</p>
</li>
<li><p><strong>初始概率</strong>（状态序列）</p>
<ul>
<li>四种概率：P(B), P(M), P(E), P(S)</li>
<li>比如，有 1000 篇文章，那么去统计这些文章的第一个字的状态。比如有 800 篇，第一个字的状态为 B；有 200 篇，第一个字的状态为 S。=&gt; 那么，P(B)=0.8，P(S)=0.2（不可能会有 E 或者 S 这两个状态，但是这两个状态概率要给一个很小的值，防止乘 0）。</li>
</ul>
</li>
</ul>
<blockquote>
<p>在 jieba 分词的代码中印证一下。</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git clone https://github.com/fxsjy/jieba.git

cd jieba/jieba/finalseg

cat prob_start.py

# 输出：
# P={'B': -0.26268660809250016,
# 'E': -3.14e+100,
# 'M': -3.14e+100,
# 'S': -1.4652633398537678}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到初始概率文件中一共有四种概率，其中 E 和 M 都是给了一个接近 0 的概率值。</p>
<ul>
<li><strong>转移概率</strong><ul>
<li>如 P(M|B), P(E|B) 等。</li>
<li>以 P(M|B) 为例，怎么计算？<ul>
<li>=&gt; <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.238ex;" xmlns="http://www.w3.org/2000/svg" width="9.502ex" height="3.607ex" role="img" focusable="false" viewBox="0 -1047.1 4199.7 1594.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mo" transform="translate(3891,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(4169,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(4928,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(689.9,-370.3) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(3599,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="3959.7" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li>
<li>统计一共有多少 B 状态，统计出现 B 状态后出现 M 状态的次数。然后相除。</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>在 jieba 分词的代码中印证一下。</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd jieba/jieba/finalseg

cat prob_start.py

# 输出：
# P={'B': {'E': -0.510825623765990, 'M': -0.916290731874155},
# 'E': {'B': -0.5897149736854513, 'S': -0.8085250474669937},
# 'M': {'E': -0.33344856811948514, 'M': -1.2603623820268226},
# 'S': {'B': -0.7211965654669841, 'S': -0.6658631448798212}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><strong>发射概率</strong><ul>
<li>如 P(广|B), P(州|M) 等。</li>
<li>以 P(广|B) 为例，怎么计算？<ul>
<li>=&gt; <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.238ex;" xmlns="http://www.w3.org/2000/svg" width="9.42ex" height="3.607ex" role="img" focusable="false" viewBox="0 -1047.1 4163.6 1594.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">广</text></g><g data-mml-node="mo" transform="translate(3840,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(4118,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(4877,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(671.8,-370.3) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(3599,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="3923.6" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li>
<li>统计一共有多少 B 状态，然后统计有多少状态是B，同时这个字是“广”的次数。然后相除。</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>在 jieba 分词的代码中印证一下。</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd jieba/jieba/finalseg

vim prob_emit.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>输出：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140002491.png"></p>
<p>以上这些概率，都是通过人工标注，然后统计得来。</p>
<p>总结一下，从具体落地实现的角度，分别需要三张词表。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140005456.png"></p>
<p>上面的概率包含了词性。在 <code>jieba</code> 的这个目录可以看到：<code>./jieba/jieba/posseg]</code>。</p>
<ul>
<li>其中<ul>
<li>BEMS 表示位置信息：B（开头）、M（中间）、E（结尾）、S（独立成词）</li>
<li>词性：n（名词）、nr（人名）、ns（地名）、v（动词）<ul>
<li>参考：<a href="https://blog.csdn.net/kevin_darkelf/article/details/39520881">中文分词词性对照表_kevin_darkelf的专栏-CSDN博客_ucn是什么词性的缩写</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>回顾之前的例子：<strong>广州塔</strong></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140008489.png"></p>
<p>对于 HMM 的应用，最典型的就是给定 O（观察序列），找最优的 S（状态序列）。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140012676.png"><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140011610.png"></p>
<h2 id="0x07-实践"><a href="#0x07-实践" class="headerlink" title="0x07 实践"></a>0x07 实践</h2><p>以分词为例，在实际业务落地过程中，需要<strong>批量处理</strong>与<strong>流式处理</strong>协同完成。比如当天24点之前的，昨天的数据使用批量处理；而今天过了24点，实时产生的数据使用流式处理。</p>
<p>使用 Hadoop 批量处理的数据可以追求模型效果，使用复杂的模型；而使用 Flink 等做流式处理的数据，需要考虑延时，追求高效性（避免阻塞），一般使用简单一些的模型，不要要求很完善的效果。</p>
<h3 id="1-批量分词"><a href="#1-批量分词" class="headerlink" title="1. 批量分词"></a>1. 批量分词</h3><blockquote>
<p>待更新</p>
</blockquote>
<h3 id="2-增量分词（流式处理）"><a href="#2-增量分词（流式处理）" class="headerlink" title="2. 增量分词（流式处理）"></a>2. 增量分词（流式处理）</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd ./nlp/1.nlp-foundation/1.2.chinese-segmentation/pyweb_pg_test

pip install web.py

python main.py 8888

# 在浏览器中输入：
# http://127.0.0.1:8888/?content=南京市长江大桥

# 或者
curl http://0.0.0.0:8888/\?content\=%E5%8D%97%E4%BA%AC%E5%B8%82%E9%95%BF%E6%B1%9F%E5%A4%A7%E6%A1%A5<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="0x08-作业"><a href="#0x08-作业" class="headerlink" title="0x08 作业"></a>0x08 作业</h2><p>作业：</p>
<ol>
<li>用自己的话，把“广州塔”的 HMM 逻辑大概介绍一下。</li>
<li>用自己的话，把 Viterbi 的逻辑大概介绍一下。</li>
</ol>
<p>面试题：</p>
<blockquote>
<p><strong>一道面试题：</strong><br>有一个非常非常长的字符串，标记为L，这个字符串里面的每个字符不限于字母和数字，由于字符串太大了，导致内存无法存储<br>另，有M个小的字符串，最大长度为N<br><strong>问题</strong>：统计出，这些M个小字符串，一共被包含多少次？</p>
<p><strong>答</strong>：同使用最大长度匹配做中文分词一样，小字符串构建前缀树，长字符串每次取 N 个字符去匹配小字符串，每次匹配的起点一位一位向后移动。</p>
</blockquote>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这篇笔记比较完备地记录了中文分词从理论到实践的全过程。</p>
<p><strong>中文分词的难点</strong>：</p>
<ul>
<li>中文不像英文天然分词，因此产生了中文分词的需求</li>
<li>以及中文分词在不同场景下的需求。<ul>
<li>比如搜索场景需要分的全，检索结果完善；</li>
<li>而推荐场景下，需要召回结果准确，不能分的太细，保留原始的完整语义。</li>
</ul>
</li>
</ul>
<p><strong>中文分词的方法</strong>：</p>
<ul>
<li>基于字典匹配<ul>
<li>基于最大长度匹配的方法</li>
<li>还有进一步改进，通过概率语言模型（N-Gram）优化的方法。</li>
<li>当然，无论是基于字典匹配或者后面的机器学习，词典都可以用前缀树来构建，提高运算的效率。</li>
</ul>
</li>
<li>基于机器学习方法<ul>
<li>HMM</li>
<li>CRF</li>
</ul>
</li>
</ul>
<p>接着也了解了动态规划——<strong>Viterbi 算法</strong>。HMM 以及 N-Gram 的实现都需要借助维特比算法，找到最大概率的分词组合。或者说，如下图，类似寻找最优路径的问题，从起点到终点，有 N 个步骤，每个步骤有 M 种选择。这样的问题都可以使用 Viterbi 算法，因为当 N 较大时，相比枚举，可节省大量时间空间。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206112109344.png"></p>
<p>最后学习了从单序列建模的马尔科夫模型到双序列建模的隐马模型。</p>
<p><strong>马尔科夫模型</strong></p>
<ul>
<li><strong>每个状态只依赖之前有限个状态</strong><ul>
<li>1 阶马尔科夫（只依赖之前一个状态） == 二元语言模型</li>
</ul>
</li>
<li>参数<ul>
<li>状态序列</li>
<li>初始概率</li>
<li>转移概率</li>
</ul>
</li>
</ul>
<p><strong>隐马尔科夫模型（HMM）</strong>：</p>
<ul>
<li>单序列建模的马尔科夫模型可以解决的问题有限，因此需要双序列的 HMM<ul>
<li>处理如机器翻译，语音识别，词性标注，拼音纠错等业务</li>
<li>机器翻译：源语言序列 &lt;–&gt; 目标语言序列</li>
<li>语音识别：源语音信号序列 &lt;–&gt; 文字序列</li>
<li>词性标注：源文字序列 &lt;–&gt; 词性序列<ul>
<li>写/一个/程序</li>
<li>Verb/Num/Noun</li>
</ul>
</li>
</ul>
</li>
<li>参数<ul>
<li>状态序列（未知）</li>
<li>观察序列（已知）</li>
<li>初始概率</li>
<li>转移概率</li>
<li>发射概率（从状态到观察）</li>
</ul>
</li>
</ul>
<p>并且也对以上的方法都进行了实践。不过批量分词里配置这个 Hadoop 集群环境真的，搞得太开心了 –!</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>算法：<br><a href="https://zhuanlan.zhihu.com/p/103392455">中文分词算法之–最大匹配法 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/32829048">自然语言处理中N-Gram模型介绍 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/161436964">维特比算法(viterbi)原理以及简单实现 - 知乎</a></p>
<p>环境配置：<br><a href="https://blog.csdn.net/u012757419/article/details/105431254">VMware虚拟机中Centos7网络配置及ping不通思路</a><br><a href="https://blog.csdn.net/ren9436/article/details/118864737">虚拟机Linux网络配置——Net模式（CentOS7）</a><br><a href="https://blog.csdn.net/weixin_45827423/article/details/122656775?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-122656775-blog-114282900.pc_relevant_antiscanv3&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-122656775-blog-114282900.pc_relevant_antiscanv3&utm_relevant_index=1">【2022】Centos7.4安装anaconda3</a><br><a href="https://blog.csdn.net/weixin_44768806/article/details/118365402">secureCRT连接不上虚拟机解决方案</a><br><a href="https://blog.csdn.net/weixin_40165004/article/details/121717405">SecureCRT连接虚拟机连接不上问题记录与解决</a></p>
]]></content>
      <categories>
        <category>FunRec</category>
        <category>01-自然语言文本处理基础</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>FunRec</tag>
      </tags>
  </entry>
  <entry>
    <title>2.1 传统机器学习 &amp; 朴素贝叶斯</title>
    <url>/1A9KV19/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>机器学习是深度学习的基础，这篇笔记是对机器学习这部分内容的一个开篇。了解什么是机器学习，然后学习一个经典的朴素贝叶斯模型，并运用到文本分类的实践。</p>
<span id="more"></span>

<h2 id="0x01-简介"><a href="#0x01-简介" class="headerlink" title="0x01 简介"></a>0x01 简介</h2><ul>
<li>人类学习：<ul>
<li>为了实现目标任务：考清华（任务）</li>
<li>通过一定训练过程：每天 10 道模拟题（数据）</li>
<li>逐步提高表现：降低错误（Loss）</li>
</ul>
</li>
<li>机器学习：<ul>
<li>模拟人类的学习，从数据中总结出规律经验，在新场景中预测</li>
<li><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101424405.png"></li>
<li>常用的数据驱动方法：<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101427986.png"></li>
<li>将上述方法用数据建模：<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101428872.png"></li>
</ul>
</li>
</ul>
<blockquote>
<p>总结：机器学习，基于大量历史数据，通过学习形成经验模型，利用模型指导具体业务。</p>
</blockquote>
<ul>
<li>对于 ML 最重要的两点：<strong>数据</strong> + <strong>模型</strong><ul>
<li>数据决定了高度</li>
<li>模型决定了达到高度的方法</li>
<li>从我的理解来讲，数据决定了下限，数据好的话出来的效果一般不会差（哪怕用简单的模型）；模型决定了上限，特别适合任务场景的模型会带来意想不到的效果。</li>
</ul>
</li>
</ul>
<p>模型训练流程：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101435078.png"></p>
<p>机器学习的定位：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101437939.png"></p>
<p>机器学习的任务类型：分类、回归、聚类和推荐<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101437563.png"></p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">目标</th>
</tr>
</thead>
<tbody><tr>
<td align="left">朴素贝叶斯</td>
<td align="left">- 侧重 NLP 文本处理<br>- 最容易上手的分类算法，实用性强</td>
</tr>
<tr>
<td align="left">LR 逻辑回归</td>
<td align="left">- 最重要、最核心、最常用的分类算法<br>- 常用于最核心的点击率模型等排序业务<br>- 解决二分类问题，深度学习必备基础</td>
</tr>
<tr>
<td align="left">Softmax</td>
<td align="left">- 可以当作 LR 逻辑回归的升级版<br>- 解决多分类问题，深度学习的必备基础</td>
</tr>
</tbody></table>
<h2 id="0x02-朴素贝叶斯的理论基础"><a href="#0x02-朴素贝叶斯的理论基础" class="headerlink" title="0x02 朴素贝叶斯的理论基础"></a>0x02 朴素贝叶斯的理论基础</h2><p>==任务：==给定一篇文章，预测出其类别（财经类、科技类、生活类等）</p>
<p>最直观的方法：将文章中所有的词作为词袋，基于提前构建好的规则（贝叶斯公式），得到预测结果。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101454182.png"></p>
<p>可能存在的问题：歧义、一词多义等。（如：苹果——手机？水果？）</p>
<p>先不去管这些问题，如果我们顺着这条道，怎么才能走通？</p>
<p>==公式：==基于贝叶斯原理的概率模型分类器<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101459652.png"></p>
<blockquote>
<p>补充：贝叶斯公式<br>P(A|B) = P(A,B) / P(B) = P(B|A) * P(A) / P(B)<br>P(A, B) = P(B|A) * P(A) = P(A|B) * P(B)</p>
</blockquote>
<blockquote>
<p>因为一篇文章 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.928ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 852 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g></g></g></svg></mjx-container> 由 n 个 词语 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="2.141ex" height="1.666ex" role="img" focusable="false" viewBox="0 -442 946.3 736.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container> 构成，所以不难理解公式推导最后一步时，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="7.971ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 3523.3 1044.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1992,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(2270,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(3134.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 等于各个词语（特征）连乘。</p>
</blockquote>
<p>换种方式表述：<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.172ex;" xmlns="http://www.w3.org/2000/svg" width="37.151ex" height="5.475ex" role="img" focusable="false" viewBox="0 -1460 16420.6 2420"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">类</text></g><g data-mml-node="mi" transform="translate(1892,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">别</text></g><g data-mml-node="mo" transform="translate(2892,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3170,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">特</text></g><g data-mml-node="mi" transform="translate(4170,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">征</text></g><g data-mml-node="mo" transform="translate(5170,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5836.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(6892.6,0)"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">特</text></g><g data-mml-node="mi" transform="translate(1892,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">征</text></g><g data-mml-node="mo" transform="translate(2892,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3170,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">类</text></g><g data-mml-node="mi" transform="translate(4170,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">别</text></g><g data-mml-node="mo" transform="translate(5170,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(5559,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(6310,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6699,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">类</text></g><g data-mml-node="mi" transform="translate(7699,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">别</text></g><g data-mml-node="mo" transform="translate(8699,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(2999.5,-710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">特</text></g><g data-mml-node="mi" transform="translate(2140,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">征</text></g><g data-mml-node="mo" transform="translate(3140,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="9288" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></p>
<p>参数：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101513762.png"></p>
<p>各概率的含义：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101504816.png"></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.303ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3228 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(892,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1709,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1987,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2839,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>：在指定文章的情况下，该文章类别为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.848ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 817 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 的概率 </p>
<p>==策略：==最大似然估计——MLE（如何得到模型中需要用到的两个概率）</p>
<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.308ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2346 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1140,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1957,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>：每个类别的先验概率<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101539570.png"></li>
</ul>
<p>例如：总共训练数据 1000 篇，其中军事类 300 篇，科技类 240 篇，生活类 140 篇。可得以下先验概率。</p>
<blockquote>
<p>P(军事)=0.3， P(科技)=0.24， P(生活)=0.14，……</p>
</blockquote>
<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.97ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3522.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1140,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2039,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(2317,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3133.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> ：在指定类别下，某个单词出现的概率<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101548374.png"></li>
</ul>
<p>例如：总共训练数据 1000 篇，其中<strong>军事类 300 篇</strong>，科技类 240 篇，生活类 140 篇。</p>
<p>已知在军事类新闻中，“谷歌”出现 15 篇，“投资”出现 9 篇，“上涨”出现 36 篇。可得以下概率。</p>
<blockquote>
<p>P(谷歌|军事)= 15/300 = 0.05， P(投资|军事)= 9/300 = 0.03，<br>P(上涨|军事)= 36/300 = 0.12，……</p>
</blockquote>
<blockquote>
<p>这里的统计其实有两种方式，一种是基于单词统计（以单词为单位），另一种是基于文章统计（以文章为单位）。</p>
<p>上面的例子是以文章为单位。在 300 军事类新闻中，“谷歌”出现 15 篇（不关心同一篇文章中重复出现的次数，只关心该词语在几篇文章中出现过），直接用 15 除以该类别文章总数 300。</p>
<p>如果以单词为单位，则是用每个单词出现的次数除以该类别总的单词出现次数。（建议不重复计数，具体原因在<a href="https://blog.csdn.net/qiaowu898/article/details/107634195">朴素贝叶斯原理分析及文本分类实战</a>有讲。）</p>
</blockquote>
<p>=&gt; <strong>小结：至此，便可以根据公式得到某篇文章是某个类别的概率。</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101459652.png"></p>
<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.308ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2346 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1140,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1957,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container><ul>
<li>P(军事)=0.3， P(科技)=0.24， P(生活)=0.14，……</li>
</ul>
</li>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.97ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3522.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1140,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2039,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(2317,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3133.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>：给定了一个 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.848ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 817 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>，去求各个特征（文章中的词语）的概率<ul>
<li>P(谷歌|军事)=0.05， P(投资|军事)=0.03， P(上涨|军事)=0.12，……</li>
<li>P(谷歌|科技)=0.15， P(投资|科技)=0.10， P(上涨|科技)=0.04，……</li>
<li>P(谷歌|生活)=0.08， P(投资|生活)=0.13， P(上涨|生活)=0.18，……</li>
<li>……</li>
</ul>
</li>
</ul>
<p>为什么说这种策略是最大似然估计呢？</p>
<p>下面补充一下概念。</p>
<blockquote>
<p>概率函数：在某个场景下（一定条件下），某个事件发生的概率（推结果）</p>
<p>似然函数：已知某个事件的发生（结果已知），该事件在不同条件下发生的可能性，（反推场景的参数 or 条件）</p>
<p>最大似然值（似然函数的最大值）：根据已知事件，推出产生这种结果的<strong>最有可能的</strong>条件（参数）</p>
</blockquote>
<p>结合当前例子理解 MLE 的理论，可以发现：最大似然估计，就是通过已知结果（数据），反推得到模型参数的过程。</p>
<p>==分类规则：==给定一篇文章 X，计算所有的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.303ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3228 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(892,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1709,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1987,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2839,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，选择概率值最大的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.848ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 817 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 作为输出（输出可能性最大的类别）</p>
<ul>
<li>给定文章<ul>
<li>X = {国内，投资，市场，……}</li>
</ul>
</li>
<li>计算各种类别概率<ul>
<li>P(军事|X)=P(国内|军事) * P(投资|军事) * P(市场|军事)……P(军事)<ul>
<li>文章所有的词为军事类别的概率（事前算好的）连乘</li>
</ul>
</li>
<li>P(科技|X)</li>
<li>P(生活|X)</li>
</ul>
</li>
</ul>
<p>==输出结果：==预测概率最大的类别作为输出<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207101840004.png"></p>
<ul>
<li>优缺点<ul>
<li>优点<ul>
<li>算法简单，常用于文本分类</li>
<li>兼容多分类任务</li>
<li>适合增量计算</li>
</ul>
</li>
<li>缺点<ul>
<li>顺序不敏感（词与词之间是离散的）</li>
<li>依赖先验概率（与数据分布强相关，有的类别多，有的类别少）</li>
<li>适合离散特征（基于独立同分布假设，同时也是这种假设，使得该方法简单，易于实现）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="0x03-朴素贝叶斯的实践"><a href="#0x03-朴素贝叶斯的实践" class="headerlink" title="0x03 朴素贝叶斯的实践"></a>0x03 朴素贝叶斯的实践</h2><p>代码放在了我的 repo 里：<a href="https://github.com/1nnoh/Dive-into-NLP/tree/main/2.ml-and-dl-foundation/2.1.naive-bayes-model">Dive-into-NLP/2.ml-and-dl-foundation/2.1.naive-bayes-model at main · 1nnoh/Dive-into-NLP · GitHub</a></p>
<h3 id="1-实践一"><a href="#1-实践一" class="headerlink" title="1. 实践一"></a>1. 实践一</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">python DataConvert.py data/ nb_data

# train
python NB.py 1 nb_data.train model

# test
python NB.py 0 nb_data.test model out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>在使用 <code>DataConvert.py</code> 做数据预处理这一步，我用的 wsl2 子系统会把隐藏文件一起处理。</p>
<p>所以在这行代码后面加一个筛选：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for filename in os.listdir(inpath):
	if not filename.startswith('.'):  # 筛除以 “.” 开头的隐藏文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h3 id="2-实践二（作业）"><a href="#2-实践二（作业）" class="headerlink" title="2. 实践二（作业）"></a>2. 实践二（作业）</h3><p>换个数据集 <code>/nb_homework/data</code>。</p>
<p>该数据集划分好了训练集测试集，但是没有分词。每篇文章如 <code>7_52.txt</code> 的命名，下划线之前为该文章类别。</p>
<pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown">数据：
	训练集：6300篇
	测试集：700篇

标签：1 财经；2 科技；3 汽车；4 房产；5 体育；6 娱乐；7 其他<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="0x04-评估指标"><a href="#0x04-评估指标" class="headerlink" title="0x04 评估指标"></a>0x04 评估指标</h2><h3 id="1-混淆矩阵"><a href="#1-混淆矩阵" class="headerlink" title="1. 混淆矩阵"></a>1. 混淆矩阵</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207102019239.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207102020521.png"></p>
<ul>
<li>准确度 Accuracy：(50 + 35) / (35 + 5 + 10 + 50) = 85%</li>
<li>精确率（查准率） Precision：50 / (50 + 5) = 90.9% （竖着看）</li>
<li>召回率 Recall：50 / (50 + 10) = 83.3% （横着看）</li>
</ul>
<blockquote>
<p>如何去看待精确率与召回率？<br>假设一个极端情况：还是上面的例子，模型认为 100 篇文章都是军事类，那么</p>
</blockquote>
<table>
<thead>
<tr>
<th></th>
<th>军事</th>
<th>科技</th>
</tr>
</thead>
<tbody><tr>
<td>军事（60）</td>
<td>60</td>
<td>0</td>
</tr>
<tr>
<td>科技（40）</td>
<td>40</td>
<td>0</td>
</tr>
</tbody></table>
<blockquote>
<p>求得 =&gt;</p>
<p>精确率：正确预测为军事的样本数 / 预测为军事的样本数<br>P = 60 / (60 + 40) = 60% </p>
<p>召回率：正确预测为军事的样本数 / 实际为军事的样本数<br>R = 60 / 60 = 100%</p>
</blockquote>
<p>所以不能追求单一的 P 或者 R 高，这两个指标需要协调兼顾。</p>
<h3 id="2-PR-曲线"><a href="#2-PR-曲线" class="headerlink" title="2. PR 曲线"></a>2. PR 曲线</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207102318656.png"></p>
<h2 id="0x05-面试题"><a href="#0x05-面试题" class="headerlink" title="0x05 面试题"></a>0x05 面试题</h2><blockquote>
<ol>
<li>朴素贝叶斯算法，对缺失值、异常值是否敏感？</li>
</ol>
<p>答：敏感。比如对于没有计算过概率的词语，只能赋给一个默认值。</p>
</blockquote>
<blockquote>
<ol start="2">
<li>朴素贝叶斯为什么适合增量计算？</li>
</ol>
<p>答：因为朴素贝叶斯本质是计算概率，这些概率是从统计得到。比如之前有 1000 篇文章，后面增加了 500 篇，只需要在之前的基础上统计新的数据，得到新的概率就可以了。或者说，模型可以比较容易地基于之前的模型，继续训练。</p>
</blockquote>
<blockquote>
<ol start="3">
<li>朴素贝叶斯的优缺点？</li>
</ol>
<p>答：优点：简单，常用于文本分类；适合增量计算。缺点：数据离散，词语之前不连续，对位置信息不敏感；依赖先验概率。</p>
</blockquote>
<h2 id="0x06-作业题"><a href="#0x06-作业题" class="headerlink" title="0x06 作业题"></a>0x06 作业题</h2><p>就是第三章实践二的内容，做完数据预处理之后，其他的同实践一。</p>
<blockquote>
<p>Accuracy: 0.8657142857142858<br>Precision and recall for Class 1 : 0.8021978021978022 0.73<br>Precision and recall for Class 3 : 0.9595959595959596 0.95<br>Precision and recall for Class 5 : 1.0 0.95<br>Precision and recall for Class 4 : 0.9318181818181818 0.82<br>Precision and recall for Class 6 : 0.868421052631579 0.99<br>Precision and recall for Class 2 : 0.831858407079646 0.94<br>Precision and recall for Class 7 : 0.68 0.68</p>
</blockquote>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>介绍了什么是机器学习。然后学习了朴素贝叶斯模型，并用于文本分类实践。</p>
<p><strong>重点公式</strong>：<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.172ex;" xmlns="http://www.w3.org/2000/svg" width="24.194ex" height="5.475ex" role="img" focusable="false" viewBox="0 -1460 10693.6 2420"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(1899,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2177,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(2927,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3593.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(4649.6,0)"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(1890,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2168,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(2927,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(3316,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(4067,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4456,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(5215,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(1882.5,-710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(1890,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="5804" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></p>
<blockquote>
<p>推导：<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.172ex;" xmlns="http://www.w3.org/2000/svg" width="19.394ex" height="5.475ex" role="img" focusable="false" viewBox="0 -1460 8572.2 2420"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(1899,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(2177,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(2927,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3593.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(4649.6,0)"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(1890,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2334.7,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(3093.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(821.8,-710)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(751,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1140,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(1890,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="3682.7" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></p>
</blockquote>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>理论：<br><a href="https://blog.csdn.net/u014182497/article/details/82252456">最大似然估计(Maximum likelihood estimation)</a><br><a href="https://zhuanlan.zhihu.com/p/26262151">带你理解朴素贝叶斯分类算法 - 知乎</a></p>
<p>实践：<br><a href="https://blog.csdn.net/qiaowu898/article/details/107634195">朴素贝叶斯原理分析及文本分类实战</a> 代码注释很详细，而且以单词为单位，以文章为单位的两种统计方式都做了实现。</p>
<p>代码：<br><a href="https://github.com/1nnoh/Dive-into-NLP/tree/main/2.ml-and-dl-foundation/2.1.naive-bayes-model">Dive-into-NLP/2.ml-and-dl-foundation/2.1.naive-bayes-model at main · 1nnoh/Dive-into-NLP · GitHub</a></p>
]]></content>
      <categories>
        <category>FunRec</category>
        <category>02-机器学习与深度学习基础</category>
      </categories>
      <tags>
        <tag>FunRec</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>2.2 逻辑回归-part1</title>
    <url>/3E6EVE1/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><ul>
<li>回归：假设现在有一些数据点，我们用一条直线对这些点进行拟合（这条直线称为最佳拟合直线），<strong>这个拟合的过程就叫做回归</strong>。</li>
<li>线性回归：假设因变量和自变量之间是线性关系（拟合出一条直线）。</li>
<li>逻辑回归和线性回归都是<strong>广义的线性回归模型的特例</strong>。</li>
<li>线性回归只能用于<strong>回归问题</strong>，逻辑回归则用于分类问题（可由二分类推广至多分类）。</li>
<li>线性回归使用<strong>最小二乘法</strong>作为参数估计方法，逻辑回归使用<strong>极大似然法</strong>作为参数估计方法。</li>
<li>逻辑回归去除 Sigmoid 函数就是线性回归，可以说线性回归是逻辑回归的理论基础。逻辑回归通过 Sigmoid 函数引入了非线性因素，得以解决分类问题。</li>
</ul>
<span id="more"></span>

<h2 id="0x01-LR-逻辑回归的理论"><a href="#0x01-LR-逻辑回归的理论" class="headerlink" title="0x01 LR 逻辑回归的理论"></a>0x01 LR 逻辑回归的理论</h2><p>以一个例子开始：《信用舆情预测》<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207130030484.png" style="zoom: 60%;"></p>
<p>学习输入到输出的映射：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.424ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3723.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(1823.6,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(2373.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2762.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3334.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p>
<ul>
<li>x：输入（个人的信息）</li>
<li>y：输出（预测逾期风险）</li>
<li>标签为 1 代表逾期。</li>
</ul>
<h3 id="1-建模"><a href="#1-建模" class="headerlink" title="1. 建模"></a>1. 建模</h3><p>定义一个条件概率：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.93ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2621 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1382,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1660,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2232,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，相当于用模型来捕获 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container> 与 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 490 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 之间的关系。</p>
<p>如果令 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="16.911ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 7474.8 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1382,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1660,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2232,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2898.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(3954.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(5251.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(6045.6,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(7045.8,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>，由于这样一个线性回归的式子的输出（因变量）在一个实数范围内，并不能满足二分类的需求（输出 0~1 的区间），因此需要一个激活函数来将最后的输出控制到 (0,1) 区间。这个激活函数就是 Sigmoid 函数。</p>
<p>Sigmoid 函数：<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.738ex;" xmlns="http://www.w3.org/2000/svg" width="12.42ex" height="4.774ex" role="img" focusable="false" viewBox="0 -1342 5489.6 2110"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(1823.6,0)"><g data-mml-node="mn" transform="translate(1583,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msup" transform="translate(1722.4,0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(499,289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></g><rect width="3426" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="24.781ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 10953.1 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(849.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mo" transform="translate(1794.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(2183.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(2961.6,0)"><path data-c="221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></g><g data-mml-node="mo" transform="translate(3961.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(4406.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(5184.2,0)"><path data-c="221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></g><g data-mml-node="mo" transform="translate(6184.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6573.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(7017.9,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(7785.7,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mo" transform="translate(8730.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(9119.4,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(9619.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(10064.1,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(10564.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207130046560.png" style="zoom:50%;"></p>
<p>Sigmoid 函数通常记作 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.346ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1921 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(571,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(960,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1532,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>。</p>
<p>因此如果将 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="7.964ex" height="2.09ex" role="img" focusable="false" viewBox="0 -841.7 3520.2 923.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(1296.8,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2091,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(3091.2,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container> 作为自变量带入到 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.346ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1921 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(571,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(960,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1532,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，那么就可以得到一个因变量（输出）在 (0,1) 区间的函数。</p>
<p>即 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="19.963ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 8823.8 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1382,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1660,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2232,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2898.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3954.6,0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(4525.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(4914.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(6211.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(7005.6,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(8005.8,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(8434.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>。</p>
<blockquote>
<p>补充：<br>通常在公式中如果出现带有转置符号的参数，如 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.934ex" height="1.929ex" role="img" focusable="false" viewBox="0 -841.7 1296.8 852.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container>，一般默认为一个列向量的转置，即行向量<br>若参数不带有转置符号，如 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>，一般默认一个列向量。</p>
</blockquote>
<p>如下，以刚开始的例子，需要对最下面一行的人做信用预测。将他的数据带入逻辑回归公式，就可以得到预测结果。对 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="5.257ex" height="1.971ex" role="img" focusable="false" viewBox="0 -666 2323.6 871"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1823.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container> 或者 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="5.257ex" height="1.971ex" role="img" focusable="false" viewBox="0 -666 2323.6 871"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1823.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container> 做预测都一样，因为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="32.322ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 14286.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1659.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2715.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3215.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(3493.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(4065.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(4510.2,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(5226.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5893,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(6948.8,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(7671,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(8671.2,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(9174.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9563.2,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(10331,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(11386.8,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(11886.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(12164.8,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(12736.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(13181.4,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(13897.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>。</p>
<img data-src=" https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207130110477.png" style="zoom: 50%;">
<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207130130300.png" style="zoom: 50%;">

<p>如上，对于二分类问题可以合并到一条公式来表示。<img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207130139038.png" style="zoom:50%;"></p>
<h3 id="2-目标函数"><a href="#2-目标函数" class="headerlink" title="2. 目标函数"></a>2. 目标函数</h3><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207130144842.png" style="zoom: 40%;">

<p>目标函数是数据集中所有样本预测结果的正确概率的连乘。</p>
<p>那么最大化目标函数，就意味着，尽可能使更多的样本预测结果正确，使得所有的预测都是对的。或者说，去寻找一条完美的决策边界，将正负样本准确无误地分到两侧。也就是找到最合适的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.62ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 716 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container> 与 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>。</p>
<p>这里也反应了<strong>回归</strong>的本质。线性回归是用一条直线尽可能的去拟合数据中的点 (x,y)，而逻辑回归的目的，<strong>也是去拟合出一条直线（决策边界）</strong>，使得正负样本可以尽可能准确地被划分在边界两侧。</p>
<h3 id="3-最大化目标函数"><a href="#3-最大化目标函数" class="headerlink" title="3. 最大化目标函数"></a>3. 最大化目标函数</h3><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202207130154005.png" style="zoom: 67%;">

<p>求解最大化目标函数的过程中，引入 log 将连乘变为相加，并且避免精度越界。取负号，将求解最大值问题转化为求解最小值。</p>
<p>最后将二分类问题公式代入。得到了形如交叉熵损失函数的公式。</p>
<p>至此，有了优化的目标：最小化这个交叉熵函数。如何实现？那就要依赖下面将要介绍的优化算法，如梯度下降法等。</p>
<h3 id="4-优化算法"><a href="#4-优化算法" class="headerlink" title="4. 优化算法"></a>4. 优化算法</h3><ul>
<li>GD (Gradient Descent)<ul>
<li>假设 f(w) 是关于 w 的凸函数，要求解该函数的最小值。那么梯度下降法的公式如下。</li>
<li><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="17.609ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7783 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(993.8,0)"><g data-mml-node="text"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="text" transform="translate(278,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g></g><g data-mml-node="mi" transform="translate(2327.6,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(3265.8,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(4266,0)"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g><g data-mml-node="mi" transform="translate(4906,0)"><path data-c="2207" d="M46 676Q46 679 51 683H781Q786 679 786 676Q786 674 617 326T444 -26Q439 -33 416 -33T388 -26Q385 -22 216 326T46 676ZM697 596Q697 597 445 597T193 596Q195 591 319 336T445 80L697 596Z"></path></g><g data-mml-node="mi" transform="translate(5739,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(6289,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6678,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(7394,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
<li>以一定的步长 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 640 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g></g></g></svg></mjx-container> 向梯度 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.509ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2877 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="2207" d="M46 676Q46 679 51 683H781Q786 679 786 676Q786 674 617 326T444 -26Q439 -33 416 -33T388 -26Q385 -22 216 326T46 676ZM697 596Q697 597 445 597T193 596Q195 591 319 336T445 80L697 596Z"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(1383,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1772,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(2488,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 增长的反方向移动，直到函数值变化极小时停止。</li>
<li>因为梯度指向<strong>函数增长最快的方向</strong>，如果想要求得最小值，那么很自然的，就应该向梯度的反方向移动 w。一般将梯度记为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.097ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4021 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(477,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(928,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1457,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(1977,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(2527,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2916,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(3632,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 或者 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.509ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2877 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="2207" d="M46 676Q46 679 51 683H781Q786 679 786 676Q786 674 617 326T444 -26Q439 -33 416 -33T388 -26Q385 -22 216 326T46 676ZM697 596Q697 597 445 597T193 596Q195 591 319 336T445 80L697 596Z"></path></g><g data-mml-node="mi" transform="translate(833,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(1383,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1772,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(2488,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>。</li>
<li>梯度下降用于求最小值，梯度上升用于求最大值。</li>
</ul>
</li>
<li>SGD (Stochastic Gradient Descent)<ul>
<li>与 GD 的区别在于：GD 每次迭代需要对整个 epoch 的数据算 Loss，然后更新 w；SGD 则是取数据中的一个样本计算 Loss 然后更新 w。</li>
<li>因此 GD 运算量大，占用内存多，时间长，但可以找到最优解；SGD 运算量少，更快，但不一定会找到最优解。</li>
</ul>
</li>
<li>Mini-batch Gradient Descent<ul>
<li>是对 GD 与 SGD 折中的方法。每次取一个小 batch 做梯度更新。</li>
</ul>
</li>
</ul>
<p>一般使用随机梯度下降 SGD。</p>
<h4 id="4-1-梯度下降法-GD"><a href="#4-1-梯度下降法-GD" class="headerlink" title="4.1. 梯度下降法 GD"></a>4.1. 梯度下降法 GD</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/image.jpeg"></p>
<ul>
<li>梯度下降求解<ul>
<li>对 w 求导 <img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208012339295.png"><ul>
<li>上面三行是对目标函数的分解</li>
<li>下面两行是对目标函数中 w 的求导</li>
<li>有趣的是，在中括号中间的两项，左项其实就是等同第一行的预测值，右项即已知的真实值。</li>
</ul>
</li>
<li>对 b 求导 <img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208012341652.png"><ul>
<li>上面三行也是对目标函数的分解</li>
<li>下面两行是对目标函数中 b 的求导</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="4-2-随机梯度下降-SGD"><a href="#4-2-随机梯度下降-SGD" class="headerlink" title="4.2. 随机梯度下降 SGD"></a>4.2. 随机梯度下降 SGD</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208012344285.png"></p>
<h2 id="0x02-实践1：LR-在简单数据集上的分类案例"><a href="#0x02-实践1：LR-在简单数据集上的分类案例" class="headerlink" title="0x02 实践1：LR 在简单数据集上的分类案例"></a>0x02 实践1：LR 在简单数据集上的分类案例</h2><p>REPO：<a href="https://github.com/1nnoh/Dive-into-NLP/tree/main/2.ml-and-dl-foundation/2.2.logistic-regression">Dive-into-NLP/2.ml-and-dl-foundation/2.2.logistic-regression at main · 1nnoh/Dive-into-NLP · GitHub</a></p>
<h3 id="1-案例描述"><a href="#1-案例描述" class="headerlink" title="1. 案例描述"></a>1. 案例描述</h3><p>在一个简单的数据集上，采用梯度下降法找到 Logistic 回归分类器在此数据集上的最佳回归系数。</p>
<h4 id="1-1-开发流程"><a href="#1-1-开发流程" class="headerlink" title="1.1. 开发流程"></a>1.1. 开发流程</h4><blockquote>
<p>数据采集: 可以使用任何方法<br>数据预处理: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳<br>数据分析: 画出决策边界<br>训练算法: 使用梯度下降找到最佳参数<br>测试算法: 使用 Logistic 回归进行分类<br>使用算法: 对简单数据集进行分类</p>
</blockquote>
<h4 id="1-2-数据采集"><a href="#1-2-数据采集" class="headerlink" title="1.2. 数据采集"></a>1.2. 数据采集</h4><p>该案例采用 100 行的数据集文本。其中前两列是特征 1 和特征 2，第三列是对应的类别标签。（两列特征并无实际含义，可以理解为特征 1 为身高，特征 2 为体重。类别判断性别为男或女。）</p>
<blockquote>
<p>testSet.txt</p>
<p>-0.017612	-14.053064	0.0<br>-1.395634	-4.662541	  1.0<br>-0.752157	-6.53862	    0.0<br>……</p>
</blockquote>
<h3 id="2-梯度下降训练算法"><a href="#2-梯度下降训练算法" class="headerlink" title="2. 梯度下降训练算法"></a>2. 梯度下降训练算法</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np

''' sigmoid跳跃函数 '''
def sigmoid(inX):
    return 1.0 / (np.exp(-inX) + 1)

'''加载数据集和类标签'''
def loaddata(filename):
    # dataMat 为原始数据， labelMat 为原始数据的标签
    datamat, labelmat = [], []
    fr = open(filename)
    for line in fr.readlines():
        linearr = line.strip().split('\t')
        datamat.append([1.0, float(linearr[0]), float(linearr[1])])
        # 为了方便计算(将 b 放入矩阵一起运算)，在第一列添加一个 1.0 作为 x0
        # w^T x + b = [w1 w2]^T * [x1 x2] + b = [b w1 w2]^T * [1.0 x1 x2]
        labelmat.append(float(linearr[-1]))
    return datamat, labelmat

''' 梯度下降法，得到的最佳回归系数 '''
def gd(data, label):
    datamat = np.mat(data)  # 转换为 NumPy 矩阵
    labelmat = np.mat(label).transpose() # 首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量
    # 转化为矩阵[[0,1,1,1,0,1.....]]，并转置[[0],[1],[1].....]
    # transpose() 行列转置函数
    # 将行向量转化为列向量   =&gt;  矩阵的转置

    # m-&gt;数据量，样本数 n-&gt;特征数
    m,n = datamat.shape  # 矩阵的行数和列数
    # print(m,n)
    weight = np.ones((n,1))  # 初始化回归系数 w^T [1 1 1]
    iters = 200  # 迭代次数
    learn_rate = 0.001 # 步长

    for i in range(iters):
        # 因为根据公式，每次更新 w 需要对所有的数据 xi 做一个误差求和，
        # 然后乘以步长 learn_rate。所以这里都是对整个数据集矩阵做运算。
        # 所以说内存里要将所有数据都存放进来做运算
        # 也正是这个原因，GD 的运算速度会慢于 SGD。
        # SGD 每次迭代只随机取数据集中的一个样本。

        # wx:
        gradient = datamat*np.mat(weight)  # 矩阵乘法
        # sigmoid(w^T*x+b):
        out = sigmoid(gradient)  # 获得预测值
        # sigmoid(w^T*x+b) - yi:
        errors = labelmat - out  # labelmat 为真实值，相减得到误差
        # 更新回归系数 w^T:
        weight = weight + learn_rate * datamat.T * errors
        # 最后一项：
	    # 0.001* (3*m)*(m*1) 即 步长*grad(w^Txi+b)*xi 的求和，得到 3*1 的列向量。
	    # 得到的是 m 列的所有数据 x0，x1，x2 的偏移量求和。

    return weight.getA()  # 矩阵转为数组

if __name__ == '__main__':
    datamat,labelmat = loaddata('testSet.txt')
    weight = gd(datamat,labelmat)
    print(weight)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>代码分析</strong>：<br><code>gd(data, label)</code> 函数的两个参数是数据加载返回的特征集和标签类集合。对数据集进行 mat 矩阵化转化，而类标签集矩阵化之后转置，便于行列式的计算。然后设定步长，和迭代次数。整个特征矩阵与回归系数相乘再求 sigmoid 值，最后返回更新得到的回归系数的值。运行结果如下：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">[[2.88492031]
 [0.39158333]
 [0.45880875]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p><strong>总结</strong>：<br>梯度下降法在每次更新梯度（回归系数）时都需要遍历整个数据集，该方法在处理有 100 个样本（仅有两个特征）的数据集时尚可，但如果数据集有上亿个样本，成千上万的特征，那么该方法的计算复杂度就会非常高。</p>
<p>针对这种情况，一种改进方法就是每次仅仅针对一个样本点来更新回归系数，该方法称为随机梯度下降 SGD。</p>
<h3 id="3-随机梯度下降训练算法"><a href="#3-随机梯度下降训练算法" class="headerlink" title="3. 随机梯度下降训练算法"></a>3. 随机梯度下降训练算法</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">import random
import numpy as np
import matplotlib.pyplot as plt

''' sigmoid 跳跃函数 '''
def sigmoid(inX):
    return 1.0 / (np.exp(-inX) + 1)

''' 加载数据集和类标签 '''
def loaddata(filename):
    # dataMat 为原始数据， labelMat 为原始数据的标签
    datamat, labelmat = [], []
    fr = open(filename)
    for line in fr.readlines():
        linearr = line.strip().split('\t')
        datamat.append([1.0, float(linearr[0]), float(linearr[1])])
        # 为了方便计算(将 b 放入矩阵一起运算)，在第一列添加一个 1.0 作为 x0
        # w^T x + b = [w1 w2]^T * [x1 x2] + b = [b w1 w2]^T * [1.0 x1 x2]
        labelmat.append(float(linearr[-1]))
    return datamat, labelmat

''' 随机梯度下降法 得到的最佳回归系数 '''
def sgd(data,label):
    datamat = np.mat(data)
    labelmat = np.mat(label).transpose()
    m, n = datamat.shape
    weight = np.ones((n,1))  # 创建与列数相同的矩阵的系数矩阵
    iters = 200
    for i in range(iters):
        dataindex = list(range(m))  # 返回 [0, 1, 2, ..., m] 的列表作为 index
        for j in range(m):
            learn_rate=4/(i+j+1)+0.01
            # 随着轮数的增加，学习率（或步长）逐渐变小
            randinx = int(random.uniform(0,len(dataindex)))  # 随机取一个 index
            # random.uniform(x, y) 随机生成一个实数，它在[x,y]范围内
            out = sigmoid(datamat[randinx]*weight)  # 计算预测值
            error = labelmat[randinx] - out  # 计算预测值与真实值的误差
            weight = weight + learn_rate  *  datamat[randinx].T * error  # 更新 w
            del(dataindex[randinx])
            # 学完一个样本，删除掉一个
    return weight.getA()

''' 数据可视化展示 '''
def plotBestFit(dataArr, labelMat, weights):
    n = np.shape(dataArr)[0]
    xcord1, xcord2, ycord1, ycord2 = [],[],[],[]
    for i in range(n):
        if int(labelMat[i]) == 1:
            xcord1.append(dataArr[i, 1])
            ycord1.append(dataArr[i, 2])
        else:
            xcord2.append(dataArr[i, 1])
            ycord2.append(dataArr[i, 2])

    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')
    ax.scatter(xcord2, ycord2, s=30, c='green')
    x = np.arange(-3.0, 3.0, 0.1)
    """
    dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])
    w0*x0+w1*x1+w2*x2=f(x)
    x0最开始就设置为1， x2就是我们画图的y值，而f(x)被我们磨合误差给算到w0,w1,w2身上去了
    所以： w0+w1*x+w2*y=0 =&gt; y = (-w0-w1*x)/w2
    """
    y = (-weights[0] - weights[1] * x) / weights[2]
    ax.plot(x, y)
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()

if __name__ == '__main__':
    # 1.加载数据
    datamat, labelmat = loaddata('testSet.txt')
    # 2.训练模型，f(x)=a1*x1+b2*x2+..+nn*xn 中 (a1,b2, .., nn).T 的矩阵值
    weight = sgd(datamat, labelmat)
    print(weight)
    # 3.数据可视化
    dataArr = np.array(datamat)
    plotBestFit(dataArr, labelmat, weight)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>代码分析</strong>：<br>加载数据以及 sigmoid 函数与 GD 算法是一样的。区别在于 <code>sgd(data,label)</code> 函数。梯度下降在每次更新数据集时都需要遍历整个数据集，计算复杂度较高；随机梯度下降一次只用一个样本点来更新回归系数。</p>
<ul>
<li>这里还对学习率 lr 做了优化，每次迭代时，lr 都会调整，随着迭代次数不断减小，但不会为 0，因为公式中有一个常数项。</li>
<li>通过 <code>randinx</code> 随机选择样本来更新梯度。每次随机从 <code>dataindex</code> 列表选择一个 index 作为 <code>randinx</code>，更新后则从 <code>dataindex</code> 列表删除该 index。直到所有样本都被选取后，算是进行了一次 iteration。</li>
</ul>
<h2 id="0x03-正则化"><a href="#0x03-正则化" class="headerlink" title="0x03 正则化"></a>0x03 正则化</h2><p>机器学习中经常会在损失函数中加入正则项，称之为正则化（Regularization）。正则化是用来防止模型过拟合的一种手段。下面先介绍什么是过拟合与欠拟合，然后再引入正则化。</p>
<h3 id="1-欠拟合-amp-过拟合"><a href="#1-欠拟合-amp-过拟合" class="headerlink" title="1. 欠拟合 & 过拟合"></a>1. 欠拟合 &amp; 过拟合</h3><ul>
<li>欠拟合：没有学习充分。</li>
<li>过拟合：过度学习，将噪声也学习了进去；过度拟合样本，泛化性差（比如过度拟合训练集，但迁移测试集时效果很差）。</li>
</ul>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208041544562.png"></p>
<p><strong>小结</strong>：欠拟合是比较容易解决的，比如增加训练轮数，让模型多学一些，或者让模型复杂些，增强学习能力。而过拟合是一个关键问题，为了保证模型的泛化性，防止过拟合，有许多的方法。<strong>正则化</strong>是其中一种常用的手段。</p>
<h3 id="2-正则化"><a href="#2-正则化" class="headerlink" title="2. 正则化"></a>2. 正则化</h3><p><strong>目的</strong>：防止过拟合，提高泛化性，防止模型只在训练集上有效、在测试集上不够有效。<br><strong>原理</strong>：在损失函数中加上某些规则（限制），缩小解空间，从而减少求出过拟合解的可能性。</p>
<ul>
<li>用一个例子来解释一下为什么需要正则化？<ul>
<li><strong>例子</strong>：还是以上面的 LR 实践的情景作为案例——做性别的二分类（男或女）。有两个特征：身高 —— <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>，体重 —— <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>。将这两个特征放入 LR 模型，就是：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="19.333ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 8545.1 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(571,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(960,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="msub" transform="translate(2112.6,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(3343.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(4343.6,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="msub" transform="translate(5496.1,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(6726.9,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(7727.1,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(8156.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.292ex" height="1ex" role="img" focusable="false" viewBox="0 -431 571 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g></g></g></svg></mjx-container> 是激活函数 Sigmoid，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="6.221ex" height="1.441ex" role="img" focusable="false" viewBox="0 -443 2749.8 637"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1152.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1597.2,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container> 分别是两个特征的权重，也是模型要学习的两个参数。</li>
<li><strong>过拟合</strong>：把班级 1 作为训练集，班级 2 作为测试集。假如班级 1 中，恰好所有男生身高都大于 1.8m，而女生身高都小于 1.7m，也就是说<strong>仅通过身高这一个特征</strong>，就可以对训练集正确分类。那就很可能学习到如：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1152.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> =70，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1152.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container> =0.4 的参数。甚至 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1152.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> 会更大，因为在当前班级 1 中，只通过身高判断就可以了。<strong>即使</strong> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1152.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> 不需要这么大也能正确判断。此时，如果在测试集（班级 2）预测，而班级 2 里的男女生身高是正常分布的，那么使用这样的一组参数 —— <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1152.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> =70，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1152.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container> =0.4 来预测，极大概率是会判断错误的（因为相当于是只考虑身高特征）。这就是过拟合，或者说不具备泛化性。</li>
<li><strong>正则化的作用</strong>：给损失函数加上正则化后，缩小了参数的解空间，让某些关键特征的权重参数<strong>不会增长的过大</strong>，仅仅<strong>够用</strong>就可以了。比如得到 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1152.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> =0.7，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1152.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container> =0.4 这样一对参数解，明显是比之前过拟合的参数更加合理。</li>
<li><strong>如何实现正则化</strong>：有 L1 正则化，L2 正则化，Dropout 正则化以及 BN，LN 等等。<ul>
<li>L1 正则化：在损失函数中加入一次惩罚项（一次的 L1 范数）。L1 范数是指向量中各个元素绝对值之和。<strong>L1 范数可以进行特征选择，即让特征的权重系数变为 0。</strong></li>
<li>L2 正则化：在损失函数中加入二次惩罚项（二次的 L2 范数）。L2 范数是指向量各元素的平方和然后求平方根。<strong>L2 范数可以防止过拟合，提升模型的泛化能力，让模型权重尽可能接近 0。</strong></li>
<li>Dropout 正则化：在训练过程中，每次都随机失活一部分神经元，神经元失活时权重即为 0。这样可以保证模型不会过度依赖某几个神经元（给予他过高的权重）。当这几个神经元失活时，就必须通过其他的特征来完成预测。和 L2 正则化的功能相近，能够实现对模型权重的约束。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="L1-与-L2-正则化"><a href="#L1-与-L2-正则化" class="headerlink" title="L1 与 L2 正则化"></a>L1 与 L2 正则化</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208041716302.png"></p>
<p><strong>从公式角度理解</strong>：<br>如图，最上面两条公式：L1 正则化添加了惩罚项 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.564ex;" xmlns="http://www.w3.org/2000/svg" width="6.571ex" height="2.26ex" role="img" focusable="false" viewBox="0 -749.5 2904.6 999"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g><g data-mml-node="mo" transform="translate(640,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(918,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1196,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(1912,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(2190,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mn" transform="translate(311,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 640 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g></g></g></svg></mjx-container> 是正则化系数，当 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 640 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g></g></g></svg></mjx-container> 越大时，正则化约束越强。<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.564ex;" xmlns="http://www.w3.org/2000/svg" width="5.123ex" height="2.26ex" role="img" focusable="false" viewBox="0 -749.5 2264.6 999"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(278,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(556,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(1272,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(1550,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mn" transform="translate(311,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> 代表 w 的 L1 范数。L2 正则化则是添加了 w 的 L2 范数的二次项作为惩罚项。</p>
<p>如果最小化损失函数，当左边原损失函数尽可能小时，右边添加的惩罚项也会尽可能小。</p>
<p><strong>从图的角度理解</strong>：<br>再看下面两张图。二维坐标上，每个点代表 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.981ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3527.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1541.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1986.2,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(3138.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 的取值。像等高线一样的曲线上，每条曲线代表着——该曲线上所有的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.981ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3527.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1541.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1986.2,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(3138.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 参数点带入损失函数的值相等。更直观的理解：以最外圈的红线为例，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1152.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> 沿着红线向右变大，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.608ex" height="1.342ex" role="img" focusable="false" viewBox="0 -443 1152.6 593"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container> 就相应变小，那么自然可以做到损失函数相等。补充：最外圈的红线，其实也是损失函数最小的地方。</p>
<p>既然红色曲线上这么多参数点的损失函数都是一样的，那我们自然希望取到一个最合适的参数点：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="6.221ex" height="1.441ex" role="img" focusable="false" viewBox="0 -443 2749.8 637"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1152.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1597.2,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container> 都不会过大，尽可能接近 0。</p>
<p>L1 正则化的曲线是一个菱形，与损失函数相交的点在 y 轴。也就是说，此时 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.368ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1930.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1541.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 的取值为 0。<strong>因此 L1 正则化会使得特征变得稀疏，起到了筛选特征，减小模型复杂度的作用。</strong> 因为 Loss 的最小值一般在坐标轴上取到，这时候说明其中有一个特征的权重变成 0 了，从而起到了特征稀疏化的作用。</p>
<p>L2 正则化会使得参数尽可能的小，从而防止模型过拟合。还有一个优点是处处可导，而 L1 正则化曲线的四个角上是不可导的。</p>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p>梳理一下，正则化有多种方式，包括 L0（向量中非零元素个数），L1（向量中元素绝对值只和），L2（向量的模）。<strong>但是 L0 范数的求解是个 NP 完全问题，而 L1 也能实现稀疏并且比 L0 有更好的优化求解特性，因此 L1 被广泛应用</strong>。</p>
<p>L2 范数指向量中各元素求<strong>平方和</strong>后开根号的值，可以令 w 各元素尽可能接近 0。<strong>虽然不如 L1 范数更彻底的降低模型复杂度（使特征稀疏），但是可以防止过拟合，而且处处可微，降低了计算难度。</strong></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>学习了逻辑回归的理论基础，并且在简单数据集上实践了，用梯度下降法来训练逻辑回归分类器。最后还学习了 L1 与 L2 正则化，并且探究了这两个正则化如何防止了模型过拟合。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>逻辑回归：<br><a href="https://zhuanlan.zhihu.com/p/39363869">浅析机器学习：线性回归 &amp; 逻辑回归 - 知乎</a><br><a href="https://www.cnblogs.com/baiboy/p/pybnc5.html">一步步教你轻松学逻辑回归模型算法 - 伏草惟存</a></p>
<p>正则化：<br><a href="https://blog.csdn.net/Sakura_day/article/details/114953343">正则化方法一篇就够了</a><br><a href="https://zhuanlan.zhihu.com/p/28023308">L1范数与L2范数的区别 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/67931198">机器学习必知必会：正则化 - 知乎</a></p>
<p>稀疏矩阵：<br><a href="https://zhuanlan.zhihu.com/p/456904535">Python稀疏矩阵详解 - 知乎</a></p>
]]></content>
      <categories>
        <category>FunRec</category>
        <category>02-机器学习与深度学习基础</category>
      </categories>
      <tags>
        <tag>FunRec</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>2.2 逻辑回归-part2</title>
    <url>/12PWF3V/</url>
    <content><![CDATA[<h2 id="0x04-评估指标"><a href="#0x04-评估指标" class="headerlink" title="0x04 评估指标"></a>0x04 评估指标</h2><p>以一个例子开始。假设我们有一个<strong>汉堡分类器</strong>，可以针对图片分类，分为是汉堡，或者不是汉堡。如果我们想知道，这个<strong>分类器的效果</strong>到底如何？该如何评价呢？</p>
<span id="more"></span>

<p>我们将手机里的照片输入分类器，得到分类结果：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120055418.png"><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120056405.png"></p>
<p>可如果样本（照片）很多的时候，这将会是一个非常大的表格。如何更加简洁的表示呢？=&gt; ==混淆矩阵 Confusion Matrix==</p>
<h3 id="1-混淆矩阵-Confusion-Matrix"><a href="#1-混淆矩阵-Confusion-Matrix" class="headerlink" title="1. 混淆矩阵 Confusion Matrix"></a>1. 混淆矩阵 Confusion Matrix</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120100205.png"></p>
<p>如图，仅用一个 2x2 的混淆矩阵就可以解决这个问题。左上角是真实类别为 Positive 样本，同时分类器预测结果也为 Positive 的样本数量统计，因此叫 True Positive，即真正样本。而右上角是真实类别为 Negative 样本，而分类器预测为 Positive 样本（即预测错了，将不是 Positive 样本的，预测为了 Positive 样本），因此叫他 False Positive，即伪正样本。第二行矩阵同理。</p>
<blockquote>
<p>这里是从预测的角度来命名的。</p>
<ul>
<li>把正样本预测对了，就是 TP，真正样本；预测错了就是 FP，伪正样本。</li>
<li>把负样本预测对了，就是 TN，真负样本；预测错了就是 FN，伪负样本。</li>
</ul>
</blockquote>
<p>推广到多分类也是一样：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208121014801.png"></p>
<p>主对角线上是预测正确的样本数，我们会希望主对角线上的数值尽可能的大。</p>
<h3 id="2-准确率，精确率，召回率，F1-值"><a href="#2-准确率，精确率，召回率，F1-值" class="headerlink" title="2. 准确率，精确率，召回率，F1 值"></a>2. 准确率，精确率，召回率，F1 值</h3><p>如果有两个或者多个<strong>汉堡分类器</strong>，如何评估他们的效果呢？</p>
<h4 id="2-1-二分类"><a href="#2-1-二分类" class="headerlink" title="2.1. 二分类"></a>2.1. 二分类</h4><ol>
<li>如果我们关心这个分类器到底<strong>分对了多少</strong>？<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120128981.png"></li>
</ol>
<p>准确率 (Accuracy) = (1+5) / (1+2+1+5) = 0.67</p>
<blockquote>
<p>预测正确的样本数（TP+TN），除以样本总数<br>分类正确的汉堡，除以样本总数</p>
</blockquote>
<ol start="2">
<li>假设在图片搜索引擎中，我们搜索汉堡，我们关心搜索返回的汉堡图片中，<strong>有多少是 True 的汉堡图片</strong>？（返回的图片中正确的有多少？）</li>
</ol>
<p>精确率 (Precision) = 1 / (1+2) = 0.33</p>
<blockquote>
<p>预测正确的正样本数（TP），除以预测为正样本的总数（TP+FP）<br>搜索引擎返回的正确的汉堡图片数量，除以返回的所有图片数量（因为返回给你的图片，就是搜索引擎预测为汉堡的图片，虽然其中有预测错误的）</p>
</blockquote>
<ol start="3">
<li>假设还是在图片搜索引擎中搜索汉堡，我们关心，<strong>有多少的汉堡图片是被找到的</strong>，有多少是没有被找到的？</li>
</ol>
<p>召回率 (Recall) = 1 / (1+1) = 0.50</p>
<blockquote>
<p>预测正确的正样本数（TP），除以正样本总数（TP+FN）<br>搜索引擎返回的正确的汉堡图片数量，除以数据库中的所有汉堡图片数量</p>
</blockquote>
<p>可以发现<strong>精确率</strong>与<strong>召回率</strong>之间，是一种此消彼长的关系。</p>
<ol start="4">
<li>F1 值</li>
</ol>
<p>假设一种极端情况：分类器将所有样本都预测为汉堡。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120147145.png"></p>
<p>所以不能单一地追求精确率或者召回率的数值高，需要平衡这两个指标。因此有这样一个指标——F1 值，对 P 和 R 做调和平均。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120149695.png"></p>
<p>F1 值是 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="2.548ex" height="2.188ex" role="img" focusable="false" viewBox="0 -680 1126.2 967.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mi" transform="translate(676,-150) scale(0.707)"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g></g></g></g></svg></mjx-container> 的特殊情况（<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="5.429ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 2399.6 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mo" transform="translate(843.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1899.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>，认为 P 和 R 一样重要）。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120152855.png"></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g></g></g></svg></mjx-container> 值应该怎么取呢？</p>
<ul>
<li>比如在医疗领域，我们不希望遗漏任何一个患者，即认为 Recall 更重要<ul>
<li>此时一般 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g></g></g></svg></mjx-container> 取为 2</li>
</ul>
</li>
<li>如果在其他领域，我们认为 Precession 更重要<ul>
<li>此时一般 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g></g></g></svg></mjx-container> 取在 (0, 1] 之间</li>
</ul>
</li>
</ul>
<p><strong>小结</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120158228.png"></p>
<h4 id="2-2-多分类"><a href="#2-2-多分类" class="headerlink" title="2.2. 多分类"></a>2.2. 多分类</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120218387.png"></p>
<ul>
<li>宏观<ul>
<li>Accuracy 依然是绿色方格（主对角线）的预测正确样本 / 总样本数。</li>
<li>[Precision, Recall, F1] 是计算每一个类别自己的 [P, R, F1]。<ul>
<li>比如计算 C3 的 Precision = (C3, C3)绿色方格 / 预测类别 C3 一整行的样本总数</li>
<li>C3 的 Recall = (C3, C3)绿色方格 / 真实类别 C3 一整列 的样本总数</li>
</ul>
</li>
<li>随后整体的 [P, R, F1] 通过对每一类的 [P, R, F1] 加起来求平均，或者做一个加权平均得到。</li>
</ul>
</li>
<li>微观<ul>
<li><strong>Accuracy = micro precision = micro recall = micro F1-score</strong></li>
<li>把每个类别的 TP, FP, FN 先相加之后，再根据二分类的公式进行计算。<ul>
<li>比如还是对 C3 来说<ul>
<li>(C3, C3)绿色方格 为 TP （预测正确的正样本）</li>
<li>预测类别为 C3 的行上其他项，为 FP （伪正样本）</li>
<li>真实类别为 C3 的列上其他项，为 FN （伪负样本）</li>
</ul>
</li>
<li>将所有类别的 TP, FP, FN 先相加，然后根据二分类公式计算<ul>
<li>比如 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.045ex;" xmlns="http://www.w3.org/2000/svg" width="39.696ex" height="3.172ex" role="img" focusable="false" viewBox="0 -940 17545.6 1402.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mi" transform="translate(1051,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1396,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(1829,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2280,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mo" transform="translate(2987.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(3987.4,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(5016.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(6072,0)"><g data-mml-node="mrow" transform="translate(3131.8,457.1) scale(0.707)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="msub" transform="translate(704,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(760,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2320,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(3098,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="msub" transform="translate(3802,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(760,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="mo" transform="translate(5417.9,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(6195.9,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g></g><g data-mml-node="mrow" transform="translate(220,-345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="msub" transform="translate(704,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(760,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2320,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(3098,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="msub" transform="translate(3802,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(760,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="mo" transform="translate(5417.9,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(6195.9,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(7367.9,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(8145.9,0)"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="msub" transform="translate(8894.9,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(760,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(10510.9,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(11288.9,0)"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="msub" transform="translate(12037.9,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mn" transform="translate(760,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="mo" transform="translate(13653.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(14431.8,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g></g><rect width="11233.6" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>微观比较难理解，<a href="https://zhuanlan.zhihu.com/p/147663370">可以参考这里</a>。</p>
<h3 id="3-ROC-曲线和-AUC-值"><a href="#3-ROC-曲线和-AUC-值" class="headerlink" title="3. ROC 曲线和 AUC 值"></a>3. ROC 曲线和 AUC 值</h3><h4 id="3-1-ROC-曲线"><a href="#3-1-ROC-曲线" class="headerlink" title="3.1. ROC 曲线"></a>3.1. ROC 曲线</h4><p>依然是汉堡问题。分类器做预测的时候，输出的是一个在 (0, 1) 的预测值，我们通过设定一个<strong>阈值</strong>，将这个连续值转为 0 或 1 的离散值。如图，如果我们设定阈值为 0.5，那么可以得到唯一一个与之对应的<strong>混淆矩阵</strong>。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120254926.png"></p>
<p>可以想像，如果我们将这个阈值，从左到右（从 0 到 1）都取一遍，<strong>就会得到很多个混淆矩阵</strong>。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120300041.png"></p>
<p>那么有没有办法可以==将这所有的混淆矩阵，表示在一个二维平面内呢==？</p>
<p>=&gt; <strong>ROC 曲线</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120303562.png"></p>
<p>如图，每个混淆矩阵都有他的 TP，FP，FN，TN 值，那么可以计算得到<strong>真阳率</strong>（True Positive Rate, TPR）与<strong>假阳率</strong>（False Positive Rate, FPR）。如果将这两个值映射到二维平面，FPR 作为横坐标，TPR 作为纵坐标。<strong>此时一个混淆矩阵就被表示为了二维平面上的一个点</strong>。如果将所有的混淆矩阵的点都表示在这个二维坐标系中，就可以拟合一条曲线，即 <strong>ROC 曲线</strong>。</p>
<blockquote>
<ul>
<li>ROC 曲线描述了 TPR 与 FPR 的关系</li>
<li>真阳率 TPR: 正样本中猜对的比例</li>
</ul>
</blockquote>
<h4 id="3-2-AUC-值"><a href="#3-2-AUC-值" class="headerlink" title="3.2. AUC 值"></a>3.2. AUC 值</h4><p>如果有 A，B 两个分类器，并且有他们的 ROC 曲线，应该如何判断哪个分类器的效果好呢？<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120313244.png"></p>
<p>可以从 TPR 与 FPR 分析入手。观察公式发现，TPR 与 FPR 的分母分别是<strong>数据集中正样本个数与负样本个数</strong>，也就是说这两个值是已经确定的。因为无论取什么阈值，得到哪个混淆矩阵，这两个值都不会变（因为数据集已经给定，不会变）。</p>
<p>所以 <strong>TPR 与 FPR 仅同他们的分子</strong>相关。</p>
<ul>
<li>TPR 的分子 TP = count(真正样本)</li>
<li>FPR 的分子 FP = count(伪正样本)。<br>自然是分类正确的正样本数 <strong>TP 越大越好</strong>，分类错误的正样本数 <strong>FP 越小越好</strong>。即<strong>TPR 尽可能的大，FPR 尽可能的小</strong>。反应到二维平面上就是，ROC 曲线越靠近左上角，说明分类器效果越好。</li>
</ul>
<blockquote>
<p>或者说，<strong>正样本中猜对的比例——真阳率 TPR</strong> 越高越好。</p>
</blockquote>
<p>那么有没有数值可以量化这种曲线靠近左上角的程度呢？</p>
<p>那就是 <strong>=&gt; AUC 值（Area Under the roc Curve）</strong>。<br> <img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120326604.png"></p>
<ul>
<li>AUC 的定义：AUC 描述 ROC 曲线下包围的面积。AUC 值在 (0, 1) 的区间内。</li>
<li>AUC 的意义：<ul>
<li>随机取一对正负样本，AUC 是把正样本预测为 1 的概率，大于把负样本预测为 1 的概率。<strong>或者如前面所说，正确预测一个正样本的概率</strong>。</li>
<li>公式如下：<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.72ex;" xmlns="http://www.w3.org/2000/svg" width="24.325ex" height="2.417ex" role="img" focusable="false" viewBox="0 -750 10751.5 1068.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750,0)"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mi" transform="translate(1517,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mo" transform="translate(2554.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3610.6,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(4361.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(4750.6,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-176.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">正</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">正</text></g></g></g><g data-mml-node="mo" transform="translate(7167.5,0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="msub" transform="translate(8223.3,0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-176.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">正</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">负</text></g></g></g><g data-mml-node="mo" transform="translate(10362.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li>
<li>其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.72ex;" xmlns="http://www.w3.org/2000/svg" width="4.84ex" height="2.265ex" role="img" focusable="false" viewBox="0 -683 2139.2 1001.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-176.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">正</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">正</text></g></g></g></g></g></svg></mjx-container> 代表将该正样本预测为 1 的概率，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.72ex;" xmlns="http://www.w3.org/2000/svg" width="4.84ex" height="2.265ex" role="img" focusable="false" viewBox="0 -683 2139.2 1001.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(675,-176.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">正</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">负</text></g></g></g></g></g></svg></mjx-container> 代表将该负样本预测为 1 的概率。</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>AUC 值也可以理解为（按照预测值做从小到大的排序后）负样本排在正样本前面的概率。</strong>（或者反过来）</p>
<p>结合实践 2 中的 AUC 实现可以更好的理解这句话。</p>
<ul>
<li>假设有一个文件，每行代表一个样本，左列真实值，右列预测值。按照右列，从小到大排序。</li>
<li>那么此时，最理想的情况是——左列是全都预测正确，先全是 0，后面全是 1。<ul>
<li>因为右边预测值从小到大排序，全部预测正确的话必然是这样。</li>
<li>此时<strong>所有的负样本都排在了正样本前面</strong>。即负样本排在正样本前面的概率为 1。即 AUC = 1。</li>
</ul>
</li>
<li>但是，假如第 7 行，预测值为 0.03，真实值为 1。<ul>
<li>也就是说预测错了，有一个正样本被排到了前面。那么 AUC 就会减小。</li>
<li>并且越多的正样本排到了负样本前面，说明错的越多，AUC 也会越小。</li>
</ul>
</li>
</ul>
</blockquote>
<blockquote>
<p>AUC 反应了整体样本间的排序能力。<br>此外还有 GAUC(Group AUC)，计算每个用户的 AUC，然后加权平均。GAUC 关注的是每个用户的排序结果。</p>
</blockquote>
<h3 id="4-PR-曲线"><a href="#4-PR-曲线" class="headerlink" title="4. PR 曲线"></a>4. PR 曲线</h3><p>如果<strong>将 ROC 曲线的 TPR 与 FPR 换成 Precision 与 Recall</strong>，就得到了 PR 曲线。所以每个 PR 曲线上的点，也是与相应的混淆矩阵对应的。并且我们知道，Precision 与 Recall 一起越大越好，因此 PR 曲线越靠近右上角越好。如下图。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120511398.png"></p>
<blockquote>
<ul>
<li>ROC 曲线描述了 Precision 与 Recall 的关系</li>
<li>所以 PR 曲线也可以看作取不同的阈值，得到不同的混淆矩阵，计算得到不同的 Precision 与 Recall 映射到二维空间，然后拟合出来的曲线。</li>
</ul>
</blockquote>
<p>那么应该什么时候用 PR 曲线，什么时候用 ROC 曲线呢？<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120531769.png"></p>
<p>如图，对于两种曲线，他们的横坐标轴是一样的，TPR = Recall。区别只在于 FPR 和 Precision。</p>
<ul>
<li>假设，有两个分类器<ul>
<li>分类器 A 得到的 FP 为 10 倍的 TP，但远小于负样本数 N</li>
<li>分类器 B 得到的 FP 为 100 倍的 TP，但远小于负样本数 N</li>
</ul>
</li>
<li>对这两个分类器的 FPR 与 Precision 做差值<ul>
<li>FPR 差值约等于 0，因为 TP 远小于 N。 </li>
<li>Precision 差值约为 0.081。这个差值与 FP 强相关。<strong>而 FP 代表着，本该是负例，却被错误预测为正例的数量，说明对正例更加关心。</strong></li>
</ul>
</li>
<li>从 FPR 与 Precision 本身的含义理解<ul>
<li>FPR：有多少负样本的被预测为正样本（预测错误）；也可以看作，有多少负样本预测正确了。</li>
<li>Precision：预测为正样本的样本中，有多少是正确预测的。</li>
</ul>
</li>
</ul>
<p>举一个例子，假设有这样一组极度不平衡的数据（左下角写错了，应该是负例：493；正例：7）。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202208120550009.png"></p>
<p>可以看到，正例一个都没预测对。但如果看左边的 ROC 曲线的话，好像效果还蛮好的。右边的 PR 曲线，反映了正例预测的效果差。</p>
<p>==因此，当我们更加关心正例的预测结果，而且数据极度不平衡时，我们一定要用 PR 曲线，而不是 ROC 曲线。==</p>
<h2 id="0x05-实践2：基于逻辑回归的个性化排序"><a href="#0x05-实践2：基于逻辑回归的个性化排序" class="headerlink" title="0x05 实践2：基于逻辑回归的个性化排序"></a>0x05 实践2：基于逻辑回归的个性化排序</h2><h3 id="1-案例描述"><a href="#1-案例描述" class="headerlink" title="1. 案例描述"></a>1. 案例描述</h3><p>当用户浏览商品时，需要对这些商品做个性化排序。即对所有商品做一个打分，然后排序。在本案例，是预测一首歌曲用户是否喜欢。喜欢为 1，不喜欢为 0。也可以做个性化排序，因为预测值在 (0, 1) 区间中，那么将分数高的歌放在前面，用户有更大的可能性会喜欢。</p>
<h4 id="1-1-开发流程"><a href="#1-1-开发流程" class="headerlink" title="1.1. 开发流程"></a>1.1. 开发流程</h4><blockquote>
<p>数据采集(数据集): 用户画像，商品信息，用户行为<br>数据预处理: 将数据处理为样本数据集——可以提供给模型训练的数据<br>训练算法: 使用逻辑回归做二分类<br>评估算法: 获得预测值，然后通过 PR 曲线，AUC 指标进行评估</p>
</blockquote>
<h4 id="1-2-数据集"><a href="#1-2-数据集" class="headerlink" title="1.2. 数据集"></a>1.2. 数据集</h4><ol>
<li><p>用户行为数据 <code>ranking_lr/data/user_watch_pref.sml</code></p>
<pre class="line-numbers language-css" data-language="css"><code class="language-css">包含以下字段：
userid<span class="token punctuation">,</span> itemid<span class="token punctuation">,</span> watch_len<span class="token punctuation">,</span> hour
userid<span class="token punctuation">,</span> itemid<span class="token punctuation">,</span> 用户对 item 收听时长<span class="token punctuation">,</span> 点击时间（小时）

01e069ed67600f1914e64c0fe7730944^A4090309101^A15^A19
01d86fc1401b283d5828c293be290e08^A6192809101^A75^A12
002f4b9c49be9a0b2c13e1c3c4f6a21c^A8915109101^A385^A18
01e3fdf415107cd6046a07481fbed499^A6470209102^A1635^A21
01e3fdf415107cd6046a07481fbed499^A6470209102^A555^A16
01e3fdf415107cd6046a07481fbed499^A6470209102^A2024^A22
......<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>歌曲元数据 <code>ranking_lr/data/music_meta</code></p>
<pre class="line-numbers language-css" data-language="css"><code class="language-css">包含以下字段：
itemid<span class="token punctuation">,</span> name<span class="token punctuation">,</span> desc<span class="token punctuation">,</span> total_timelen<span class="token punctuation">,</span> location<span class="token punctuation">,</span> tags
itemid<span class="token punctuation">,</span> name<span class="token punctuation">,</span> 内容<span class="token punctuation">,</span> 时长<span class="token punctuation">,</span> 地域<span class="token punctuation">,</span> 标签

0093709100^A韩国少女时代最新回归新专主打《I GOT A BOY》^A韩国少女时代最新回归新专主打《I GOT A BOY》^A304^A^A
0102209100^A韩国张力尹携手EXO成员CHEN《呼吸》中文版^A韩国张力尹携手EXO成员CHEN《呼吸》中文版^A274^A^ACHEN<span class="token punctuation">,</span>少&gt;
029900100^A徐颢菲《猫的借口》^A^A284^A国内^A
0368709100^A美女翻唱 别问我是谁^A美女翻唱 别问我是谁^A288^A^A流行歌曲<span class="token punctuation">,</span>翻唱<span class="token punctuation">,</span>网络歌曲<span class="token punctuation">,</span>美女<span class="token punctuation">,</span>舞曲<span class="token punctuation">,</span>社会
0603409100^A龙梅子<span class="token punctuation">,</span>老猫 老爸老爸你好吗^A龙梅子<span class="token punctuation">,</span>老猫 老爸老爸你好吗^A259^A^A你好吗<span class="token punctuation">,</span>龙梅子<span class="token punctuation">,</span>老猫<span class="token punctuation">,</span>老爸老爸
0637909100^A《中国好歌曲》  诙?镜诹?赸-韩磊天边》^A《中国好歌曲》  诙?镜诹?赸-韩磊天边》^A289^A^A中国好歌曲<span class="token punctuation">,</span>&gt;
......<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ol>
<p>第一列是 <code>歌曲id</code>，后面几列是歌曲的一些信息。而且有的歌曲某些信息是缺失的，这里是以 <code>^A</code> 作为分隔符，所以出现相连的两个分隔符时，如 <code>^A^A</code>，就说明其中的 Tag 缺失了。</p>
<blockquote>
<p>补充：<code>^A</code> 的编码是 <code>'\001'</code></p>
</blockquote>
<ol start="3">
<li>用户画像 <code>ranking_lr/data/merge_base.data</code><pre class="line-numbers language-css" data-language="css"><code class="language-css">包含以下字段：
userid<span class="token punctuation">,</span> gender<span class="token punctuation">,</span> age<span class="token punctuation">,</span> salary<span class="token punctuation">,</span> location
userid<span class="token punctuation">,</span> 性别<span class="token punctuation">,</span> 年龄<span class="token punctuation">,</span> 收入<span class="token punctuation">,</span> 地域

00ea9a2fe9c6810aab440c4d8c050000<span class="token punctuation">,</span>女<span class="token punctuation">,</span>26-35<span class="token punctuation">,</span>20000-100000<span class="token punctuation">,</span>江苏
01a0ae50fd4b9ef6ed04c22a7e421000<span class="token punctuation">,</span>女<span class="token punctuation">,</span>36-45<span class="token punctuation">,</span>0-2000<span class="token punctuation">,</span>河北
002db7d2360562dd16828c4b91402000<span class="token punctuation">,</span>女<span class="token punctuation">,</span>46-100<span class="token punctuation">,</span>5000-10000<span class="token punctuation">,</span>云南
006a184749e3b3eb83e9eb516d522000<span class="token punctuation">,</span>男<span class="token punctuation">,</span>36-45<span class="token punctuation">,</span>2000-5000<span class="token punctuation">,</span>天津
00de61c1d635ad964eef2aefa8292000<span class="token punctuation">,</span>女<span class="token punctuation">,</span>19-25<span class="token punctuation">,</span>2000-5000<span class="token punctuation">,</span>内蒙古
00d313fa79be989e330f215c39dc3000<span class="token punctuation">,</span>女<span class="token punctuation">,</span>26-35<span class="token punctuation">,</span>10000-20000<span class="token punctuation">,</span>天津
......<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ol>
<h3 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2. 数据预处理"></a>2. 数据预处理</h3><h4 id="2-1-合并数据集"><a href="#2-1-合并数据集" class="headerlink" title="2.1. 合并数据集"></a>2.1. 合并数据集</h4><blockquote>
<p>总体思路：处理原始的数据，将用户画像数据 、物品元数据、用户行为数据，3 份融合到一起，得到处理后 merge_base.data。</p>
</blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd ./pre_base_data
python gen_base.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">#coding=utf-8
import sys

# 三类原始数据文件的路径，用户画像数据、物品元数据，用户行为数据
user_action_data = '../data/user_watch_pref.sml'
music_meta_data = '../data/music_meta'
user_profile_data = '../data/user_profile.data'

# 将合并后的元数据放到新的文件里
output_file = '../data/merge_base.data'

# 将3份数据merge后的结果输出，供下游数据处理
ofile = open(output_file, 'w')

# step 1. decode music meta data
# 将处理后的结果放入字典里面，key是itemid，value为物品对应的信息，为最后写入做准备
item_info_dict = {}
with open(music_meta_data, 'r') as fd:
    for line in fd:
        ss = line.strip().split('\001')
        if len(ss) != 6:
            continue
        itemid, name, desc, total_timelen, location, tags = ss
        item_info_dict[itemid] = '\001'.join([name, desc, total_timelen, location, tags])

# step 2. decode user profile data
# 处理用户画像数据，将处理后的结果放入字典里面，key是用户id，value是用户信息
user_profile_dict = {}
with open(user_profile_data, 'r') as fd:
    for line in fd:
        ss = line.strip().split(',')
        if len(ss) != 5:
            continue
        userid, gender, age, salary, location = ss
        user_profile_dict[userid] = '\001'.join([gender, age, salary, location])

# step 3. decode user action data &amp; output merge data
# 写入最后的信息，将用户行为数据进行处理，并把step1和step2得到的数据一并归纳在文件里面
with open(user_action_data, 'r') as fd:
    for line in fd:
        ss = line.strip().split('\001')
        if len(ss) != 4:
            continue
        userid, itemid, watch_len, hour = ss

        if userid not in user_profile_dict:
            continue

        if itemid not in item_info_dict:
            continue

        ofile.write('\001'.join([userid, itemid, watch_len, hour, \
                user_profile_dict[userid], item_info_dict[itemid]]))
        ofile.write("\n")

ofile.close()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>合并后得到如下数据 <code>ranking_lr/data/merge_base.data</code>。</p>
<pre class="line-numbers language-css" data-language="css"><code class="language-css">包含以下字段：
userid<span class="token punctuation">,</span> itemid<span class="token punctuation">,</span> watch_len<span class="token punctuation">,</span> hour<span class="token punctuation">,</span> user_profile_dict[userid]<span class="token punctuation">,</span> item_info_dict[itemid]
userid<span class="token punctuation">,</span> itemid<span class="token punctuation">,</span> 用户行为数据<span class="token punctuation">(</span>收听时长<span class="token punctuation">)</span><span class="token punctuation">,</span> 用户画像<span class="token punctuation">(</span>年龄<span class="token punctuation">,</span> 性别<span class="token punctuation">,</span> 收入<span class="token punctuation">,</span> 地区<span class="token punctuation">)</span><span class="token punctuation">,</span> 物品信息<span class="token punctuation">(</span>名字<span class="token punctuation">,</span> 描述<span class="token punctuation">,</span> 时长<span class="token punctuation">,</span> 标签<span class="token punctuation">)</span>

01e069ed67600f1914e64c0fe7730944^A4090309101^A15^A19^A女^A0-18^A10000-20000^A江西^A大美妞 大哲2013最新伤感歌曲网络歌曲DJ舞曲 大连翻译^A^A248^A^A大美妞<span class="token punctuation">,</span>流行
01d86fc1401b283d5828c293be290e08^A6192809101^A75^A12^A男^A26-35^A2000-5000^A广东^A李贞贤 Summer Dance MBC现场版？？？？ ？？？ ？^A^A159^A^A李贞贤<span class="token punctuation">,</span>明星
002f4b9c49be9a0b2c13e1c3c4f6a21c^A8915109101^A385^A18^A女^A36-45^A10000-20000^A广西^A音乐 《我是歌手》 周笔畅 《慢慢》第二季第五期_1^A音乐 《我是歌手》 周笔畅 《慢慢》第二季第五期_1^A314^A^A邓紫棋<span class="token punctuation">,</span>我是歌手第二季<span class="token punctuation">,</span>周笔畅<span class="token punctuation">,</span>音乐<span class="token punctuation">,</span>我是歌手<span class="token punctuation">,</span>流行
01e3fdf415107cd6046a07481fbed499^A6470209102^A1635^A21^A男^A36-45^A20000-100000^A内蒙古^A黄家驹1993演唱会高清视频^A^A1969^A^A演唱会
01e3fdf415107cd6046a07481fbed499^A6470209102^A555^A16^A男^A36-45^A20000-100000^A内蒙古^A黄家驹1993演唱会高清视频^A^A1969^A^A演唱会
01e3fdf415107cd6046a07481fbed499^A6470209102^A2024^A22^A男^A36-45^A20000-100000^A内蒙古^A黄家驹1993演唱会高清视频^A^A1969^A^A演唱会
......<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="2-2-制作数据集"><a href="#2-2-制作数据集" class="headerlink" title="2.2. 制作数据集"></a>2.2. 制作数据集</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd ./pre_data_for_rankmodel
python gen_samples.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>将之前合并的数据进一步处理，将年龄、性别等 Feature 转为 index 表示。并且把歌曲标题转为 token。得到如下数据 <code>ranking_lr/data/samples.data</code>。</p>
<pre class="line-numbers language-css" data-language="css"><code class="language-css">包含以下字段：
标签 label <span class="token punctuation">(</span>是否喜欢<span class="token punctuation">)</span><span class="token punctuation">,</span> 用户特征<span class="token punctuation">(</span>性别<span class="token punctuation">,</span> 年龄<span class="token punctuation">)</span><span class="token punctuation">,</span> 物品特征<span class="token punctuation">(</span>标题token<span class="token punctuation">)</span>

0 0<span class="token punctuation">:</span>1 2<span class="token punctuation">:</span>1 4702<span class="token punctuation">:</span>1.38682463766 11188<span class="token punctuation">:</span>0.996230625242 1350<span class="token punctuation">:</span>0.996230625242 5943<span class="token punctuation">:</span>0.996230625242 16069<span class="token punctuation">:</span>0.996230625242 10378<span class="token punctuation">:</span>0.793220918108 23573<span class="token punctuation">:</span>0.697935392737 11664<span class="token punctuation">:</span>0.631809128186 21965<span class="token punctuation">:</span>0.62434573841 24869<span class="token punctuation">:</span>0.517456462871 1003<span class="token punctuation">:</span>0.503245881179
1 0<span class="token punctuation">:</span>1 5<span class="token punctuation">:</span>1 7353<span class="token punctuation">:</span>1.99246125048 4324<span class="token punctuation">:</span>1.71495995655 3972<span class="token punctuation">:</span>1.64293773069 4968<span class="token punctuation">:</span>1.52451457681 19756<span class="token punctuation">:</span>1.10494485228 1118<span class="token punctuation">:</span>1.03970163946
1 1<span class="token punctuation">:</span>1 5<span class="token punctuation">:</span>1 11299<span class="token punctuation">:</span>2.64150609428 1998<span class="token punctuation">:</span>2.39095350058 1161<span class="token punctuation">:</span>2.13636036542 20762<span class="token punctuation">:</span>1.99773092931 4221<span class="token punctuation">:</span>1.61142664699
1 1<span class="token punctuation">:</span>1 5<span class="token punctuation">:</span>1 11299<span class="token punctuation">:</span>2.64150609428 1998<span class="token punctuation">:</span>2.39095350058 1161<span class="token punctuation">:</span>2.13636036542 20762<span class="token punctuation">:</span>1.99773092931 4221<span class="token punctuation">:</span>1.61142664699
1 1<span class="token punctuation">:</span>1 5<span class="token punctuation">:</span>1 11299<span class="token punctuation">:</span>2.64150609428 1998<span class="token punctuation">:</span>2.39095350058 1161<span class="token punctuation">:</span>2.13636036542 20762<span class="token punctuation">:</span>1.99773092931 4221<span class="token punctuation">:</span>1.61142664699
1 0<span class="token punctuation">:</span>1 2<span class="token punctuation">:</span>1 11299<span class="token punctuation">:</span>2.64150609428 1998<span class="token punctuation">:</span>2.39095350058 1161<span class="token punctuation">:</span>2.13636036542 20762<span class="token punctuation">:</span>1.99773092931 4221<span class="token punctuation">:</span>1.61142664699<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3. 训练模型"></a>3. 训练模型</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd /rankmodel
python lr.py ../data/samples.data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h3 id="4-评估指标"><a href="#4-评估指标" class="headerlink" title="4. 评估指标"></a>4. 评估指标</h3><ul>
<li>如何评价模型的效果？<ul>
<li>PR 曲线</li>
<li>AUC</li>
</ul>
</li>
</ul>
<h4 id="4-1-PR-曲线"><a href="#4-1-PR-曲线" class="headerlink" title="4.1. PR 曲线"></a>4.1. PR 曲线</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># pr.py

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, average_precision_score

#y_true = np.array([0, 0, 1, 1])
#y_scores = np.array([0.1, 0.5, 0.4, 0.8])

data = pd.read_csv('auc.txt')
print(data)
y_scores = data['pred']
y_true = data['true']

#画曲线
precision, recall, thresholds = precision_recall_curve(y_true, y_scores)
plt.figure("P-R Curve")
plt.title('Precision/Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.plot(recall,precision)
plt.show()

#计算AP
AP = average_precision_score(y_true, y_scores, average='macro', pos_label=1, sample_weight=None)
print('AP:', AP)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>由于这里读取的时候是用的 <code>data = pd.read_csv('auc.txt')</code>，所以要把 <code>auc.raw</code> 转为 <code>.csv</code> 格式。</p>
<p>首先输出预测值与真实值的数据：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd /rankmodel
python lr_auc.py ../data/samples.data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>生成了两个文件，<code>ranking_lr/rankmodel/T.txt</code> 与 <code>ranking_lr/rankmodel/P.txt</code>。分别是测试集的真实值与预测值。</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">真实值：
ranking_lr/rankmodel/T.txt
# P(y=1|x)
0
0
1
0
1
1
......<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text">预测值：
ranking_lr/rankmodel/P.txt
# 因为是二分类，所以预测值这里有两个列
# 左边：P(y=0|x) 右边：P(y=1|x)

[ 0.36431328  0.63568672]
[ 0.068367  0.931633]
[ 0.19792306  0.80207694]
[ 0.43181549  0.56818451]
[ 0.18593127  0.81406873]
[ 0.15577786  0.84422214]
......<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>然而只需要一列的预测值就可以了，所以将 <code>P.txt</code> 中的第二列取出。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cat P.txt | awk '{print $2}' | tr '\]' ' ' |&gt; P_2.txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>解释一下这条命令做了什么。以管道符为分隔，依次解释。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">1. 输出 P.txt
cat P.txt | head

2. 将 P.txt 的第二列输出
cat P.txt | awk '{print $2}' | head
输出如下：
0.75734437]
0.87088091]
0.71707007]
 
3. 将以上输出右边的中括号替换为空字符
cat P.txt | awk '{print $2}' | tr '\]' ' ' | head

4. 将以上结果存入文件 P_2.txt
cat P.txt | awk '{print $2}' | tr '\]' ' ' |&gt; P_2.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>将真实值与预测值的文件作为两列数据合到一个文件 <code>auc.raw</code> 中</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">paste T.txt P_2.txt &gt; auc.raw<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>得到如下格式的文件：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">1^M&gt;0.75734437·
1^M&gt;0.87088091·
0^M&gt;0.71707007·
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到真实值与预测值之间用 <code>^M&gt;</code> 分隔，预测值右边还有一个空格。需要把这个文件改为 <code>.csv</code> 的文件格式。</p>
<p>用 vim 打开 <code>auc.raw</code>，然后替换。需要用到以下命令：</p>
<pre class="line-numbers language-vim" data-language="vim"><code class="language-vim">:{作用范围}s/{目标字符}/{替换的字符}/{替换标志}

- 目标字符：origin
- 替换的字符：new
- 作用范围：用于指定替换的范围。
	- `1,3`表示替换第一行至第三行
	- `1,$`表示替换第一行到最后一行
	- 也可以直接用`%`表示替换所有行。 
- 替换标志（可以组合使用）： 
	- c: confirm，每次替换前都会询问
	- e：不显示error 
	- g: globe，不询问，整个替换
	- i: ignore，即不区分大小写<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">vim auc.raw

:%s/ $//g  # 将全局行尾的空格去掉
:%s/^M//g  # 这里的 `^M` 要使用 `CTRL-V CTRL-M` 生成，而不是直接键入 `^M`

cp auc.raw auc.txt  # 将 auc.raw 复制到一个新的文件，因为原文件后面还有用

vim auc.txt

:%s/^I/,/g  # 这里的 `^I` 就是 vim 里显示的 `&gt;`，用 Tab 键输入。或者输入 `\t`。

# 跳转命令
:1  # 跳转到第 1 行
# 然后在第一行之前插入表头：
# true,pred

# 最终得到如下格式的文件：
# true,pred
# 1,0.75734437
# 1,0.87088091
# ...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>绘制 PR 曲线：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">python pr.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h4 id="4-2-AUC"><a href="#4-2-AUC" class="headerlink" title="4.2. AUC"></a>4.2. AUC</h4><p>计算 AUC：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cat auc.raw | sort -t$'\t' -k2g |awk -F'\t' '($1==0){++x;a+=y;}($1==1){++y;}END{print 1.0-a/(x*y);}'

# cat auc.raw | sort -t$'\t' -k2g 对第二列数据从小到大做排序
# -F'\t' 以 '\t' 作为分隔符，第一列用 $1 表示，第二列用 $2 表示
# $1==0){++x;a+=y;} 如果满足 $1==0，那么执行大括号里的内容。
# x：负样本个数 y：正样本个数 a：错误预测样本 pair 个数
# x*y：正负样本 pair 个数
# a/x*y：错误的概率
# 1-a/x*y：正确的概率<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="0x06-面试题"><a href="#0x06-面试题" class="headerlink" title="0x06 面试题"></a>0x06 面试题</h2><blockquote>
<p><strong>1. 逻辑回归和朴素贝叶斯的区别？</strong></p>
<ul>
<li>逻辑回归是判别模型，朴素贝叶斯是生成模型<ul>
<li>判别模型：逻辑回归是直接对 P(y|x) 的问题进行建模，学习和求解，是在给定观测变量值的前提下，目标变量的==条件生成概率==。</li>
<li>生成模型：基于条件独立假设，在计算 P(y|x)之前，先要从训练数据中计算 P(x|y)和 P(y)的先验概率，从而利用贝叶斯公式计算 P(y|x)。需要所有变量的==全概率模型==。</li>
</ul>
</li>
</ul>
</blockquote>
<blockquote>
<p><strong>2. 线性回归和逻辑回归的区别？</strong></p>
<ul>
<li>逻辑回归是线性回归加了 Sigmoid 函数。</li>
<li>逻辑回归输出只取 0 和 1，线性回归输出连续值</li>
<li>拟合函数也有区别<ul>
<li>线性回归：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.299ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1900 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(939,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1511,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 目标是拟合函数</li>
<li>逻辑回归：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="14.419ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6373 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(645,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(990,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(1467,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2345,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(2830,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3175,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(3695,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4084,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(4634,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5023,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(5595,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5984,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 目标是拟合对一类样本的概率</li>
</ul>
</li>
</ul>
</blockquote>
<blockquote>
<p><strong>3. 随机梯度下降（SGD）和批量梯度下降（BGD）的区别？</strong></p>
<ul>
<li>SGD 是每次随机取一个样本做梯度更新</li>
<li>BGD 是每次随机取一个 batch（n 个样本）做梯度更新</li>
</ul>
</blockquote>
<blockquote>
<p><strong>4. 什么是 AUC？</strong></p>
</blockquote>
<blockquote>
<p><strong>5. 模型中的 w 参数，为什么不宜过大？</strong></p>
<ul>
<li>因为当某一个或者某几个 w 参数过大时，会导致模型过于依赖这几个特征，使得模型的泛化性变差。</li>
<li>也会导致梯度更新波动较大。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>6. 什么是正则化项？</strong></p>
<ul>
<li>在损失函数中添加==惩罚项（范数）==，作为约束，使得 w 保持接近 0 值，不会过大。</li>
<li>如果是 L1 正则化，除了缩小解空间，使模型参数尽可能接近 0；还可以过滤掉一些特征（结合 L1 的图理解），让模型变得简洁，可解释性更好。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>7. 过拟合的问题？</strong></p>
<ul>
<li>如果数据中存在噪音，模型有将==噪音==一起过度学习，导致模型失效。</li>
<li>如果不使用正则化，模型参数学习的较大，导致模型过拟合，过度依赖某些特征，==不具备泛化性==。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>8. 解决过拟合的方法？</strong></p>
<ul>
<li>降低模型复杂度：处理过拟合的第一步就是降低模型复杂度。</li>
<li>增加数据量：使用更大的数据集训练模型。</li>
<li>数据增强，对原有样本做变换：比如在 CV 中，对图片进行翻转。</li>
<li>正则化：<ul>
<li>L1, L2</li>
<li>Dropout</li>
</ul>
</li>
<li>早停：当相邻两次结果变化小于一定程度时，就停止训练，防止过度学习。</li>
<li>重新清洗数据：把明显异常值剔除。</li>
<li>使用集成学习方法：把多个模型集成在一起，降低单个模型的过拟合风险。</li>
<li>BatchNorm 批量归一</li>
</ul>
</blockquote>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>评估指标：<br><a href="https://www.bilibili.com/video/BV1oz4y1R71a">【小萌五分钟】机器学习 | 混淆矩阵 Confusion Matrix_bilibili</a><br><a href="https://www.bilibili.com/video/BV1wz4y197LU">【小萌五分钟】机器学习 | 模型评估: ROC曲线与AUC值_bilibili</a><br><a href="https://zhuanlan.zhihu.com/p/147663370">多分类模型 Accuracy, Precision, Recall 和 F1-score 的超级无敌深入探讨 - 知乎</a><br><a href="https://www.xuebawang.net/t/47417">「评估」AUC离线好,上线差?试试GAUC</a></p>
<p>Linux 命令：<br><a href="https://www.ruanyifeng.com/blog/2018/11/awk.html">awk 入门教程 - 阮一峰的网络日志</a><br><a href="https://www.runoob.com/linux/linux-comm-awk.html">Linux awk 命令 | 菜鸟教程</a><br><a href="https://www.runoob.com/linux/linux-comm-tr.html">Linux tr命令 | 菜鸟教程</a></p>
<p>音乐推荐系统实战：<br><a href="https://blog.csdn.net/qq_36816848/article/details/108383078">音乐推荐系统-CSDN博客</a><br><a href="https://github.com/GoAlers/Music-Top-Recommend">GitHub - GoAlers/Music-Top-Recommend</a></p>
]]></content>
      <categories>
        <category>FunRec</category>
        <category>02-机器学习与深度学习基础</category>
      </categories>
      <tags>
        <tag>FunRec</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫与网络编程基础</title>
    <url>/2FSK6BP/</url>
    <content><![CDATA[<p>跟着 <a href="https://coggle.club/blog/30days-of-ml-202203">Coggle 三月份的学习活动</a> 补充一下这方面的技能。恰好最近收集数据集也比较需要这方面的知识。</p>
<h2 id="0x00-内容介绍"><a href="#0x00-内容介绍" class="headerlink" title="0x00 内容介绍"></a>0x00 内容介绍</h2><p>爬虫与网络编程基础这部分的入门学习的要解决的问题如下：</p>
<ul>
<li>对网络编程了解较少，不会从 HTML 中提取信息；</li>
<li>不会爬虫，不会收集数据，也不会部署模型。</li>
</ul>
<p>而上述问题都是一个合格算法工程师所必备的。因此借助这个活动来入门。</p>
<span id="more"></span>

<h2 id="0x01-学习内容"><a href="#0x01-学习内容" class="headerlink" title="0x01 学习内容"></a>0x01 学习内容</h2><p>当今的世界是一个互联的世界，绝大多数的计算机和人都在通过网络和他人传递信息、沟通互联。我们在网络上学习、游戏、工作，我们提供各种各样的网络服务，又有很多人使用着各种各样的网络服务。网络改变了世界，而程序员“定义”了网络。我们在代码中实现了网络的通信，让一切变得可能。网络爬虫是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。<strong>在本次学习中我们将学习基础的爬虫操作，并学习基础的 HTTP 协议，最后尝试完成基础的网络编程。</strong></p>
<h2 id="0x02-打卡汇总"><a href="#0x02-打卡汇总" class="headerlink" title="0x02 打卡汇总"></a>0x02 打卡汇总</h2><table>
<thead>
<tr>
<th>任务名称</th>
<th>难度</th>
<th>所需技能</th>
</tr>
</thead>
<tbody><tr>
<td>任务 1：计算机网络基础</td>
<td>低、1</td>
<td>json、xml</td>
</tr>
<tr>
<td>任务 2：HTTP 协议与 requests</td>
<td>低、1</td>
<td>requests</td>
</tr>
<tr>
<td>任务 3：bs4 基础使用</td>
<td>中、2</td>
<td>bs4</td>
</tr>
<tr>
<td>任务 4：bs4 高阶使用</td>
<td>高、3</td>
<td>bs4</td>
</tr>
<tr>
<td>任务 5：正则表达式</td>
<td>高、3</td>
<td>re</td>
</tr>
<tr>
<td>任务 6：Python 网络编程基础</td>
<td>高、3</td>
<td>scoket</td>
</tr>
<tr>
<td>任务 7：tornado 基础使用</td>
<td>中、2</td>
<td>tornado</td>
</tr>
<tr>
<td>任务 8：tornado 用户注册&#x2F;登录</td>
<td>高、3</td>
<td>tornado</td>
</tr>
<tr>
<td>任务 9：tornado 部署机器学习模型</td>
<td>中、2</td>
<td>tornado</td>
</tr>
</tbody></table>
<ul>
<li><p><a href="network-spider-programming-task01.md">任务 1：计算机网络基础</a></p>
<ul>
<li>步骤 1：在 Pyhon 中创建一个 list，存储以下个人信息（姓名、年龄、成绩）：[小王、40、50]，[小贾、50、23]。</li>
<li>步骤 2：将步骤 1 的数据存储为 json 格式，并进行读取。</li>
<li>步骤 3：将步骤 1 的数据存储为 xml 格式，并进行读取。</li>
<li>步骤 4：学习 <a href="https://www.runoob.com/w3cnote/summary-of-network.html">计算机网络基础</a> ，思考从打开 <a href="https://coggle.club/">coggle.club</a> 到网页展示，有什么步骤？将你的思考结果写到博客。</li>
</ul>
</li>
<li><p>任务 2：HTTP 协议与 requests</p>
<ul>
<li>步骤 1： <a href="https://www.cnblogs.com/an-wen/p/11180076.html">学习 HTTP 协议</a></li>
<li>步骤 2：HTTP 的 get 和 post 有什么区别？用处在哪儿？将你的思考写到博客。</li>
<li>步骤 3：使用 Python 中 requests 中的 get 访问百度。</li>
</ul>
</li>
<li><p>任务 3：bs4 基础使用</p>
<ul>
<li>学习资料： <a href="https://beautiful-soup-4.readthedocs.io/en/latest/">https://beautiful-soup-4.readthedocs.io/en/latest/</a></li>
<li>步骤 1：使用 requests 和 bs4 爬取 <a href="https://scikit-learn.org/stable/modules/classes.html">sklearn api 页面</a>。</li>
<li>步骤 2：在 api 页面中有多少个模块？有多少个 API？如 sklearn.base.DensityMixin，其中 base 为模块，DensityMixin 为 API。</li>
<li>步骤 3：将模块名作为 key，api 作为 value 存储为字典。</li>
</ul>
</li>
<li><p>任务 4：bs4 高阶使用</p>
<ul>
<li>步骤 1：爬取 <a href="https://scikit-learn.org/stable/glossary.html">sklearn 机器学习名词页面</a> , 爬取所有的名词，如 1d、2d 和 api；</li>
<li>步骤 2：有一些名词在介绍时，会有额外的链接，请将每个名词介绍对应的链接也找出。</li>
<li>步骤 3：原始的名词安装本身有类别，如下所示，你能将爬取的结果进行分类吗？</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">General Concepts
Class APIs and Estimator Types
Target Types
Methods
Parameters
Attributes
Data and sample properties<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><p>任务 5：正则表达式</p>
<ul>
<li>步骤 1： <a href="https://www.runoob.com/python/python-reg-expressions.html">学习正则表达式 re 模块使用</a> 。</li>
<li>步骤 2：使用 re 筛选出机器学习名词，只包含字母的名词；</li>
<li>步骤 3：使用 re 筛选出机器学习名词，首字母为 A 或 首字母为 B 的名词。</li>
</ul>
</li>
<li><p>任务 6：Python 网络编程基础</p>
<ul>
<li>步骤 1： <a href="https://www.runoob.com/python/python-socket.html">学习 Socket 编程</a></li>
<li>步骤 2：使用编写一个 Socket 聊天机器人，程序 A 发送数据给程序 B，程序 B 也可以发送信息给程序 A；</li>
<li>步骤 3：使用编写一个 Socket 聊天机器人，程序 A 发送文件内容给 程序 B，程序 B 将文件进行存储。</li>
</ul>
</li>
<li><p>任务 7：tornado 基础使用</p>
<ul>
<li>步骤 1：学习 tornado 基础使用<ul>
<li>tornado 官网： <a href="https://www.tornadoweb.org/en/stable/">https://www.tornadoweb.org/en/stable/</a></li>
<li>tornado 教程：<ul>
<li><a href="http://www.ttlsa.com/docs/tornado/">http://www.ttlsa.com/docs/tornado/</a></li>
<li><a href="http://doc.iplaypy.com/tornado/ch1.html">http://doc.iplaypy.com/tornado/ch1.html</a></li>
</ul>
</li>
</ul>
</li>
<li>步骤 2：编写 tornado 的 hello word 程序。</li>
<li>步骤 3：编写 tornado 的 handler，分别接受 post 和 get 请求，请求为两个数字，进行求和，然后返回结果。</li>
</ul>
</li>
<li><p>任务 8：tornado 用户注册&#x2F;登录</p>
<ul>
<li>步骤 1：使用 sqlite 创建用户信息表，表包含 <code>uid</code>，<code>name</code>，<code>passwd</code> 三个字段。</li>
<li>步骤 2：编写 tornado 的用户注册 handler，完成用户注册逻辑，具体需要判断用户名和 passwd 合理性（不包含空格 &amp; 最大长度限制），然后插入数据。</li>
<li>步骤 3：编写 tornado 的用户登录 handler，完成用户登录逻辑，根据 name 和 passwd 判断是否登录成功。</li>
<li>步骤 4：结合 requests 和 tornado 完成上述逻辑。</li>
</ul>
</li>
<li><p>任务 9：tornado 部署机器学习模型</p>
<ul>
<li>步骤 1：读取 <a href="https://mirror.coggle.club/dataset/waimai_10k.csv">外卖评论数据集</a>。</li>
<li>步骤 2：使用 jieba 进行分词，TFIDF 提取特征，并选择分类器进行训练。</li>
<li>步骤 3：将文本分类模型使用 tornado 进行部署，客户端 requests 发送文本进行分类。</li>
</ul>
</li>
</ul>
<h2 id="0x03-学习资料"><a href="#0x03-学习资料" class="headerlink" title="0x03 学习资料"></a>0x03 学习资料</h2><ul>
<li><a href="https://www.jianshu.com/p/f6292d732217">https://www.jianshu.com/p/f6292d732217</a></li>
<li><a href="https://www.bilibili.com/video/av59706997/">https://www.bilibili.com/video/av59706997/</a></li>
<li><a href="https://cuiqingcai.com/1319.html">https://cuiqingcai.com/1319.html</a></li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p> <a href="https://coggle.club/blog/30days-of-ml-202203">Coggle 30 Days of ML（22 年 3 月） - Coggle 数据科学</a></p>
]]></content>
      <categories>
        <category>爬虫与网络编程基础</category>
      </categories>
      <tags>
        <tag>Coggle 30 Days of ML</tag>
        <tag>Spider</tag>
        <tag>Network Programming</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫实战：批量爬取三维模型数据集</title>
    <url>/ET7FEN/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>因为实验需要，从 <a href="http://web.3dsource.cn/web.html">零件库网页版</a> 批量爬取数据集。由于是<strong>动态网页</strong>，难度比爬取 HTML 中的文字信息大了很多。经过查询资料，决定使用 Selenium 来做这部分的工作。但落地过程中其实涉及许多文档之外的实际问题，也是花了蛮多时间才基本实现爬取需求，稍微记录一下。仅面向实现需求记录，一些 Selenium 的基本操作不多解释，有需要的话可以在 <a href="https://www.selenium.dev/zh-cn/documentation/">官方文档</a> 学习。整理的可用代码在最后。</p>
<p>这篇文章涉及以下知识点：</p>
<ul>
<li>Selenium 爬取动态页面</li>
<li>Selenium 对网页元素的定位与查找</li>
<li>Selenium 处理 iframe（比如登录框）</li>
<li>Selenium 动作链</li>
<li>Selenium 显性等待（处理动态页面加载）</li>
</ul>
<span id="more"></span>

<h2 id="0x01-需求"><a href="#0x01-需求" class="headerlink" title="0x01 需求"></a>0x01 需求</h2><p>由于这个网站的模型是下面这种动态选择参数，然后下载文件的构造，但是纯手工一个一个选择然后下载的工作太繁琐了。所以希望实现对一个页面中的所有参数型号的零件批量下载。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204081143687.png"></p>
<h2 id="0x02-解决方案"><a href="#0x02-解决方案" class="headerlink" title="0x02 解决方案"></a>0x02 解决方案</h2><p>通过 Selenium 模拟鼠标点击，从而实现批量下载一个页面下所有参数的零件。</p>
<h3 id="使用-Selenium-控制的浏览器打开网页"><a href="#使用-Selenium-控制的浏览器打开网页" class="headerlink" title="使用 Selenium 控制的浏览器打开网页"></a>使用 Selenium 控制的浏览器打开网页</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">from selenium import webdriver

# 导入所需模块
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver import ActionChains

options &#x3D; webdriver.ChromeOptions()
options.add_experimental_option(&quot;excludeSwitches&quot;, [&quot;enable-automation&quot;])
options.add_experimental_option(&#39;useAutomationExtension&#39;, False)

# 获取浏览器对象
driver &#x3D; webdriver.Chrome()
# 设置显性等待
wait&#x3D; WebDriverWait(driver, 20)
# 设置浏览器窗口大小
driver.maximize_window()
# 隐藏浏览器特征
driver.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123;
  &quot;source&quot;: &quot;&quot;&quot;
    Object.defineProperty(navigator, &#39;webdriver&#39;, &#123;
      get: () &#x3D;&gt; undefined
    &#125;)
  &quot;&quot;&quot;
&#125;)

url &#x3D; &quot;http:&#x2F;&#x2F;web.3dsource.cn&#x2F;2012110910&#x2F;product_181.html&quot;
driver.get(url)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>其中设置了一些反反爬的东西。最大化窗口防止有些元素在小窗口里位置不对，或者加载不出来。</p>
<h3 id="登录网站"><a href="#登录网站" class="headerlink" title="登录网站"></a>登录网站</h3><p>可能因为这种方式打开网页没有 cookies 吧，所以不能使用之前的信息自动登录。为了避免每打开一个零件页面就要输一次账号密码，先写一个自动登录的脚本。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 导入动作链模块
from selenium.webdriver import ActionChains

# 登录
wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;divSearch&quot;]&#x2F;div&#x2F;div&#x2F;div[2]&#x2F;div[2]&#x2F;a&#39;)))
driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;divSearch&quot;]&#x2F;div&#x2F;div&#x2F;div[2]&#x2F;div[2]&#x2F;a&#39;).click()
# 定位 iframe
iframe &#x3D; driver.find_elements(By.TAG_NAME, &#39;iframe&#39;)[-1]
# 切换到 iframe 框架
driver.switch_to.frame(iframe)
# 使用动作链
# 实例化动作链对象
action &#x3D; ActionChains(driver)
# 找到并选择账号密码登录
change_login &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;app&quot;]&#x2F;div&#x2F;div[2]&#x2F;div[1]&#x2F;div&#x2F;div[1]&#x2F;ul&#x2F;li[2]&#39;)
action.click(change_login).perform()
# 找到账号框并输入
user_input &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;app&quot;]&#x2F;div&#x2F;div[2]&#x2F;div[1]&#x2F;div&#x2F;div[3]&#x2F;div[1]&#x2F;div[1]&#x2F;div[1]&#x2F;input&#39;)
user_input.send_keys(&#39;******&#39;)
# 找到密码框并输入
pwd_input &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;app&quot;]&#x2F;div&#x2F;div[2]&#x2F;div[1]&#x2F;div&#x2F;div[3]&#x2F;div[1]&#x2F;div[2]&#x2F;div[1]&#x2F;input&#39;)
pwd_input.send_keys(&#39;******&#39;)
# 找到登录按钮并点击
#time.sleep(2)
login_button &#x3D; driver.find_element(by&#x3D;By.XPATH, value&#x3D;&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;app&quot;]&#x2F;div&#x2F;div[2]&#x2F;div[1]&#x2F;div&#x2F;div[3]&#x2F;a&#39;)
action.click(login_button).perform()
# 释放动作链
action.release().perform()
# 从 iframe 跳出
driver.switch_to.default_content()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>这一部分的难点主要是这个登陆小窗口是通过 iframe 实现的，相当于是代码在另一个网页里，所以需要<strong>切换到 iframe 框架</strong>这么一步操作，最后还要记得跳出来，<strong>回到主页面</strong>，否则不能对原网页的元素操作。</p>
<p>由于该网页中有两个 iframe，而且没有 id 之类的元素，我只能通过 <code>By.TAG_NAME</code> 来定位。我一开始定位错了（定位到了第一个），所以死活不能对登录框操作。现在是定位到第二个，然后所有问题就都解决了。</p>
<h3 id="检测并遍历规格与类型的列表"><a href="#检测并遍历规格与类型的列表" class="headerlink" title="检测并遍历规格与类型的列表"></a>检测并遍历规格与类型的列表</h3><p>通过这一步看看怎么打开下图中这两个选择框，并且遍历里面的所有元素。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204081143687.png"></p>
<p>这一步会碰到的问题是，这两个选择框也是动态实现的。源码里面用了 js 方法做了动态实现。也就是说点击之后，选择框的代码才会<strong>动态加载</strong>到网页中。如果程序执行太快，网页中还没加载出来这一部分代码，那么就会报错，显示该页面找不到元素之类的错误。代码中使用了<strong>显性等待</strong>的方法来解决这个问题，<code>wait</code> 是在第一步中定义的。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 遍历规格参数列表
wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[1]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;)))
driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[1]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;).click()
wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_0&quot;]&#39;)))
ds &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_0&quot;]&#39;)
children &#x3D; ds.find_elements(By.TAG_NAME, &#39;dd&#39;)
l &#x3D; len(children)
print(len(children))
driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[1]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;).click()

# 遍历类型参数列表
wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[5]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;)))
driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[5]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;).click()
wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_4&quot;]&#39;)))
typediv &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_4&quot;]&#39;)
type &#x3D; typediv.find_elements(By.TAG_NAME, &#39;dd&#39;)
l_type &#x3D; len(type)
print(l_type)
driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[5]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;).click()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>显性等待</strong>很重要，我想这也是爬取动态页面会经常遇到的问题。</p>
<h3 id="下载所有规格"><a href="#下载所有规格" class="headerlink" title="下载所有规格"></a>下载所有规格</h3><p>先不去管类型，尝试把所有规格都下载下来。其实就是两步，首先依次选择（选择框中的）规格，然后点击下载。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 下载所有规格
for k in range(l):
    wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[1]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;)))
    driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[1]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;).click()
    wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_0&quot;]&#39;)))
    ds &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_0&quot;]&#39;)
    children &#x3D; ds.find_elements(By.TAG_NAME, &#39;dd&#39;)
    
    children[k].click()

    wait.until(EC.visibility_of_element_located((By.LINK_TEXT, &quot;下载&quot;)))
    driver.find_element(By.LINK_TEXT, &quot;下载&quot;).click()
    wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;tw_optionDownloadFile&quot;]&#x2F;div[2]&#x2F;div&#x2F;div[6]&#x2F;a[1]&#39;)))
    driver.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;tw_optionDownloadFile&quot;]&#x2F;div[2]&#x2F;div&#x2F;div[6]&#x2F;a[1]&#39;).click()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>children[k].click()</code> 之前是同上一步<em>遍历规格参数列表</em>一样的，重复一遍是因为我一开始碰到了找不到元素的问题，也就是上一步说的，动态加载那部分没有加载出来。所以先把选择框打开，让元素都加载出来。重新存一遍 <code>children</code> 是防止动态加载的这个选择框的 <code>XPATH</code> 是动态的（可能不需要，可以去掉试试）。<code>children[k].click()</code> 之后的代码就是点击下载按钮了。同样每一步都设置了显性等待。</p>
<h3 id="依次选择所有类型"><a href="#依次选择所有类型" class="headerlink" title="依次选择所有类型"></a>依次选择所有类型</h3><p>本来以为我大不了手动切换一下类型，然后下载该类型的所有规格就好了。但没想到，每选一次规格，网页会自动切回第一个类型。这样的话就只能遍历规格的过程中，加入遍历类型。比如先选择第一个规格，然后下载第一个规格的第一个类型，然后第一个规格的第二个类型，再选择第二个规格，以此类推。</p>
<p>这里只尝试依次选择所有类型。有用到<em>检测并遍历规格与类型的列表</em>这一节中统计的规格数量 <code>l_type</code>。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for t in range(l_type):
    wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[5]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;)))
    driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[5]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;).click()
    wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_4&quot;]&#39;)))
    typediv &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_4&quot;]&#39;)
    types &#x3D; typediv.find_elements(By.TAG_NAME, &#39;dd&#39;)
    len(types)
    types[t].click()
	# sleep 为了看清切换的操作，实际应用时可以不加
    time.sleep(2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="下载所有规格与类型参数的零件"><a href="#下载所有规格与类型参数的零件" class="headerlink" title="下载所有规格与类型参数的零件"></a>下载所有规格与类型参数的零件</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 下载所有参数
for k in range(l):
    wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[1]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;)))
    driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[1]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;).click()
    wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_0&quot;]&#39;)))
    ds &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_0&quot;]&#39;)
    children &#x3D; ds.find_elements(By.TAG_NAME, &#39;dd&#39;)
    
    children[k].click()
    time.sleep(2)
    
    for t in range(l_type):
        action &#x3D; ActionChains(driver)
        wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[5]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;)))
        driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[5]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;).click()
        wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_4&quot;]&#39;)))
        typediv &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_4&quot;]&#39;)
        types &#x3D; typediv.find_elements(By.TAG_NAME, &#39;dd&#39;)

        action.click(types[t]).perform()
        wait.until(EC.visibility_of_element_located((By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;divContent&quot;]&#x2F;div[2]&#x2F;div[3]&#x2F;div[1]&#x2F;div[2]&#x2F;div[2]&#x2F;div[2]&#x2F;div&#x2F;div[3]&#x2F;a[3]&#39;)))
        download &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;divContent&quot;]&#x2F;div[2]&#x2F;div[3]&#x2F;div[1]&#x2F;div[2]&#x2F;div[2]&#x2F;div[2]&#x2F;div&#x2F;div[3]&#x2F;a[3]&#39;)
        action.click(download).perform()
        
        wait.until(EC.visibility_of_element_located((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;tw_optionDownloadFile&quot;]&#x2F;div[2]&#x2F;div&#x2F;div[6]&#x2F;a[1]&#39;)))
        download2 &#x3D; driver.find_element_by_xpath(&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;tw_optionDownloadFile&quot;]&#x2F;div[2]&#x2F;div&#x2F;div[6]&#x2F;a[1]&#39;)
        action.click(download2).perform()
        action.release().perform()
        time.sleep(5)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>基础代码如上，还没定义方法。中间的一些 <code>time.sleep</code> 是为了等待网页，比如最后下载完的等待，因为有时网页的服务器慢了，好几秒还没完成下载的动作，所以页面还停留在当前的下载小窗上，导致下一个循环重新设置参数报错（找不到元素位置），所以设置一个等待五秒，可以根据自己的网速测试决定。</p>
<p>明明选择参数有一个显性等待，为什么还会报错呢？我猜是因为有下载小窗的时候，那些选择框不可选中了，等结束才可以选中，所以哪怕显性等待结束再选择也会报错。而且我这里只是简单的统一使用定位到可见元素的等待，可见不等于可点击。<code>EC.element_to_be_clickable</code> 这个方法是元素等待直到元素被加载，为可见状态，并且是可点击的状态，才会结束等待。估计换上这个就可以了。</p>
<h2 id="0x03-拓展"><a href="#0x03-拓展" class="headerlink" title="0x03 拓展"></a>0x03 拓展</h2><p>其他零件的网页布局可能会不一样，所以以上针对这一个网页的 <code>XPATH</code> 很有可能在别的网页失效。更合理的设置是通过统一规定的 <code>class</code> 来找下面的 <code>onclick</code>，通过一些相对位置不会变的方式来定位（比如这一个选择框必定在 <code>&lt;div class=&quot;cnt&quot;&gt;</code> 中，然后再从这个标签对下面定位需要点击的位置）。</p>
<p>还有一种情况是某些规格的类型只有一个 <code>类型A</code>，但是会多出来比如外径的参数选择。那么选择另一种类型时就会报错，因为没有 <code>类型B</code>。</p>
<h2 id="0x04-补充最终代码"><a href="#0x04-补充最终代码" class="headerlink" title="0x04 补充最终代码"></a>0x04 补充最终代码</h2><h3 id="检测下载弹窗关闭"><a href="#检测下载弹窗关闭" class="headerlink" title="检测下载弹窗关闭"></a>检测下载弹窗关闭</h3><p>在 <code>def download_action(driver)</code> 中需要这样一个功能：检测下载弹窗是否关闭。</p>
<p>因为下载弹窗关闭后，才可以进行下一个零件的下载。但是下载的时间不确定，因为不仅取决于自己的网速，还取决于网站服务器的情况。</p>
<p>先尝试了显性等待，<code>EC.staleness_of</code> 方法检测的内容需要从 Dom 中移除，这个案例中好像是不会移除，所以不适用。检查参数框按键或者下载按键是否可点击也不适用，不知原因。一开始没找到优雅的解决方案，直接用最简单的方法 <code>time.sleep</code>，设置 6 秒报错（但某一个下载时长超了），所以增加到 10 秒。</p>
<p><strong>后续解决</strong>：<code>wait.until_not(EC.presence_of_element_located())</code></p>
<p>后来终于找到还有 <code>until_not</code> 的方法，经过实验是可以的。方法同 <code>until</code>，作用就是相反的。</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>代码规整美化了一下。</p>
<p>代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from selenium import webdriver

# 导入所需模块
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver import ActionChains

options &#x3D; webdriver.ChromeOptions()
options.add_experimental_option(&quot;excludeSwitches&quot;, [&quot;enable-automation&quot;])
options.add_experimental_option(&#39;useAutomationExtension&#39;, False)

# 获取浏览器对象
driver &#x3D; webdriver.Chrome()
# 设置显性等待
wait&#x3D; WebDriverWait(driver, 20)
# 设置隐形等待
driver.implicitly_wait(3)
# 设置浏览器窗口大小
driver.maximize_window()
driver.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123;
  &quot;source&quot;: &quot;&quot;&quot;
    Object.defineProperty(navigator, &#39;webdriver&#39;, &#123;
      get: () &#x3D;&gt; undefined
    &#125;)
  &quot;&quot;&quot;
&#125;)

# 遍历规格参数列表 获取该参数选择框内的子选项的数量
def get_num(addr1:str, addr2:str, driver):
    wait.until(EC.element_to_be_clickable((By.XPATH, addr1)))
    driver.find_element(By.XPATH, addr1).click()
    wait.until(EC.element_to_be_clickable((By.XPATH, addr2)))
    ds &#x3D; driver.find_element(By.XPATH, addr2)
    children &#x3D; ds.find_elements(By.TAG_NAME, &#39;dd&#39;)
    num_size &#x3D; len(children)
    print(len(children))
    driver.find_element(By.XPATH, addr1).click()
    return num_size

# 下载动作（点击下载）
# 因为下载按键是固定的 所以这里就不多设变量了
def download_action(driver):
    action &#x3D; ActionChains(driver)
    wait.until(EC.element_to_be_clickable((By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;divContent&quot;]&#x2F;div[2]&#x2F;div[3]&#x2F;div[1]&#x2F;div[2]&#x2F;div[2]&#x2F;div[2]&#x2F;div&#x2F;div[3]&#x2F;a[3]&#39;)))
    download &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;divContent&quot;]&#x2F;div[2]&#x2F;div[3]&#x2F;div[1]&#x2F;div[2]&#x2F;div[2]&#x2F;div[2]&#x2F;div&#x2F;div[3]&#x2F;a[3]&#39;)
    action.click(download).perform()
    
    wait.until(EC.element_to_be_clickable((By.XPATH,&#39;&#x2F;&#x2F;*[@id&#x3D;&quot;tw_optionDownloadFile&quot;]&#x2F;div[2]&#x2F;div&#x2F;div[6]&#x2F;a[1]&#39;)))
    download2 &#x3D; driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;tw_optionDownloadFile&quot;]&#x2F;div[2]&#x2F;div&#x2F;div[6]&#x2F;a[1]&#39;)
    action.click(download2).perform()
    # 等待下载的弹出页面下载完毕
    # 这里用 until_not 就可以解决等待弹出页面下载完毕的问题
	# XPATH 是我在网页里定位的弹窗的定位
    wait.until_not(EC.presence_of_element_located((By.XPATH, &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;tw_waitCreateFile&quot;]&#x2F;div[2]&#x2F;div[2]&#x2F;a&#39;)))
    action.release().perform()
    #time.sleep(10)

# 依次选择参数选择框
def scan_list(num:int, addr1:str, addr2:str, driver):
    for k in range(num):
        wait.until(EC.element_to_be_clickable((By.XPATH, addr1)))
        driver.find_element(By.XPATH, addr1).click()
        wait.until(EC.element_to_be_clickable((By.XPATH, addr2)))
        ds &#x3D; driver.find_element(By.XPATH, addr2)
        children &#x3D; ds.find_elements(By.TAG_NAME, &#39;dd&#39;)
        children[k].click()

# 单次选择操作（就是上面的方法去掉了遍历）
def click_list_once(k:int, addr1:str, addr2:str, driver):
    action &#x3D; ActionChains(driver)
    wait.until(EC.element_to_be_clickable((By.XPATH, addr1)))
    c1 &#x3D; driver.find_element(By.XPATH, addr1)
    action.click(c1).perform()
    wait.until(EC.element_to_be_clickable((By.XPATH, addr2)))
    ds &#x3D; driver.find_element(By.XPATH, addr2)
    children &#x3D; ds.find_elements(By.TAG_NAME, &#39;dd&#39;)
    c2 &#x3D; children[k]
    action.click(c2).perform()
    action.release().perform()

# 依次选择参数选择框并且下载（嵌套中最后一步）
def scan_list_download(num:int, addr1:str, addr2:str, driver):
    for k in range(num):
        action &#x3D; ActionChains(driver)
        wait.until(EC.element_to_be_clickable((By.XPATH, addr1)))
        driver.find_element(By.XPATH, addr1).click()
        wait.until(EC.element_to_be_clickable((By.XPATH, addr2)))
        ds &#x3D; driver.find_element(By.XPATH, addr2)
        children &#x3D; ds.find_elements(By.TAG_NAME, &#39;dd&#39;)
        download &#x3D; children[k]
        action.click(download).perform()
        action.release().perform()
        download_action(driver)
		# 这里加个强行 sleep，防止爬取太快被网站反爬
        time.sleep(8)  # 等待8秒等下载弹出的 iframe 消失
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>首先定义一些需要用到的方法。点击动作好像加上动作链会解决很多问题。</p>
<p>然后以 <a href="http://web.3dsource.cn/2012110910/product_192.html">圆轮缘手轮 JB&#x2F;T7273.5-1994 | 3DSource零件库</a> 这个页面为例，里面有三个参数框。</p>
<p>三个参数框的例子代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 有几个参数选择框 写几对参数选择框的 XPATH 定位
# 在每一对 XPATH 中：
# 第一个是展开参数框 让动态元素加载出来
# 第二个是定位展开的参数框里面的列表
addr1 &#x3D; &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[1]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;
addr2 &#x3D; &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_0&quot;]&#39;
addr3 &#x3D; &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[2]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;
addr4 &#x3D; &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_1&quot;]&#39;
addr5 &#x3D; &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;parameter_list&quot;]&#x2F;tbody&#x2F;tr[3]&#x2F;td[2]&#x2F;div&#x2F;div[1]&#x2F;i&#39;
addr6 &#x3D; &#39;&#x2F;&#x2F;*[@id&#x3D;&quot;param_select_2&quot;]&#39;

# 获得第一层遍历的 num_size1
# 虽然后面两个 num_size 在每次遍历时需要重新读取 但这里可以作为测试先输出个结果
num_size1 &#x3D; get_num(addr1, addr2, driver)
num_size2 &#x3D; get_num(addr3, addr4, driver)
num_size3 &#x3D; get_num(addr5, addr6, driver)

# 最终遍历
# 因为第一个参数的数量不会变的 以他为第一层遍历 可以直接保存不改
# 后面的参数都可能会变 所以每次遍历都重新获取下一层遍历的参数数量
# 以此类推 保证遍历列表不会超出 index
for x in range(num_size1):
    click_list_once(x, addr1, addr2, driver)
    time.sleep(2)
    num_size2 &#x3D; get_num(addr3, addr4, driver)
    for y in range(num_size2):
        click_list_once(y, addr3, addr4, driver)
        time.sleep(2)  # 加个 sleep 等待网页加载
        num_size3 &#x3D; get_num(addr5, addr6, driver)
		# 第三个参数框的遍历并下载
        scan_list_download(num_size3, addr5, addr6, driver)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>总共 88 个零件，下载完成：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202204081624820.png"></p>
<p>但是，还是但是，这种只能处理这种始终是三个参数框的网页。比如这个网页 <a href="http://web.3dsource.cn/2012110910/product_188.html">波纹圆轮缘手轮 JB&#x2F;T7273.6-1994 | 3DSource零件库</a> ，刚开始是两个参数框，然后有的规格参数选择之后，类型参数只剩一个了；还有些规格选择之后，又会多出来一个参数框（变成共三个）。所以又要添加一些判断语句了，暂时先这样吧，够用就行。毕竟没剩多少需要的数据了，都搞完善怕是还没我徒手下的快。</p>
<p>对于别的零件网页有些参数框位置不一样的，改一下对应 <code>addr</code> 的 <code>XPATH</code> 就好。参数框数量不一样的，改一下遍历循环的次数就好。</p>
<p>就酱 ^ ^</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>通过实战学到了很多“教科书”上没有的内容，所以越来越发现要多动手，或者说学一个东西可以先动手，然后在过程中学习理论。除此之外，遇到了问题要有清晰的逻辑来解决，比如 iframe 定位错那里，我纠结了好久是不是我 <code>click()</code> 不对，是不是 iframe 里面的 <code>XPATH</code> 不能这样写。</p>
<p>这个三维模型网站中其他的零件页面其实还有其他的情况，比如有更多的参数可以选择，或者有的只有规格可以选择，或者中途参数框数量改变。这次爬虫只能说勉强满足需求，想要实现对所有零件网页的一键式下载要复杂太多了。Anyway，也是学到了蛮有意思的东西。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://www.selenium.dev/zh-cn/documentation/">Selenium 浏览器自动化项目 | Selenium</a><br><a href="https://blog.51cto.com/xianhuan/5108646">Selenium 之显性等待详解</a><br><a href="https://blog.csdn.net/huilan_same/article/details/52544521">Python selenium 三种等待方式解读</a><br><a href="https://cloud.tencent.com/developer/article/1596841">Selenium自动化测试-iframe处理</a><br><a href="http://web.3dsource.cn/web.html">零件库网页版</a></p>
]]></content>
      <categories>
        <category>爬虫与网络编程基础</category>
      </categories>
      <tags>
        <tag>Spider</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫与网络编程基础-Task01：计算机网络基础</title>
    <url>/65GTKQ/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>为了使网络上获得的数据干净整洁，并且可以为自己的应用所用，我们得想出一些形式来表示应用与网络之间来来往往的数据。最常使用的两种技术就是：XML 和 JSON。</p>
<span id="more"></span>

<p>但是 Python 中的列表或者字典并不能在 Java 中使用（Java 中使用 HashMap），所以不能直接把 Python 中的字典传给 Java，而是应该发送 Python 与 Java 都能接受的数据。</p>
<p>因此，两者之间需要一种 <em>Wire Format</em> 来作为互相交换数据的媒介。而 XML 和 JSON 是两种比较常用的 <em>Wire Format</em>。</p>
<h2 id="0x01-Python-中创建-List"><a href="#0x01-Python-中创建-List" class="headerlink" title="0x01 Python 中创建 List"></a>0x01 Python 中创建 List</h2><h3 id="1-创建列表"><a href="#1-创建列表" class="headerlink" title="1. 创建列表"></a>1. 创建列表</h3><p>在 Pyhon 中创建一个 list，存储以下个人信息（姓名、年龄、成绩）：[小王、40、50]，[小贾、50、23]</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 定义列表
list &#x3D; [(&quot;小王&quot;,40,50),(&quot;小贾&quot;,50,23)]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[(&#39;小王&#39;, 40,50), (&#39;小贾&#39;, 50,23)]</code></p>
</blockquote>
<h3 id="2-导入列表"><a href="#2-导入列表" class="headerlink" title="2. 导入列表"></a>2. 导入列表</h3><p>如果想把 Numpy 数组转成 Json 格式，可以首先将数组转为列表。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">data &#x3D; data.tolist()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h2 id="0x02-将数据存储为-Json-格式，并进行读取"><a href="#0x02-将数据存储为-Json-格式，并进行读取" class="headerlink" title="0x02 将数据存储为 Json 格式，并进行读取"></a>0x02 将数据存储为 Json 格式，并进行读取</h2><h3 id="JSON-库的一些用法"><a href="#JSON-库的一些用法" class="headerlink" title="JSON 库的一些用法"></a>JSON 库的一些用法</h3><table>
<thead>
<tr>
<th>方法</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>json.dumps ()</td>
<td>将 Python 对象编码成 Json 字符串</td>
</tr>
<tr>
<td>json.loads ()</td>
<td>将 Json 字符串解码成 Python 对象</td>
</tr>
<tr>
<td>json.dump ()</td>
<td>将 Python 中的对象转化成 Json 储存到文件中</td>
</tr>
<tr>
<td>json.load ()</td>
<td>将文件中的 Json 的格式转化成 Python 对象提取</td>
</tr>
</tbody></table>
<p>json.dump () 和 json.dumps () 的区别</p>
<ul>
<li>json.dumps () 是把 Python 对象转换成 Json 对象的一个过程，生成的是字符串。</li>
<li>json.dump () 是把 Python 对象转换成 Json 对象生成一个 fp 的文件流，和文件相关。</li>
</ul>
<p>所以后面多的一个 <code>s</code> 可以理解为 <code>string</code></p>
<p>更详细的内容可以参考 <a href="https://www.w3cschool.cn/article/30808038.html">Python json.dumps()函数使用解析 | w3c 笔记</a></p>
<h3 id="1-存储到-JSON-文件中"><a href="#1-存储到-JSON-文件中" class="headerlink" title="1. 存储到 JSON 文件中"></a>1. 存储到 JSON 文件中</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 导入 json 模块
import json

# 为新建 json 文件命名
filename &#x3D; &#39;info.json&#39;
# 使用json.dump()方法转为json格式数据
with open(filename, &#39;w&#39;) as file:
    json.dump(list, file)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 读取 json 文件
with open(filename, &#39;r&#39;) as file:
    info &#x3D; json.load(file)

print (info)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>printout:<br><code>[[&#39;小王&#39;, 40,50], [&#39;小贾&#39;, 50,23]]</code></p>
</blockquote>
<p>Tips：</p>
<ul>
<li>默认会转为二进制数据，目前存储的 Json 文件内容为：<code>[[&quot;\u5c0f\u738b&quot;, 40,50], [&quot;\u5c0f\u8d3e&quot;, 50,23]]</code>。读取的时候默认又转到了可读的 Python 对象。所以读取并打印的时候看见的是字符。</li>
<li>如果希望文件中不使用二进制存储，通过 <code>ensure_ascii=False</code> 设置不转为二进制，即存储文件时：<code>json.dump(list, file, ensure_ascii=False)</code>。</li>
</ul>
<h3 id="2-编码成-JSON-字符串"><a href="#2-编码成-JSON-字符串" class="headerlink" title="2. 编码成 JSON 字符串"></a>2. 编码成 JSON 字符串</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 使用 json.dumps() 方法转为json格式数据
# 注意：默认会转为二进制数据，使用 ensure_ascii&#x3D;False 设置不转为二进制
json_data &#x3D; json.dumps(list, ensure_ascii&#x3D;False)
print (json_data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[[&quot;小王&quot;, 40,50], [&quot;小贾&quot;, 50,23]]</code></p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">json_data &#x3D; json.dumps(list)
print(json_data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[[&quot;\u5c0f\u738b&quot;, 40,50], [&quot;\u5c0f\u8d3e&quot;, 50,23]]</code></p>
</blockquote>
<p><code>json.dumps()</code> 也是默认编码成二进制数据。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 打印数据类型
print(type(list))
print(type(json_data))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>&lt;class ‘list’&gt;<br>&lt;class ‘str’&gt;</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">py_data &#x3D; json.loads(json_data)
print(type(py_data))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>&lt;class ‘list’&gt;</p>
</blockquote>
<h2 id="0x03-将数据存储为-XML-格式，并进行读取"><a href="#0x03-将数据存储为-XML-格式，并进行读取" class="headerlink" title="0x03 将数据存储为 XML 格式，并进行读取"></a>0x03 将数据存储为 XML 格式，并进行读取</h2><h3 id="XML-eXtensible-Markup-Language"><a href="#XML-eXtensible-Markup-Language" class="headerlink" title="XML (eXtensible Markup Language)"></a>XML (eXtensible Markup Language)</h3><p>主要目的是帮助信息系统交换结构化数据，是一种比 JSON 要老一些的技术。</p>
<p>XML 代码美化工具：Pretty Printer。</p>
<p>XML 是一种树状结构的数据。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203142247567.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203142248500.png"></p>
<p>And you can think of this as, like folders on your computer.</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203142255711.png"></p>
<p>我们要做的，就是从根节点，一层一层向下挖掘，找到这些数据。</p>
<p>更多信息可以参考：<br> <a href="https://www.w3school.com.cn/xml/index.asp">XML 教程</a><br> <a href="https://www.runoob.com/xml/xml-tutorial.html">XML 教程 | 菜鸟教程</a><br> <a href="https://www.coursera.org/learn/python-network-data/lecture/EaR2d/13-2-extensible-markup-language-xml">13.2 eXtensible Markup Language (XML) | Coursera</a></p>
<h3 id="1-存储为-XML-格式"><a href="#1-存储为-XML-格式" class="headerlink" title="1. 存储为 XML 格式"></a>1. 存储为 XML 格式</h3><p>首先将 List 转为字典：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">index &#x3D; [&#39;0&#39;, &#39;1&#39;] # 这里要用 str 型
d &#x3D; dict(zip(index, list))
print(d)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&#123;&#39;0&#39;: (&#39;小王&#39;, 40,50), &#39;1&#39;: (&#39;小贾&#39;, 50,23)&#125;</code></p>
</blockquote>
<p>然后定义一个方法将字典转为 XML。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from xml.etree.ElementTree import Element

# Turn a simple dict of key&#x2F;value pairs into XML
def dict_to_xml(tag, d):
    elem &#x3D; Element(tag)
    for key, val in d.items():
        child &#x3D; Element(key)
        child.text &#x3D; str(val)
        elem.append(child)
    return elem<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">e &#x3D; dict_to_xml(&#39;info&#39;, d)
print(e)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&lt;Element &#39;info&#39; at 0x0000018CA90C69A8&gt;</code></p>
</blockquote>
<p>或者可以使用 <code>pip3 install dicttoxml</code> 来转换。</p>
<h3 id="2-读取-XML-格式"><a href="#2-读取-XML-格式" class="headerlink" title="2. 读取 XML 格式"></a>2. 读取 XML 格式</h3><p>转换结果是一个 <code>Element</code> 实例。对于 I&#x2F;O 操作，使用 <code>xml.etree.ElementTree</code> 中的 <code>tostring()</code> 函数很容易就能将它转换成一个字节字符串。例如：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from xml.etree.ElementTree import tostring
tostring(e)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>b&quot;&lt;info&gt;&lt;0&gt;(&#39;&amp;#23567;&amp;#29579;&#39;, 40,50)&lt;/0&gt;&lt;1&gt;(&#39;&amp;#23567;&amp;#36158;&#39;, 50,23)&lt;/1&gt;&lt;/info&gt;&quot;</code></p>
</blockquote>
<p>目前的 <code>e</code> 就是一个树状结构的示例，探索一下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">print(e.tag)
print(e.text)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>info</code><br><code>None</code></p>
</blockquote>
<p>相当于目前在 <code>root</code> 节点。</p>
<p>把子节点打印出来：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for child in e:
    print(child.tag)
    print (child.text)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>0<br>(‘小王’, 40,50)<br>1<br>(‘小贾’, 50,23)</p>
</blockquote>
<p>或者也可以将节点读取出来后，存成字典或者列表。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">to_dict &#x3D; &#123;&#125;
to_list &#x3D; []
for child in e:
    to_dict[child.tag] &#x3D; child.text
    
to_list.append(to_dict)

print(to_dict)
print (to_list)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&#123;&#39;0&#39;: &quot;(&#39;小王&#39;, 40,50)&quot;, &#39;1&#39;: &quot;(&#39;小贾&#39;, 50,23)&quot;&#125;</code><br><code>[&#123;&#39;0&#39;: &quot;(&#39;小王&#39;, 40,50)&quot;, &#39;1&#39;: &quot;(&#39;小贾&#39;, 50,23)&quot;&#125;]</code></p>
</blockquote>
<p>也可以使用 <code>pip3 install xmltodict</code> 来转换并读取。</p>
<h2 id="0x04-计算机网络基础"><a href="#0x04-计算机网络基础" class="headerlink" title="0x04 计算机网络基础"></a>0x04 计算机网络基础</h2><p>学习 <a href="https://www.runoob.com/w3cnote/summary-of-network.html">计算机网络基础</a> ，思考从打开 <a href="https://coggle.club/">coggle.club</a> 到网页展示，有什么步骤？</p>
<h3 id="1-网络层次分层"><a href="#1-网络层次分层" class="headerlink" title="1. 网络层次分层"></a>1. 网络层次分层</h3><h4 id="OSI-模型"><a href="#OSI-模型" class="headerlink" title="OSI 模型"></a>OSI 模型</h4><ul>
<li><p>OSI 概念：</p>
<ul>
<li>为了使不同计算机厂家生产的计算机能够相互通信，以便在更大的范围内建立计算机网络，国际标准化组织（ISO）在 1978 年提出了”开放系统互联参考模型”，即著名的 OSI&#x2F;RM 模型（Open System Interconnection&#x2F;Reference Model）。“开放性”是指世界上任何地方、任何遵循 OSI 标准的系统，只要连接起来就能互相通信。</li>
<li>注意：OSI 是一种 <strong>模型</strong>，并不是实际投入使用的协议，而是用来了解和设计网络体系结构的。</li>
</ul>
</li>
<li><p>OSI 模型的目的：</p>
<ul>
<li>框架化；SOA。</li>
<li>规范不同系统的互联标准，使两个不同的系统能够较容易的通信，而不需要改变底层的硬件或者软件的逻辑。</li>
</ul>
</li>
<li><p>OSI 模型分为 7 层：</p>
<ul>
<li>将计算机网络体系结构的通信协议划分为七层，自下而上依次为：<ul>
<li>物理层（Physics Layer）</li>
<li>数据链路层（Data Link Layer）</li>
<li>网络层（Network Layer）</li>
<li>传输层（Transport Layer）</li>
<li>会话层（Session Layer）</li>
<li>表示层（Presentation Layer）</li>
<li>应用层（Application Layer）</li>
</ul>
</li>
<li>其中第四层完成数据传输服务，上面三层面向用户（ User Support Layers）。</li>
</ul>
</li>
</ul>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151239418.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151240138.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151240443.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151228935.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151249700.png"></p>
<h4 id="TCP-x2F-IP-协议"><a href="#TCP-x2F-IP-协议" class="headerlink" title="TCP&#x2F;IP 协议"></a>TCP&#x2F;IP 协议</h4><p>目前使用最多的是 TCP&#x2F;IP 四层协议。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151129079.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151301087.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203151135178.png"></p>
<h4 id="输入-URL-后执行的全过程"><a href="#输入-URL-后执行的全过程" class="headerlink" title="输入 URL 后执行的全过程"></a>输入 URL 后执行的全过程</h4><p>从打开 <a href="https://coggle.club/">coggle.club</a> 到网页展示，有什么步骤？</p>
<ol>
<li>DNS 域名解析：客户端浏览器通过 DNS 解析得到 Coggle 网站的 IP 地址。</li>
<li>客户端与服务端建立 TCP 连接：TCP 三次握手。</li>
<li>客户端发起 HTTP 请求：建立 TCP 连接后，把客户端信息（携带 cookies）传递给服务端。</li>
<li>服务端响应 HTTP 请求。</li>
<li>释放 TCP 连接：TCP 四次挥手。</li>
<li>浏览器解析 HTML 代码，并请求 HTML 代码中的资源（如图片、音频、视频、CSS、JS 等等）。</li>
<li>浏览器对页面进行渲染呈现给用户。</li>
</ol>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>JSON 格式比较好理解，不多赘述。</p>
<p>XML 格式是一种树状结构的数据表示，数据的写入与读取都是基于这种结构的。所以首先要理解 XML 的这种内在结构，才能明白为什么 XML 的读写操作是这个样子的，否则直接照搬方法也很难清楚为什么这样做。我这里也只是浅尝辄止的了解基础概念，因为工作重心不在这，暂时不需要太深入的研究。</p>
<p>虽然 XML 的格式看起来跟 HTML 十分相似，但这两种格式是负责不一样的工作的。<strong>XML 被设计用来传输和存储数据。HTML 被设计用来显示数据。</strong> 而且 HTML 中使用的标签（以及 HTML 的结构）是预定义好的；而 <strong>XML 仅仅是纯文本</strong>，允许创作者定义自己的标签和自己的文档结构。也就是说是针对自己的应用自行设计标签与结构，使得能够读懂 XML 的应用程序可以有针对性地处理 XML 的标签。标签的功能性意义依赖于应用程序的特性。</p>
<p>之前学过的计算机网络基础大部分都还给老师了，只是模糊地记得一些概念，但要我去说每个层或者每种协议里面传输了哪些数据，数据分为哪几种，各自负责什么功能，或者计算一下 IP 已经是想不起来了。这次先复习一下 OSI 和 TCP&#x2F;IP 的基本结构。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>Json 读写：<br> <a href="https://www.cnblogs.com/tizer/p/11067098.html">Python 中如何将数据存储为 json 格式的文件</a><br> <a href="https://www.w3cschool.cn/article/30808038.html">Python json.dumps()函数使用解析 | w3c 笔记</a><br> <a href="https://cloud.tencent.com/developer/article/1181728">Python json 模块 dumps、dump、loads、load 的使用</a></p>
<p>XML 读写：<br><a href="https://www.w3school.com.cn/xml/xml_intro.asp">XML 简介</a><br> <a href="https://www.coursera.org/learn/python-network-data/lecture/EaR2d/13-2-extensible-markup-language-xml">13.2 eXtensible Markup Language (XML) | Coursera</a><br> <a href="https://zhmou.github.io/2021/10/12/Use-Python-to-Read-and-Write-XML-files/">使用 Python 读写 XML 文件 | Zh 某的备忘录</a><br> <a href="https://www.bilibili.com/video/BV1Sx411Q7zp?from=search&seid=13822148224876736274&spm_id_from=333.337.0.0">【一起学 Python·网络爬虫】XML 简介与解析 | bilibili</a><br> <a href="https://blog.csdn.net/Strive_0902/article/details/119856186">Python 列表转字典</a><br> <a href="https://blog.csdn.net/Jarry_cm/article/details/104925292">字典和列表相互转换</a><br> <a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p05_turning_dictionary_into_xml.html">6.5 将字典转换为 XML — python3-cookbook 3.0.0 文档</a><br> <a href="https://www.jianshu.com/p/ae932d11f059">使用两个三方库来读写</a></p>
<p>计算机网络基础：<br> <a href="https://www.bilibili.com/video/BV1FL4y1M7oM?p=17">OSI、TCP&#x2F;IP、计算机基础、操作系统 | bilibili</a><br> <a href="https://www.bilibili.com/video/BV1ZD4y1R7RT/?spm_id_from=333.788.recommend_more_video.0">计算机基础知识 | bilibili</a><br> <a href="https://www.jianshu.com/p/9b9438dff7a2">图解 OSI 七层模型</a><br> <a href="https://www.runoob.com/w3cnote/summary-of-network.html">计算机网络基础</a></p>
<p>输入 URL 后执行的全过程：<br> <a href="https://juejin.cn/post/6844903757877084174">这里的三次握手，四次挥手讲的很清楚。</a> ⭐⭐⭐<br> <a href="https://blog.csdn.net/qq_41837249/article/details/117001110">输入 URL 后的全部过程 会用到哪些协议？</a><br> <a href="https://blog.csdn.net/TTTZZZTTTZZZ/article/details/87214111">结合协议来讲，每个步骤在哪一层</a></p>
]]></content>
      <categories>
        <category>爬虫与网络编程基础</category>
      </categories>
      <tags>
        <tag>Coggle 30 Days of ML</tag>
        <tag>Spider</tag>
        <tag>Network Programming</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫与网络编程基础-Task02：HTTP 协议与 Requests</title>
    <url>/2F4301H/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>Requests 是一个 Python 公认的，优秀的第三方网络爬虫库，可以自动爬取 HTML 页面，自动进行网络请求的提交。所以本次任务首先来了解，学习 Requests 库以及 HTTP 协议。另外，由于Requests 库与 HTTP 协议的方法是一一对应的，所以会穿插着学习，以便更容易理解。</p>
<span id="more"></span>

<h2 id="0x01-初探-Requests-库"><a href="#0x01-初探-Requests-库" class="headerlink" title="0x01 初探 Requests 库"></a>0x01 初探 Requests 库</h2><p> <a href="https://docs.python-requests.org/zh_CN/latest/">Requests: 让 HTTP 服务人类 — Requests 2.18.1 文档</a><br> <a href="https://pypi.org/project/requests/">requests · PyPI</a></p>
<h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><p>安装 <code>requests</code>。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pip install requests<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="2-Requests-库的七个主要方法"><a href="#2-Requests-库的七个主要方法" class="headerlink" title="2. Requests 库的七个主要方法"></a>2. Requests 库的七个主要方法</h3><table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>requests.request()</td>
<td>构造一个请求，支持以下各方法的基础方法</td>
</tr>
<tr>
<td>requests.get()</td>
<td>获取 HTML 网页的主要方法，对应于 HTTP 的 GET</td>
</tr>
<tr>
<td>requests.head()</td>
<td>获取 HTML 网页头信息的方法，对应于 HTTP 的 HEAD</td>
</tr>
<tr>
<td>requests.post()</td>
<td>向 HTML 网页提交 POST 请求的方法，对应于 HTTP 的 POST</td>
</tr>
<tr>
<td>requests.put()</td>
<td>向 HTML 网页提交 PUT 请求的方法，对应于 HTTP 的 PUT</td>
</tr>
<tr>
<td>requests.patch()</td>
<td>向 HTML 网页提交局部修改请求，对应于 HTTP 的 PATCH</td>
</tr>
<tr>
<td>requests.delete()</td>
<td>向 HTML 页面提交删除请求，对应于 HTTP 的 DELETE</td>
</tr>
</tbody></table>
<p>先来动动手：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
r &#x3D; requests.get(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;) # GET
r.status_code # 检测请求状态码<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>200</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">r.encoding &#x3D; &#39;utf-8&#39; # 修改内容编码
r.text[:500] # 查看响应内容的字符串<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&#39;&lt;!DOCTYPE html&gt;\r\n&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta content=always name=referrer&gt;&lt;link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&gt;&lt;title&gt;百度一下，你就知道&lt;/title&gt;&lt;/head&gt; &lt;body link=#0000cc&gt; &lt;div id=wrapper&gt; &lt;div id=head&gt; &lt;div class=head_wrapper&gt; &lt;div class=s_form&gt; &lt;div class=s_form_wrapper&gt; &lt;div id=lg&gt; &lt;img hidefocus=true src=//www.baidu.com/img/bd_&#39;</code></p>
</blockquote>
<p>下面仅先介绍最常使用的 <code>requests.get()</code> 方法，其余的方法待学习了 HTTP 协议之后，再对照着理解。</p>
<h4 id="requests-get-方法"><a href="#requests-get-方法" class="headerlink" title="requests.get() 方法"></a>requests.get() 方法</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200044542.png"></p>
<p>具体参数：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">r &#x3D; requests.get(url, params, **kwargs)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<ul>
<li><code>url</code> ：拟获取页面的 URL 链接。</li>
<li><code>params</code> ：URL 中的额外参数，字典或者字节流格式，可选。</li>
<li><code>**kwargs</code> : 12 个控制访问的参数</li>
</ul>
<p>通过 <code>requests.get()</code> 方法返回得到的 Response 对象具有以下属性：</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r.status_code</td>
<td>HTTP 请求的返回状态，若为 200 则表示请求成功</td>
</tr>
<tr>
<td>r.text</td>
<td>HTTP 响应内容的字符串形式，即，url 对应的页面内容</td>
</tr>
<tr>
<td>r.encoding</td>
<td>从 HTTP header 中猜测的相应内容编码方式</td>
</tr>
<tr>
<td>r.apparent_encoding</td>
<td>从内容中分析出的响应内容编码方式（备选编码方式）</td>
</tr>
<tr>
<td>r.content</td>
<td>HTTP 响应内容的二进制形式</td>
</tr>
</tbody></table>
<p>这五个属性是最常使用的五个属性。<br><code>r.status_code</code> 获得的状态码，只有返回 200 表示请求成功，其他则为失败，如 404。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200100765.png" alt="|600"></p>
<p>使用 <code>requests.get()</code> 方法获取信息的基本流程：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200102154.png" alt="|500"></p>
<p><strong>Case  通过 get 获取网页信息</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
r &#x3D; requests.get(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)
r.status_code # 返回 200，确认响应成功
r.text[-300:] # 查看响应内容<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&#39;out Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;&amp;copy;2017&amp;nbsp;Baidu&amp;nbsp;&lt;a href=http://www.baidu.com/duty/&gt;ä½¿ç\x94¨ç\x99¾åº¦å\x89\x8då¿\x85è¯»&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;æ\x84\x8fè§\x81å\x8f\x8dé¦\x88&lt;/a&gt;&amp;nbsp;äº¬ICPè¯\x81030173å\x8f·&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;\r\n&#39;</code></p>
</blockquote>
<p>可以发现，返回的内容中有很多乱码。那么来看一下内容的编码方式。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">r.encoding # 从 header 猜测编码<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>‘ISO-8859-1’</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">r.apparent_encoding # 备选编码<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<blockquote>
<p>‘utf-8’</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">r.encoding &#x3D; &#39;utf-8&#39; # 使用备选编码替换当前编码方式
r.text[-300:] # 再次查看响应内容<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p><code>&#39; href=http://ir.baidu.com&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;&amp;copy;2017&amp;nbsp;Baidu&amp;nbsp;&lt;a href=http://www.baidu.com/duty/&gt;使用百度前必读&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;意见反馈&lt;/a&gt;&amp;nbsp;京 ICP 证 030173 号&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;\r\n&#39;</code></p>
</blockquote>
<p>经过替换编码方式，原来的一些乱码变成了中文。变得具有可读性了。为什么会这样？那就需要理解 Response 的编码。</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r.encoding</td>
<td>从 HTTP header 中猜测的相应内容编码方式</td>
</tr>
<tr>
<td>r.apparent_encoding</td>
<td>从内容中分析出的响应内容编码方式（备选编码方式）</td>
</tr>
</tbody></table>
<p><code>r.encoding</code>：如果 header 中不存在 charset 字段，则默认编码为 ISO-8859-1。<br>就是当我们刚刚访问百度的时候，默认的编码。但这个编码并不能解析中文，所以查看内容时，会出现乱码。</p>
<p><code>r.apparent_encoding</code>：根据网页内容分析出的编码方式。<br>为了避免上述情况，<code>Requests</code> 库还提供了另一种备选编码方案，从 HTTP 的内容部分（而不是头部份），去分析内容可能的编码形式。严格来说 <code>r.apparent_encoding</code> 比 <code>r.encoding</code> 更加准确，因为前者是实实在在去分析内容，找到可能的编码；而后者只是从 header 的相关字段中提取编码。所以当后者不能正确解码时，需要使用前者——备用编码，来解析返回的信息。这也是为什么示例中替换编码方式后，即可正确解析出中文的原因。</p>
<h3 id="3-爬取网页的通用代码框架"><a href="#3-爬取网页的通用代码框架" class="headerlink" title="3. 爬取网页的通用代码框架"></a>3. 爬取网页的通用代码框架</h3><p>注意 <code>Requests</code> 库有时会产生异常，比如网络连接错误、<code>HTTP</code> 错误异常、重定向异常、请求 <code>URL</code> 超时异常等等。所以我们需要判断 <code>r.status_codes</code> 是否为 200，在这里我们怎么样去捕捉异常呢？</p>
<p>这里我们可以利用 <code>r.raise_for_status()</code> 语句去捕捉异常，该语句在方法内部判断 <code>r.status_code</code> 是否等于 200，如果不等于，则抛出异常。</p>
<p>于是在这里我们有一个爬取网页的通用代码框架：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">try:
    r &#x3D; requests.get(url, timeout&#x3D;30) # 请求超时时间为 30 秒
    r.raise_for_status() # 如果状态不是 200，则引发 HTTPError 异常
    r.encoding &#x3D; r.apparent_encoding # 配置编码
    return r.text 
except:
    return &quot;产生异常&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200150745.png"></p>
<p>可见当 <code>url</code> 链接没有协议，产生错误时，抛出了异常。</p>
<p>通过这种框架，可以使我们的爬虫方法更加可靠，稳定。</p>
<h2 id="0x02-学习-HTTP-协议"><a href="#0x02-学习-HTTP-协议" class="headerlink" title="0x02 学习 HTTP 协议"></a>0x02 学习 HTTP 协议</h2><p>为了更好的理解 Requests 库中的这些方法，我们需要学习并了解 HTTP 协议。</p>
<h3 id="HTTP-协议简介"><a href="#HTTP-协议简介" class="headerlink" title="HTTP 协议简介"></a>HTTP 协议简介</h3><p>HTTP，<strong>H</strong>yper<strong>T</strong>ext <strong>T</strong>ransfer <strong>P</strong>rotocol，超文本传输协议。</p>
<p>HTTP 是万维网的数据通信的基础。</p>
<p>HTTP 是一种基于“请求与响应”模式的，无状态的应用层协议。</p>
<ul>
<li>“请求与响应”：用户发起请求，服务器做相关响应。</li>
<li>无状态：第一次请求与第二次请求之间，没有关联。并且对于发送过的请求或响应都不进行保存。</li>
<li>应用层协议：该协议基于 TCP 协议。</li>
</ul>
<h3 id="HTTP-工作原理"><a href="#HTTP-工作原理" class="headerlink" title="HTTP 工作原理"></a>HTTP 工作原理</h3><p>HTTP 协议定义 Web 客户端如何从 Web 服务器请求 Web 页面，以及服务器如何把 Web 页面传送给客户端。HTTP 协议采用了请求&#x2F;响应模型。客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。服务器以一个状态行作为响应，响应的内容包括协议的版本、成功或者错误代码、服务器信息、响应头部和响应数据。</p>
<p>以下是 HTTP 请求&#x2F;响应的步骤：</p>
<ol>
<li>客户端连接到 Web 服务器。</li>
<li>发送 HTTP 请求。</li>
<li>服务器接受请求并返回 HTTP 响应。</li>
<li>释放连接 TCP 连接。</li>
<li>客户端浏览器解析 HTML 内容。</li>
</ol>
<p>在浏览器地址栏键入 URL，按下回车之后会经历以下流程：</p>
<ol>
<li>浏览器向 DNS 服务器请求解析该 URL 中的域名所对应的 IP 地址。</li>
<li>解析出 IP 地址后，根据该 IP 地址和默认端口 80，和服务器建立 TCP 连接。</li>
<li>浏览器发出读取文件 (URL 中域名后面部分对应的文件) 的 HTTP 请求，该请求报文作为 TCP 三次握手的第三个报文的数据发送给服务器。</li>
<li>服务器对浏览器请求作出响应，并把对应的 HTML 文本发送给浏览器。</li>
<li>释放 TCP 连接。</li>
<li>浏览器将该 HTML 文本并显示内容。</li>
</ol>
<h3 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h3><p>URL，Universal Resource Locator，<strong>统一资源定位符</strong>。</p>
<p>HTTP 协议采用 URL 作为定位网络资源的标识。</p>
<p>URL 遵守一种标准的语法，它由协议、域名、端口、路径名称、查询字符串、以及锚部分这六个部分构成，其中端口可以省略，查询字符串和锚部分为参数。具体语法规则如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">protocol:&#x2F;&#x2F;host: port&#x2F;pathname?query#fragment<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>在上述语法规则中，<code>protocol</code> 表示协议，<code>host</code> 表示主机名（域名或 IP 地址），<code>port</code> 表示端口，<code>pathname</code> 表示路径名称，<code>query</code> 表示查询字符串，<code>fragment</code> 表示锚部分。<code>#</code> 代表网页中的一个位置，作为页面 <code>定位符</code> 出现在 URL 中。其右面的字符，就是该位置的 <code>标识符</code>（锚部分）。</p>
<p><strong>统一资源定位符</strong> 将从因特网获取信息的几种基本元素包含在一个简单的地址中：</p>
<ul>
<li>传输协议 - protocol<ul>
<li>如 http&#x2F;https&#x2F;ftp 等。</li>
</ul>
</li>
<li>服务器 - host<ul>
<li>通常为域名：因为 IP 地址不好记，因此用域名美化，可以理解为 IP 的别名。使用域名 -&gt; 通过 DNS 解析找到对应的 IP。</li>
<li>有时为 IP 地址：主机地址，每台电脑都有自己特定的标识，IPv4&#x2F;IPv6。</li>
</ul>
</li>
<li>端口号 - port （可省略）：可以理解为窗口，交流的端口<ul>
<li>http：默认打开 80 端口</li>
<li>https：默认打开 443 端口</li>
</ul>
</li>
<li>路径 - path：访问主机分享文件的地址<ul>
<li>以 <code>/</code> 字符区别路径中的每一个目录名称</li>
<li>文件路径：用户直接访问主机分享的文件</li>
<li>路由：目前比较流行的形式（后端监听地址访问事件，返回特定的内容）</li>
</ul>
</li>
<li>查询参数 - query：帮助用户访问到特定的资源<ul>
<li>格式 <code>?name=value&amp;name=value...</code></li>
<li>GET 模式的表单参数，以 <code>?</code> 字符为起点，每个参数以 <code>&amp;</code> 隔开，再以 <code>=</code> 分开参数名称与资料，通常以 UTF8 的 URL 编码，避开字符冲突的问题。</li>
</ul>
</li>
<li>锚点 - fragment：<ul>
<li><code>#fragment</code> 锚点，又叫命名锚记，是文档中的一种标记，网页设计者可以用它和 URL“锚”在一起，其作用像一个迅速定位器一样，可快速将访问者带到指定位置。</li>
<li>HTTP 请求不包括 <code>#</code> ：<code>#</code> 是用来指导浏览器动作的，对服务器端完全无用。</li>
</ul>
</li>
</ul>
<p>典型的统一资源定位符看上去是这样的：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">http:&#x2F;&#x2F;zh.wikipedia.org:80&#x2F;w&#x2F;index.php?title&#x3D;Special:%E9%9A%8F%E6%9C%BA%E9%A1%B5%E9%9D%A2&amp;printable&#x3D;yes<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>其中：</p>
<ol>
<li><code>http</code> 是协议；</li>
<li><code>zh.wikipedia.org</code> 是服务器；</li>
<li><code>80</code> 是服务器上的网络端口号；</li>
<li><code>/w/index.php</code> 是路径；</li>
<li><code>?title=Special:%E9%9A%8F%E6%9C%BA%E9%A1%B5%E9%9D%A2&amp;printable=yes</code> 是查询参数。</li>
</ol>
<p>大多数网页浏览器不要求用户输入网页中 <code>http://</code> 的部分，因为绝大多数网页内容是超文本传输协议文件。同样，<code>80</code> 是超文本传输协议文件的常用端口号，因此一般也不必写明。一般来说用户只要键入统一资源定位符的一部分就可以了。<strong>这只是浏览器中可以省略，写爬虫的时候，URL 中的传输协议是不可以省略的。</strong></p>
<p><strong>总结</strong>：URL 是通过 HTTP 协议存取资源的 Internet 路径，一个 URL 对应一个数据资源。</p>
<h3 id="HTTP-协议对资源的操作"><a href="#HTTP-协议对资源的操作" class="headerlink" title="HTTP 协议对资源的操作"></a>HTTP 协议对资源的操作</h3><table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>GET</td>
<td>请求获取 URL 位置的资源</td>
</tr>
<tr>
<td>HEAD</td>
<td>请求获取 URL 位置资源的响应消息报告，即获取该资源的头部信息</td>
</tr>
<tr>
<td>POST</td>
<td>请求向 URL 位置的资源后附加新的数据</td>
</tr>
<tr>
<td>PUT</td>
<td>请求向 URL 位置存储一个资源，覆盖原 URL 位置的资源</td>
</tr>
<tr>
<td>PATCH</td>
<td>请求局部更新 URL 位置的资源，即改变该处资源的部分内容</td>
</tr>
<tr>
<td>DELETE</td>
<td>请求删除 URL 位置存储的资源</td>
</tr>
</tbody></table>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203201707788.png"></p>
<p><strong>GET 与 POST 的区别</strong>：</p>
<ul>
<li>原理区别：GET 请求获取资源，不会产生动作；POST 可能会修改服务器上的资源。因此 POST 用于修改和写入数据，GET 一般用于搜索排序和筛选之类的操作，目的是资源的获取，读取数据。</li>
<li>参数位置区别：GET 把参数包含在 URL 中，POST 通过 Request body 传递参数。所以 POST 可以发送的数据更大，GET 有 URL 长度限制；POST 能发送更多的数据类型（GET 只能发送 ASCII 字符）。</li>
<li>POST 更加安全：不会作为 URL 的一部分，不会被缓存、保存在服务器日志、以及浏览器浏览记录中。</li>
<li>POST 比 GET 慢：POST 在真正接收数据之前会先将请求头发送给服务器进行确认，然后才真正发送数据。<ul>
<li>浏览器请求 TCP 连接（第一次握手）</li>
<li>服务器答应进行 TCP 连接（第二次握手）</li>
<li>浏览器确认，并发送 POST 请求头（第三次握手，这个报文比较小，所以 HTTP 会在此时进行第一次数据发送）</li>
<li>服务器返回 100 Continue 响应</li>
<li>浏览器发送数据</li>
<li>服务器返回 200 OK 响应</li>
</ul>
</li>
</ul>
<p><strong>PATCH 与 PUT 的区别</strong>：<br>假设 URL 位置有一组数据 UserInfo，包括 UserID，UserName 等 20 个字段。<br>需求：修改 UserName，其他不变。</p>
<ul>
<li>采用 PATCH，仅向 URL 提交 UserName 的局部更新请求。</li>
<li>采用 PUT，必须将所有 20 个字段一并提交到 URL，未提交的字段将被删除。</li>
</ul>
<p>因此 PATCH 的最主要好处是：节省网络带宽。</p>
<h2 id="0x03-再探-Requests-库"><a href="#0x03-再探-Requests-库" class="headerlink" title="0x03 再探 Requests 库"></a>0x03 再探 Requests 库</h2><p>HTTP 协议方法与 Requests 库方法是一一对应的。因此有了对 HTTP 协议方法的理解之后，我们再来学习 Requests 库中的其他方法。</p>
<h3 id="Requests-库中的其他方法"><a href="#Requests-库中的其他方法" class="headerlink" title="Requests 库中的其他方法"></a>Requests 库中的其他方法</h3><h4 id="request-head-方法"><a href="#request-head-方法" class="headerlink" title="request.head() 方法"></a>request.head() 方法</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; r &#x3D; requests.head(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;)
&gt;&gt;&gt; r.headers
&#123;&#39;Date&#39;: &#39;Sat, 19 Mar 2022 18:27:08 GMT&#39;, &#39;Content-Type&#39;: &#39;application&#x2F;json&#39;, &#39;Content-Length&#39;: &#39;307&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Server&#39;: &#39;gunicorn&#x2F;19.9.0&#39;, &#39;Access-Control-Allow-Origin&#39;: &#39;*&#39;, &#39;Access-Control-Allow-Credentials&#39;: &#39;true&#39;&#125;
&gt;&gt;&gt; r.text
&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到 <code>r.text</code> 为空，因此通过 <code>head()</code> 方法可以用很少的网络流量，来获得网页的概括信息。</p>
<h4 id="requests-post-方法"><a href="#requests-post-方法" class="headerlink" title="requests.post () 方法"></a>requests.post () 方法</h4><ol>
<li>向 URL POST 一个字典，自动编码为 form。</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; payload &#x3D; &#123;&quot;key1&quot;: &quot;value1&quot;,&quot;key2&quot;: &quot;value2&quot;&#125;
&gt;&gt;&gt; r &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, data &#x3D; payload)
&gt;&gt;&gt; print(r.text)
&#123; ...
  &quot;form&quot;: &#123;
    &quot;key1&quot;: &quot;value1&quot;, 
    &quot;key2&quot;: &quot;value2&quot;
  &#125;, 
...
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可见，当向 URL POST 一个字典或者键值对的时候，会默认存储在表单 form 的字段下。</p>
<ol start="2">
<li>向 URL POST 一个字符串，自动编码为 data。</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt;r &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, data &#x3D; &#39;juanbudongle&#39;)
&gt;&gt;&gt;print(r.text)
&#123; ...
  &quot;data&quot;: &quot;juanbudongle&quot;,
  &quot;files&quot;: &#123;&#125;,
  &quot;form&quot;: &#123;&#125;
&#125;
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="requests-put-方法"><a href="#requests-put-方法" class="headerlink" title="requests.put() 方法"></a>requests.put() 方法</h4><p>与 POST 类似，只是会覆盖原 URL 位置的资源</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; payload &#x3D; &#123;&quot;key1&quot;: &quot;value1&quot;,&quot;key2&quot;: &quot;value2&quot;&#125;
&gt;&gt;&gt; r &#x3D; requests.put(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, data &#x3D; payload)
&gt;&gt;&gt; print(r.text)
&#123; ...
  &quot;form&quot;: &#123;
    &quot;key1&quot;: &quot;value1&quot;, 
    &quot;key2&quot;: &quot;value2&quot;
  &#125;, 
...
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>requests.post ()</code> 与 <code>requests.put()</code> 类似，上节末尾已经分析过两种方法的区别。</p>
<h3 id="Requests-库的基础方法"><a href="#Requests-库的基础方法" class="headerlink" title="Requests 库的基础方法"></a>Requests 库的基础方法</h3><p><code>requests.request()</code> 是 Requests 库所有方法的基础方法，因此将其单列出来。或者说，库中其他方法的实现，都是基于 <code>requests.request()</code> 方法。</p>
<h4 id="requests-request-方法"><a href="#requests-request-方法" class="headerlink" title="requests. request () 方法"></a>requests. request () 方法</h4><p><code>requests.request(method, url, **kwargs)</code></p>
<ul>
<li><code>method</code> ：请求方式，对应 GET&#x2F;PUT&#x2F;POST 等 7 种。</li>
<li><code>url</code> ：拟获取页面的 URL 链接。</li>
<li><code>**kwargs</code> : 控制访问的参数，共 13 个。</li>
</ul>
<p>7 种请求方式如下：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200326757.png" alt="|500"></p>
<p><code>**kwargs</code> 控制访问的参数，一共有 13 个，均为可选项，下面依次讲解：</p>
<ul>
<li>params：字典或字节序列，作为参数增加到 <code>url</code> 中。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200336254.png" alt="|700"></li>
<li>data：字典，字节序或文件对象，作为 Request 的内容。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200340659.png" alt="|700"><br>提交的键值对不直接存在 <code>url</code> 链接中，而是放在 Request 规定的存储位置。参考 <a href="#requests%20post%20%E6%96%B9%E6%B3%95">requests post 方法</a> 。</li>
<li>json：json 格式的数据，作为 Request 的内容。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200347742.png" alt="|700"></li>
<li>headers：字典，HTTP 定制头。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200349208.png"></li>
<li>cookies：字典或 CookieJar，Request 中的 cookie。</li>
<li>auth：元组，支持 HTTP 认证功能。</li>
<li>files：字典类型，传输文件。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200351450.png" alt="|700"></li>
<li>timeout: 设定超时时间，秒为单位。</li>
<li>proxies：字典类型，设定访问代理服务器，可以增加登录认证。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203200354523.png"><br>使用这个字段，可以有效隐藏用户爬取网页的源的 IP 地址信息，有效防止对爬虫的逆追踪。</li>
<li>allow_redirects: True&#x2F;False，默认为 True，重定向开关。</li>
<li>stream：True&#x2F;False，默认为 True，获取内容立即下载开关。</li>
<li>verify：True&#x2F;False，默认为 True，认证 SSL 证书开关。</li>
<li>cert： 本地 SSL 证书路径。</li>
</ul>
<h2 id="0x04-Requests-实战"><a href="#0x04-Requests-实战" class="headerlink" title="0x04 Requests 实战"></a>0x04 Requests 实战</h2><h3 id="Case-1-京东商品页面的爬取"><a href="#Case-1-京东商品页面的爬取" class="headerlink" title="Case 1. 京东商品页面的爬取"></a>Case 1. 京东商品页面的爬取</h3><p>对这个 <a href="https://item.jd.com/100029199502.html">手机商品页面</a> 进行爬取。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; import requests
&gt;&gt;&gt; url &#x3D; &#39;https:&#x2F;&#x2F;item.jd.com&#x2F;100029199502.html&#39;
&gt;&gt;&gt; r &#x3D; requests.get(url)
&gt;&gt;&gt; r.status_code
200
&gt;&gt;&gt; r.encoding
&#39;UTF-8&#39;
&gt;&gt;&gt; r.text
&#123;&#39;Date&#39;: &#39;Sat, 19 Mar 2022 18:27:08 GMT&#39;, &#39;Content-Type&#39;: &#39;application&#x2F;json&#39;, &#39;Content-Length&#39;: &#39;307&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Server&#39;: &#39;gunicorn&#x2F;19.9.0&#39;, &#39;Access-Control-Allow-Origin&#39;: &#39;*&#39;, &#39;Access-Control-Allow-Credentials&#39;: &#39;true&#39;&#125;
&gt;&gt;&gt; r.text
&quot;&lt;script&gt;window.location.href&#x3D;&#39;https:&#x2F;&#x2F;passport.jd.com&#x2F;new&#x2F;login.aspx?ReturnUrl&#x3D;http%3A%2F%2Fitem.jd.com%2F100029199502.html&#39;&lt;&#x2F;script&gt;&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>以上都是基本流程操作，我们发现返回的信息并不是商品页面。如果将这个返回的 URL 在浏览器中打开，会呈现一个登录页面。或者说京东阻止了这个爬虫，把我们重定向到了登陆页面。因为我们即使不登陆，在浏览器上还是可以访问的。</p>
<p>一般网页限制爬虫的方式有两种，一种是通过 Robots 协议，告知爬虫，哪些资源可以访问，哪些不可以；另一种是根据 HTTP 的 Header 中的 user-agent 来检测，判断这个 HTTP 访问是不是由爬虫发起的，对于爬虫的请求，网站是可以选择拒绝的。</p>
<p>由于 <code>Requests</code> 库返回的 <code>Response</code> 对象（也就是 <code>requests.get()</code> 返回的 <code>r</code> ），包含发出的 <code>request</code> 请求，因此我们可以查看之前对京东发出的请求到底是什么内容。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; r.request.headers  # 查看发出的头信息
&#123;&#39;User-Agent&#39;: &#39;python-requests&#x2F;2.27.1&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*&#x2F;*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;&#125;
# 可以发现，这里标明了我们是使用 &#39;python-requests&#x2F;2.27.1&#39; 发出的请求
# 也就是说我们的爬虫诚实地告诉了服务器，这个请求是由一个爬虫产生的
# 因此，如果京东进行来源审查，并且不支持这种访问的时候，我们的访问会被拒绝
# 那么此时，我们就应该模拟浏览器来进行爬虫请求
&gt;&gt;&gt; kv &#x3D; &#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;7.0&#39;&#125;  # 构造一个键值对，指定 user-agent
&gt;&gt;&gt; url &#x3D; &#39;https:&#x2F;&#x2F;item.jd.com&#x2F;100029199502.html&#39;
&gt;&gt;&gt; r &#x3D; requests.get(url, headers &#x3D; kv)  # 修改 HTTP 头中的 user-agent
&gt;&gt;&gt; r.status_code
200
&gt;&gt;&gt; r.request.headers  # 查看发出请求的头信息
&#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;7.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*&#x2F;*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;&#125;
# 可以看到 user-agent 已经修改为我们键值对中的内容
&gt;&gt;&gt; r.text[:600]  # 查看返回内容
&#39;&lt;!DOCTYPE HTML&gt;\n&lt;html lang&#x3D;&quot;zh-CN&quot;&gt;\n&lt;head&gt;\n    &lt;!-- shouji --&gt;\n    &lt;meta http-equiv&#x3D;&quot;Content-Type&quot; content&#x3D;&quot;text&#x2F;html; charset&#x3D;utf-8&quot; &#x2F;&gt;\n    &lt;title&gt;【华为Mate X2】华为 HUAWEI Mate X2 5G全网通12GB+512GB墨黑素皮款 典藏版 麒麟芯片 超感知徕卡四摄 折叠屏 华为手机【行情 报价 价格 评测】-京东&lt;&#x2F;title&gt;\n    &lt;meta name&#x3D;&quot;keywords&quot; content&#x3D;&quot;HUAWEIMate X2,华为Mate X2,华为Mate X2报价,HUAWEIMate X2报价&quot;&#x2F;&gt;\n    &lt;meta name&#x3D;&quot;description&quot; content&#x3D;&quot;【华为Mate X2】京东JD.COM提供华为Mate X2正品行货，并包括HUAWEIMate X2网购指南，以及华为Mate X2图片、Mate X2参数、Mate X2评论、Mate X2心得、Mate X2技巧等信息，网购华为Mate X2上京东,放心又轻松&quot; &#x2F;&gt;\n    &lt;meta name&#x3D;&quot;format-detection&quot; content&#x3D;&quot;telephone&#x3D;no&quot;&gt;\n    &lt;meta http-equiv&#x3D;&quot;mobile&#39;
# 正常返回商品页面<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>以上是在 Python IDLE 中的测试，下面给出全代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
url &#x3D; &#39;https:&#x2F;&#x2F;item.jd.com&#x2F;100029199502.html&#39;
try:
    kv &#x3D; &#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;7.0&#39;&#125;
    r &#x3D; requests.get(url, headers &#x3D; kv)
    r.raise_for_status()
    r.encoding &#x3D; r.apparent_encoding
    print(r.text[:1000])
except:
    print(&quot;Error&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>总结：有的时候需要修改头信息 ‘user-agent’，模拟浏览器向服务器提交 HTTP 请求。</strong></p>
<h3 id="Case-2-百度搜索结果爬取"><a href="#Case-2-百度搜索结果爬取" class="headerlink" title="Case 2. 百度搜索结果爬取"></a>Case 2. 百度搜索结果爬取</h3><p>百度的搜索关键词接口：<br><code>https://www.baidu.com/s?wd=keyword</code></p>
<p>这里要注意的是，简单的给 HTTP 头信息进行修改是没用的，需要去找一个真实的 User-Agent 修改到头信息中。</p>
<p>在浏览器中按下 F12：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203201605217.png" alt="|800"></p>
<p>随便选取一个 GET 请求，复制请求标头里的用户代理 User-Agent 信息。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; agent &#x3D; &#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko&#x2F;20100101 Firefox&#x2F;84.0&#39;&#125;
&gt;&gt;&gt; kv &#x3D; &#123;&#39;wd&#39;: &#39;Python&#39;&#125;
&gt;&gt;&gt; r &#x3D; requests.get (&#39; https:&#x2F;&#x2F;www.baidu.com&#x2F;s &#39;, params&#x3D;kv, headers&#x3D;agent)
&gt;&gt;&gt; r.request.url  # 查看发出的 HTTP 请求的 URL
&#39;https:&#x2F;&#x2F;www.baidu.com&#x2F;s?wd&#x3D;Python&#39;
# 可以看到，搜素关键词添加到了后面
&gt;&gt;&gt; r.request.headers
&#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko&#x2F;20100101 Firefox&#x2F;84.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*&#x2F;*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;&#125;
&gt;&gt;&gt; r.status_code
200
&gt;&gt;&gt; len(r.text)  # 查看返回内容的长度
786376
# 这就说明返回成功了
# 修改 User-Agent 不正确时，返回的长度只有 227，即代表被百度服务器阻止访问了<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>总结：在实际应用时，可能会涉及各种伪装技巧，反反爬也是要考虑的一方面。</strong></p>
<h3 id="Case-3-网络图片的爬取和存储"><a href="#Case-3-网络图片的爬取和存储" class="headerlink" title="Case 3. 网络图片的爬取和存储"></a>Case 3. 网络图片的爬取和存储</h3><p>在国家地理的网站上随便找 <a href="http://www.ngchina.com.cn/news/detail?newsId=47344">一幅图</a>，通过查看网页源码找到这幅图的 URL。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; import requests
&gt;&gt;&gt; path &#x3D; &quot;city.jpg&quot;  # 给图片一个存储位置，存在当前目录下，命名为 city
&gt;&gt;&gt; url &#x3D; &#39;https:&#x2F;&#x2F;ngimages.oss-cn-beijing.aliyuncs.com&#x2F;2022&#x2F;03&#x2F;16&#x2F;fabe20d0-01cc-400d-ae07-8f56516a556a.jpg&#39;  # 源码中找到的图的 URL
&gt;&gt;&gt; r &#x3D; requests.get(url)
&gt;&gt;&gt; r.status_code
200  # 成功
&gt;&gt;&gt; with open(path, &#39;wb&#39;) as f:  # 将图片以二进制形式写入文件
	    f.write(r.content)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>完整代码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests
import os

url &#x3D; &#39;https:&#x2F;&#x2F;ngimages.oss-cn-beijing.aliyuncs.com&#x2F;2022&#x2F;03&#x2F;16&#x2F;fabe20d0-01cc-400d-ae07-8f56516a556a.jpg&#39;
root &#x3D; &quot;E:&#x2F;&#x2F;pics&#x2F;&#x2F;&quot;
path &#x3D; root + url.split(&#39;&#x2F;&#39;)[-1]  # .spilt 返回⼀个将字符串以 &#x2F; 划分的列表，取 [-1]，即最后一个（或者说倒数第一个），也就是 URL 中图片的名字
try:
    kv &#x3D; &#123;&#39;user-agent&#39;: &#39;Mozilla&#x2F;5.0&#39;&#125;
    if not os.path.exists(root):  # 如果根目录不存在，则新建根目录
        os.mkdir(root)
    if not os.path.exists(path):  # 如果文件不存在，则从网上爬取并存储
        r &#x3D; requests.get(url)
        with open(path, &#39;wb&#39;) as f:
            f.write(r.content)
            print(&quot;文件保存成功&quot;)
    else:
        print(&quot;文件保存成功&quot;)
except:
    print(&quot;爬取失败&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>总结：在工程要求上，代码的可靠性与稳定性是非常重要的。因此哪怕是简单的代码，也要考虑可能出现的问题，并对这些问题做相关处理。</strong></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p> <a href="https://www.cnblogs.com/an-wen/p/11180076.html">学习 HTTP 协议</a> ⭐⭐⭐<br> <a href="http://www.dedenotes.com/html/url-fragment.html">URL 锚点、fragment 锚部分、锚点定位的九点知识 </a> ⭐⭐<br> <a href="https://blog.csdn.net/caryee89/article/details/7398088">统一资源定位符 URL</a><br> <a href="https://blog.csdn.net/Selina_lxh/article/details/121840190">URL(统一资源定位符)各部分详解</a><br> <a href="https://www.bilibili.com/video/BV1pt41137qK?p=7">Python 爬虫视频教程全集 | bilibili</a> ⭐⭐⭐<br> <a href="https://blog.csdn.net/qq_29339467/article/details/105342399">Python 爬虫教程中转站</a><br> <a href="https://blog.csdn.net/pittpakk/article/details/81218566">python3 requests 详解</a><br> <a href="https://zhuanlan.zhihu.com/p/275695831">http 请求中 get 和 post 方法的区别</a><br> <a href="https://zhuanlan.zhihu.com/p/370768951">python 爬虫-百度搜索结果爬取</a></p>
]]></content>
      <categories>
        <category>爬虫与网络编程基础</category>
      </categories>
      <tags>
        <tag>Coggle 30 Days of ML</tag>
        <tag>Spider</tag>
        <tag>Network Programming</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫与网络编程基础-Task03：BS4 基础使用</title>
    <url>/1AF2W7B/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p><a href="http://www.crummy.com/software/BeautifulSoup/">Beautiful Soup</a> 是一个可以从 HTML 或 XML 文件中提取数据的 Python 库。它能够通过你喜欢的转换器实现惯用的文档导航，查找，修改文档的方式。Beautiful Soup 会帮你节省数小时甚至数天的工作时间。</p>
<span id="more"></span>

<ul>
<li>学习资料： <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/">Beautiful Soup 4.2.0 documentation</a> </li>
<li>步骤 1：使用 requests 和 bs4 爬取 <a href="https://scikit-learn.org/stable/modules/classes.html">sklearn api 页面</a> </li>
<li>步骤 2：在 api 页面中有多少个模块？有多少个 API？如 sklearn.base.DensityMixin，其中 base 为模块，DensityMixin 为 API。</li>
<li>步骤 3：将模块名作为 key，api 作为 value 存储为字典。</li>
</ul>
<h2 id="0x01-爬取-sklearn-api-页面"><a href="#0x01-爬取-sklearn-api-页面" class="headerlink" title="0x01 爬取 sklearn api 页面"></a>0x01 爬取 <a href="https://scikit-learn.org/stable/modules/classes.html">sklearn api 页面</a></h2><h3 id="Requests-获得网页-HTML-源代码"><a href="#Requests-获得网页-HTML-源代码" class="headerlink" title="Requests 获得网页 HTML 源代码"></a>Requests 获得网页 HTML 源代码</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">import requests

r &#x3D; requests.get(&quot;https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;classes.html&quot;)
print(r.text)
demo &#x3D; r.text<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="BS4-制作-Soup"><a href="#BS4-制作-Soup" class="headerlink" title="BS4 制作 Soup"></a>BS4 制作 Soup</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">from bs4 import BeautifulSoup

soup &#x3D; BeautifulSoup(demo, &quot;html.parser&quot;)
print(soup.prettify())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>成功解析网页：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292151305.png"></p>
<p>PS： <code>.prettify()</code> 方法可以帮助我们对 HTML 格式化与编码，为 HTML 源码添加一些换行符 <code>\n</code>，来增强 HTML 的可读性。</p>
<h2 id="0x02-BS4-解析库用法详解"><a href="#0x02-BS4-解析库用法详解" class="headerlink" title="0x02 BS4 解析库用法详解"></a>0x02 BS4 解析库用法详解</h2><p><strong>Beautiful Soup 库的理解</strong><br>Beautiful Soup 库是解析、遍历、维护“标签树”的功能库。</p>
<p><strong>Beautiful Soup 类的理解</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292140235.png"></p>
<p><strong>Beautiful Soup 库解析器</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292202930.png"></p>
<p>官方推荐使用 lxml 作为解析器，因为效率更高。</p>
<h3 id="BS4-常用语法"><a href="#BS4-常用语法" class="headerlink" title="BS4 常用语法"></a>BS4 常用语法</h3><p>Beautiful Soup 将 HTML 文档转换成一个树形结构，该结构有利于快速地遍历和搜索 HTML 文档。下面使用树状结构来描述一段 HTML 文档：</p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>html</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>head</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>title</span><span class="token punctuation">></span></span>c语言中文网<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>title</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>head</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>h1</span><span class="token punctuation">></span></span>c.biancheng.net<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>h1</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>b</span><span class="token punctuation">></span></span>一个学习编程的网站<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>b</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>body</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>html</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>树状图如下：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292213495.png" alt="HTML 文档树结构图"></p>
<p>文档树中的每个节点都是 Python 对象，这些对象大致分为四类：Tag , NavigableString , BeautifulSoup , Comment 。其中使用最多的是 Tag 和 NavigableString。</p>
<ul>
<li>Tag：标签类，<strong>HTML 文档中所有的标签都可以看做 Tag 对象。</strong></li>
<li>NavigableString：字符串类，指的是标签中的文本内容，使用 text、string、strings 来获取文本内容。</li>
<li>BeautifulSoup：表示一个 HTML 文档的全部内容，您可以把它当作一个人特殊的 Tag 对象。</li>
<li>Comment：表示 HTML 文档中的注释内容以及特殊字符串，它是一个特殊的 NavigableString。（不常用）</li>
</ul>
<h4 id="1-Tag-节点"><a href="#1-Tag-节点" class="headerlink" title="1. Tag 节点"></a>1. Tag 节点</h4><p>标签（Tag）是组成 HTML 文档的基本元素。在 BS4 中，通过标签名和标签属性可以提取出想要的内容。看一组简单的示例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from bs4 import BeautifulSoup

soup &#x3D; BeautifulSoup(&#39;&lt;p class&#x3D;&quot;Web site url&quot;&gt;&lt;b&gt;c.biancheng.net&lt;&#x2F;b&gt;&lt;&#x2F;p&gt;&#39;, &#39;html.parser&#39;)

#获取整个p标签的html代码
print(soup.p)

#获取b标签
print(soup.p.b)

#获取p标签内容，使用NavigableString类中的string、text、get_text()
print(soup.p.text)

#返回一个字典，里面是属性和值
print(soup.p.attrs)

#查看返回的数据类型
print(type(soup.p))

#根据属性，获取标签的属性值，返回值为列表
print(soup.p[&#39;class&#39;])

#给class属性赋值,此时属性值由列表转换为字符串
soup.p[&#39;class&#39;]&#x3D;[&#39;Web&#39;,&#39;Site&#39;]
print(soup.p)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text">soup.p输出结果:
&lt;p class="Web site url">&lt;b>c.biancheng.net&lt;/b>&lt;/p>

soup.p.b输出结果：
&lt;b>c.biancheng.net&lt;/b>

soup.p.text输出结果：
c.biancheng.net

soup.p.attrs输出结果：
&#123;'class': ['Web', 'site', 'url']&#125;

type(soup.p)输出结果：
&lt;class 'bs4.element.Tag'>

soup.p['class']输出结果：
['Web', 'site', 'url']

class属性重新赋值：
&lt;p class="Web Site">&lt;b>c.biancheng.net&lt;/b>&lt;/p><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>小结：Beautiful Soup 类的五种基本元素</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292240002.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292242273.png"></p>
<h4 id="2-遍历节点"><a href="#2-遍历节点" class="headerlink" title="2. 遍历节点"></a>2. 遍历节点</h4><p>由于 HTML 本身是一种树形结构，所以我们有以下几种遍历方式。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203292250137.png"></p>
<p>Tag 对象提供了许多遍历 tag 节点的属性，比如 contents、children 用来遍历子节点；parent 与 parents 用来遍历父节点；而 next_sibling 与 previous_sibling 则用来遍历兄弟节点。</p>
<p><strong>2.1. 下行遍历（子节点）</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300017220.png"></p>
<p>Tag 的 <code>.contents</code> 属性可以将 tag 的子节点以列表的方式输出:</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from bs4 import BeautifulSoup

html_doc &#x3D; &quot;&quot;&quot;
&lt;html&gt;&lt;head&gt;&lt;title&gt;&quot;c语言中文网&quot;&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;
&lt;body&gt;
&lt;p class&#x3D;&quot;title&quot;&gt;&lt;b&gt;c.biancheng.net&lt;&#x2F;b&gt;&lt;&#x2F;p&gt;
&lt;p class&#x3D;&quot;website&quot;&gt;一个学习编程的网站&lt;&#x2F;p&gt;
&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;c.biancheng.net&#x2F;python&#x2F;&quot; id&#x3D;&quot;link1&quot;&gt;python教程&lt;&#x2F;a&gt;,
&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;c.biancheng.net&#x2F;c&#x2F;&quot; id&#x3D;&quot;link2&quot;&gt;c语言教程&lt;&#x2F;a&gt; and
&quot;&quot;&quot;
soup &#x3D; BeautifulSoup(html_doc, &#39;html.parser&#39;)
body_tag&#x3D;soup.body

#打印原&lt;body&gt;标签内容
print(body_tag)

#以列表的形式输出，所有子节点
print(body_tag.contents)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">&lt;body>
&lt;p class="title">&lt;b>c.biancheng.net&lt;/b>&lt;/p>
&lt;p class="website">一个学习编程的网站&lt;/p>
&lt;a href="http://c.biancheng.net/python/" id="link1">python教程&lt;/a>,
&lt;a href="http://c.biancheng.net/c/" id="link2">c语言教程&lt;/a> and
&lt;/body>

#以列表的形式输出 可以看见换行符也在内
['\n', &lt;p class="title">&lt;b>c.biancheng.net&lt;/b>&lt;/p>, '\n', &lt;p class="website">一个学习编程的网站&lt;/p>, '\n', &lt;a href=" http://c.biancheng.net/python/" id="link1">python教程&lt;/a>, '\n', &lt;a href=" http://c.biancheng.net/c/" id="link2">c语言教程&lt;/a>, '\n']<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>因为 <code>.contents</code> 返回的是一个列表，所以可以有以下操作：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">#打印列表的长度
print(len(body_tag.contents))

#打印列表中第二个元素（第一个元素是换行符）
print(body_tag.contents[1])
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">#打印列表的长度
9

#打印列表中第二个元素
&lt;p class="title">&lt;b>c.biancheng.net&lt;/b>&lt;/p><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>除了 <code>.contents</code> 之外，Tag 的 <code>.children</code> 属性会生成一个<strong>可迭代对象</strong>，可以用来遍历子节点，示例如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for child in body_tag.children:
    print(child)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>输出结果：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">#注意此处已将换行符"\n"省略
&lt;p class="title">&lt;b>c.biancheng.net&lt;/b>&lt;/p>
&lt;p class="website">一个学习编程的网站&lt;/p>
&lt;a href=" http://c.biancheng.net/python/" id="link1">python教程&lt;/a>
&lt;a href=" http://c.biancheng.net/c/" id="link2">c语言教程&lt;/a><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>.contents</code> 和 <code>.children</code> 属性仅包含 tag 的直接子节点。例如，当前的 &lt;head&gt; 标签只有一个直接子节点 &lt;title&gt;：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">head_tag.contents

&#39;&#39;&#39;输出
[&lt;title&gt;The Dormouse&#39;s story&lt;&#x2F;title&gt;]
&#39;&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>但是 &lt;title&gt; 标签也包含一个子节点:字符串 “The Dormouse’s story”，这种情况下字符串“The Dormouse’s story”也属于 &lt;head&gt; 标签的子孙节点。<code>.descendants</code> 属性可以对所有 tag 的子孙节点进行递归循环：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for child in head_tag.descendants:
    print(child)

&#39;&#39;&#39;输出
&lt;title&gt;The Dormouse&#39;s story&lt;&#x2F;title&gt;
The Dormouse&#39;s story
&#39;&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>2.2. 上行遍历（父节点）</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300020443.png"></p>
<p>打印 <code>soup.a</code> 的所有父节点：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300029975.png"></p>
<p>由于 <code>soup.a</code> 的父节点遍历会包含 <code>soup</code> 本身，但是当遍历到 <code>soup.parent</code> 时，由于 <code>soup</code> 不存在先辈节点，所以为空。因此这里要加一个判断 <code>parent</code> 是否为空。即 <code>BeautifulSoup</code> 对象的 <code>.parent</code> 是 None。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">print(soup.parent)
# None<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><strong>2.2. 平行遍历（兄弟节点）</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300034762.png"></p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300040532.png"></p>
<p><strong>小结：</strong><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300047393.jpg"></p>
<h4 id="3-基于-BS4-库的-HTML-内容查找"><a href="#3-基于-BS4-库的-HTML-内容查找" class="headerlink" title="3. 基于 BS4 库的 HTML 内容查找"></a>3. 基于 BS4 库的 HTML 内容查找</h4><p>了解 BS4 的基本元素之后，我们就需要利用 BS4 来对 HTML 做信息提取，从中找到我们需要的信息。信息提取的一般方法如下。</p>
<ul>
<li>方法一：完整解析信息的标记形式，再提取关键信息。<ul>
<li>信息标记的三种形式：<strong>XML JSON YAML</strong></li>
<li>需要标记解析器，如：bs4 库的标签树遍历</li>
<li>优点：信息解析准确</li>
<li>缺点：提取过程繁琐，速度慢</li>
</ul>
</li>
<li>方法二：无视标记形式，直接搜索关键信息<ul>
<li><strong>搜索</strong>：对信息的文本使用查找函数即可</li>
<li>优点：提取过程简洁，速度较快</li>
<li>缺点：提取结果的准确性与信息内容直接相关（也就是如果信息质量不好，提取的准确性差）</li>
</ul>
</li>
</ul>
<p><strong>3.1. find_all() 与 find()</strong><br><code>find_all()</code> 与 <code>find()</code> 是解析 HTML 文档的常用方法，它们可以在 HTML 文档中按照一定的条件（相当于过滤器）查找所需内容。<code>find()</code> 与 <code>find_all()</code> 的语法格式相似，希望大家在学习的时候，可以举一反三。</p>
<blockquote>
<p>BS4 库中定义了许多用于搜索的方法，find() 与 find_all() 是最为关键的两个方法，其余方法的参数和使用与其类似。</p>
</blockquote>
<p><strong>3.1.1 find_all()</strong><br><code>find_all()</code> 方法用来搜索当前 tag 的所有子节点，并判断这些节点是否符合过滤条件，最后以列表形式将符合条件的内容返回，语法格式如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">find_all(name, attrs, recursive, text, limit)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>参数说明：</p>
<ul>
<li>name：查找所有名字为 name 的 tag 标签，字符串对象会被自动忽略。</li>
<li>attrs：按照属性名和属性值搜索 tag 标签，**注意由于 class 是 Python 的关键字码，所以要使用 “class_”**。</li>
<li>recursive：find_all() 会搜索 tag 的所有子孙节点，设置 recursive&#x3D;False 可以只搜索 tag 的直接子节点。</li>
<li>text：用来搜文档中的字符串内容，该参数可以接受字符串 、正则表达式 、列表、True。</li>
<li>limit：由于 find_all() 会返回所有的搜索结果，这样会影响执行效率，通过 limit 参数可以限制返回结果的数量。</li>
</ul>
<p><strong>3.1.2 find_all()</strong><br><code>find()</code> 方法与 <code>find_all()</code> 类似，不同之处在于 <code>find_all()</code> 会将文档中所有符合条件的结果返回，而 <code>find()</code> 仅返回一个符合条件的结果，所以 find() 方法没有 <code>limit</code> 参数。</p>
<p>两种方法可以结合 <code>re</code> 使用，官方文档中说明的很详细，给的应用样例也很好，这里就不赘述了。<br><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/">Beautiful Soup 4.4.0 文档 — Beautiful Soup 4.2.0 documentation</a></p>
<h3 id="BS4-入门方法总结"><a href="#BS4-入门方法总结" class="headerlink" title="BS4 入门方法总结"></a>BS4 入门方法总结</h3><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300053492.png"></p>
<p>与信息提取的方法 <code>find_all</code> 和 <code>find()</code>。</p>
<h2 id="0x03-提取-sklearn-api-页面-的信息"><a href="#0x03-提取-sklearn-api-页面-的信息" class="headerlink" title="0x03 提取 sklearn api 页面 的信息"></a>0x03 提取 <a href="https://scikit-learn.org/stable/modules/classes.html">sklearn api 页面</a> 的信息</h2><p>具体任务：在 api 页面中有多少个模块？有多少个 API？如 sklearn.base.DensityMixin，其中 base 为模块，DensityMixin 为 API。</p>
<h3 id="1-查找-Module"><a href="#1-查找-Module" class="headerlink" title="1. 查找 Module"></a>1. 查找 Module</h3><p>首先查看网页的源代码。观察一下网页与源代码。<br>由于我们已知 sklearn.base 为模块，那我们先来在源代码中搜索一下。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203300231498.png"></p>
<p>可以很轻松地发现有这么一块，API Reference，结合网页发现是左边的一个索引栏（或者说目录）。检查后确认所有 Module 在这个索引栏的 <code>&lt;li&gt;</code> 列表里有依次列出。因此我们只需要从这一块代码来找 Module 就好了。</p>
<p>以第一个 Module 为例：</p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>reference internal<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>[#module-sklearn.base](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.base)<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>code</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>xref py py-mod docutils literal notranslate<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>pre<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>sklearn.base<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>code</span><span class="token punctuation">></span></span>: Base classes and utility functions
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到内容写在 <code>&lt;span&gt;</code> 这个 Tag 里，然后外面还嵌套了好几层其他的标签。<br>上面的是我手工美化的，源码如下：</p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>reference internal<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>[#module-sklearn.base](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.base)<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>code</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>xref py py-mod docutils literal notranslate<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>pre<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>sklearn.base<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>code</span><span class="token punctuation">></span></span>: Base classes and utility functions<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ul</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>reference internal<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>[#base-classes](https://scikit-learn.org/stable/modules/classes.html#base-classes)<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Base classes<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>li</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>reference internal<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>[#functions](https://scikit-learn.org/stable/modules/classes.html#functions)<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>Functions<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ul</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>li</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>如果你观察了网页源码会发现，这样一个 <code>&lt;li&gt;</code> 标签包裹的列表是第一个 Module <code>sklearn.base</code>的全部内容（在网页左侧 API Reference 这一栏中）。<strong>然后所有的 Module 又一起嵌套在一个更大的 <code>&lt;li&gt;</code> 中。</strong> 为什么这里要重点提一下，因为我直到发现这些信息，才明白我直接做搜索的时候，那些一开始想不通的事情。</p>
<p>那么很简单，直接来把 API Reference 这一栏的所有 Module 检索出来就好了。既然都嵌套在 <code>&lt;li&gt;</code> 中，在源码中再搜索一下这个标签，发现有一大部分集中在一起（API Reference 的位置），但还有零散的一些分布在其他位置。但没关系直接从这些所有的 <code>&lt;li&gt;</code> 标签中找符合我们条件的就好。</p>
<p>思路如下：先找 <code>&lt;li&gt;</code>，然后找其中嵌套着 <code>class=&quot;pre&quot;</code> 的 <code>&lt;span&gt;</code> 标签。<code>&lt;span&gt;</code> 标签的 <code>string</code> 正是我们要搜索的信息，用一个正则表达式搜索 <code>sklearn.</code> 就好咯。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 嵌套查询
# 不能直接 soup.find_all(&#39;li&#39;).find_all()，会报错
li &#x3D; soup.find_all(&#39;li&#39;)
module &#x3D; []  # 设置一个列表来存储读到的数据
# 我第一次搜素的时候发现有很多空列表值，所以加一个 if条件
# 另外一个重点是，对 class 属性设置条件时，应该用 class_ []
for i in li:
    if i.find_all(&#39;span&#39;, class_ &#x3D; &#39;pre&#39;, string &#x3D; re.compile(&#39;sklearn.&#39;)) !&#x3D; []:
        module.append(i.find_all(&#39;span&#39;, class_ &#x3D; &#39;pre&#39;, string &#x3D; re.compile(&#39;sklearn.&#39;)))

print(module[0])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>来看一下获得的 <code>module</code> 列表：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">module[0]

[&lt;span class="pre">sklearn.base&lt;/span>,
 &lt;span class="pre">sklearn.calibration&lt;/span>,
 &lt;span class="pre">sklearn.cluster&lt;/span>,
 &lt;span class="pre">sklearn.compose&lt;/span>,
 ...
 &lt;span class="pre">sklearn.svm&lt;/span>,
 &lt;span class="pre">sklearn.tree&lt;/span>,
 &lt;span class="pre">sklearn.utils&lt;/span>]

module[1]:

[&lt;span class="pre">sklearn.base&lt;/span>]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到 <code>module[0]</code> 存储了所有的 Module，<code>module[1]</code> 存储了第一个 Module。后面的也都是存储单个的 Module。</p>
<p>我感觉是因为 <code>api[0]</code> 存的是，从最大的 <code>&lt;li&gt;</code> 找的里面的所有 <code>&lt;span&gt;</code> 标签。后面的是从最大的 <code>&lt;li&gt;</code> 里面嵌套的 <code>&lt;li&gt;</code> 中的 <code>&lt;span&gt;</code> 找到的信息，因为最小级别的 <code>&lt;li&gt;</code> 里面只有一个 <code>&lt;span&gt;</code>。</p>
<p>再看一下 <code>sklearn.base</code> Module 的源码，整体关系是这样的：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">&lt;li>
	&lt;li>
		&lt;span class="pre">sklearn.base&lt;/span>
		&lt;li>&lt;/li>
		&lt;li>&lt;/li>
	&lt;/li>
	...
&lt;/li><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>理清了关系，来看一下有多少个 Module：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">print(len(api[0]))
print(len(api))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>38<br>39</p>
</blockquote>
<p>因此，有 38 个 Module。后面多的那一个，不用问是什么了吧？</p>
<p>将 Module 存入 <code>module</code> 列表：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">li &#x3D; soup.find_all(&#39;li&#39;)
module &#x3D; []

# 首先在所有&lt;li&gt;中遍历
# 然后找到我们需要的&lt;li&gt;
# 接着只用第一个最大的&lt;li&gt;，所以是 find()，只找第一个
# 最后找到这个最大&lt;li&gt;中所有的&lt;span&gt;，并返回他们的 string
# 存储后跳出循环，否则会继续遍历最大&lt;li&gt;中的其余子&lt;li&gt;标签
for i in li:
    if i.find(&#39;a&#39;, class_ &#x3D; &#39;reference internal&#39;) !&#x3D; None:
        if i.find(&#39;span&#39;, class_ &#x3D; &#39;pre&#39;, string &#x3D; re.compile(&#39;sklearn.&#39;)) !&#x3D; None:
            span &#x3D; i.find_all(&#39;span&#39;, class_ &#x3D; &#39;pre&#39;, string &#x3D; re.compile(&#39;sklearn.&#39;))
            module &#x3D; [x.string for x in span]
            break<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>这里有一个细节需要注意。<code>find()</code> 如果找不到，返回 <code>None</code>； <code>find_all()</code> 如果找不到，返回的是空列表 <code>[]</code>。也就是说前者返回一串字符，后者返回一个包含所有结果的列表。</p>
<h3 id="2-查找-API"><a href="#2-查找-API" class="headerlink" title="2. 查找 API"></a>2. 查找 API</h3><p>一样的步骤，以 <code>base</code> 的第一个 API 为例，先找一下  <code>base.BaseEstimator</code> 的位置。可以发现一个 Module 的 API 都在 <code>&lt;tbody&gt;</code> 中（一个 Module 的所有 API 在两个 <code>&lt;tbody&gt;</code> 里面。一个 <code>&lt;section id=&quot;base-classes&quot;&gt;</code>，一个 <code>&lt;section id=&quot;functions&quot;&gt;</code> ）。然后每一个 API 在其中的一个个 <code>&lt;tr&gt;</code> 里面。</p>
<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203301712851.png"></p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>tr</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>row-odd<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>reference internal<span class="token punctuation">"</span></span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>[generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator](https://scikit-learn.org****mator)<span class="token punctuation">"</span></span> <span class="token attr-name">title</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>sklearn.base.BaseEstimator<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>code</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>xref py py-obj docutils literal notranslate<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>pre<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>base.BaseEstimator<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>code</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>

	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>td</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span><span class="token punctuation">></span></span>Base class for all estimators in scikit-learn.<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>p</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>td</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>tr</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>我们要找的 API 是在 <code>&lt;tr&gt;</code> 中的第一个 <code>&lt;td&gt;</code> 的 <code>.string</code>。</p>
<p>思路：首先找到 <code>&lt;tbody&gt;</code> 获得所有 API 的信息，然后在 <code>&lt;tbody&gt;</code>  中解析 <code>&lt;tr&gt;</code> 获得每一个 API 的信息，再把 <code>&lt;tr&gt;</code> 标签中的 <code>&lt;td&gt;</code> 标签找到，把 API 的名字写入列表。通过遍历节点已经查找方法获得。</p>
<h4 id="2-1-首先试试对第一个-lt-tbody-gt-爬取："><a href="#2-1-首先试试对第一个-lt-tbody-gt-爬取：" class="headerlink" title="2.1 首先试试对第一个 &lt;tbody&gt; 爬取："></a>2.1 首先试试对第一个 <code>&lt;tbody&gt;</code> 爬取：</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203301756250.png"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import bs4

# 因为每一个标签的子标签可能存在字符串类型
# 而所有的 API 信息封装在 &lt;tr&gt; 标签中，&lt;tr&gt; 标签是标签类型
# 所以要过滤掉非标签类型的其他信息
for tr in soup.find(&#39;tbody&#39;).children:
    if isinstance(tr, bs4.element.Tag):  # 检测 tr 是否为标签类型
        tds &#x3D; tr(&#39;td&#39;)
        print(tds[0].string)  # 打印第一个 &lt;td&gt;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text"># 输出了第一个 &lt;tbody> 里面的 API
base.BaseEstimator
base.BiclusterMixin
base.ClassifierMixin
base.ClusterMixin
base.DensityMixin
base.RegressorMixin
base.TransformerMixin
feature_selection.SelectorMixin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="2-2-爬取所有-lt-tbody-gt"><a href="#2-2-爬取所有-lt-tbody-gt" class="headerlink" title="2.2 爬取所有 &lt;tbody&gt;"></a>2.2 爬取所有 <code>&lt;tbody&gt;</code></h4><p><strong>正解</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">api &#x3D; []

for tbody in soup.find_all(&#39;tbody&#39;):  # 找到所有 &lt;tbody&gt;
    for tr in tbody.find_all(&#39;tr&#39;):  # 对每一个 &lt;tbody&gt; 的子孙节点遍历
        if isinstance(tr, bs4.element.Tag):  # 检测 tr 是否为标签类型
            tds &#x3D; tr(&#39;td&#39;)
            api.append(tds[0](&#39;span&#39;)[0].string)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>代码中的最后一步是，取 <code>&lt;tr&gt;</code> 中的第一个 <code>&lt;td&gt;</code>，然后找 <code>&lt;td&gt;</code> 中的第一个 <code>&lt;span&gt;</code> 并且读取 <code>.string</code>。</p>
<p><strong>思考</strong><br>正解是经过尝试，发现如果直接用之前的 <code>tds[0].string</code>，会导致一些 API 读不到，所以取到更精确的那一级 <code>&lt;span&gt;</code> 标签。还没想明白为什么。情况如下（不正确的读取）：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">api &#x3D; []

for tbody in soup.find_all(&#39;tbody&#39;):  # 找到所有 &lt;tbody&gt;
    for tr in tbody.find_all(&#39;tr&#39;):  # 对每一个 &lt;tbody&gt; 的子孙节点遍历
        if isinstance(tr, bs4.element.Tag):  # 检测 tr 是否为标签类型
            tds &#x3D; tr(&#39;td&#39;)  # 查找所有 &lt;td&gt;
			api.append(tds[0]) # 只保存第一个 &lt;td&gt;
            print(tds[0].string)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text"># 输出如下（省略了一些）
...
base.RegressorMixin
base.TransformerMixin
feature_selection.SelectorMixin
None
None
None
None
...
experimental.enable_iterative_imputer
experimental.enable_halving_search_cv
None
None
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>长度跟正解是一样的，只是有的 <code>.string</code> 读不出来。</p>
<p>再对存为列表的所有 <code>&lt;td&gt;</code> 研究一下：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203302141074.png"></p>
<p>这里是从中选了两个 <code>&lt;td&gt;</code> 标签的内容。</p>
<p>第一个能读出来，第二个读不出来：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203302148928.png"></p>
<p>进一步精细搜索：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203302146810.png"></p>
<p>原因未知。我猜可能是因为那些读不出来的是解码器识别不到，看看源码：</p>
<pre class="line-numbers language-text" data-language="text"><code class="language-text"># api[50]
&lt;td>&lt;p>&lt;a class="reference internal" href="generated/sklearn.covariance.OAS.html#sklearn.covariance.OAS" title="sklearn.covariance.OAS">&lt;code class="xref py py-obj docutils literal notranslate">&lt;span class="pre">covariance.OAS&lt;/span>&lt;/code>&lt;/a>(*[, store_precision, ...])&lt;/p>&lt;/td><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><code>&lt;td&gt;</code> 中确实没有 <code>string</code>（不知道 <code>&lt;/p&gt;</code> 前面那个算不算），包含 <code>string</code> 的是其中的 <code>&lt;span&gt;</code> 标签，可能是这个原因吧。</p>
<p>后来经过群友的努力，大概是这个原因：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">Type:        property
String form: &lt;property object at 0x000002036EFD32C8&gt;
Source:     
# text.string.fget 
@property 
def string(self):     
	&quot;&quot;&quot;Convenience property to get the single string within this
    PageElement.

    TODO It might make sense to have NavigableString.string return
    itself.

    :return: If this element has a single string child, return
     value is that string. If this element has one child tag,
     return value is the &#39;string&#39; attribute of the child tag,
     recursively. If this element is itself a string, has no
     children, or has more than one child, return value is None.
    &quot;&quot;&quot;     
	if len(self.contents) !&#x3D; 1:
		return None     
	child &#x3D; self.contents[0]     
	if isinstance(child, NavigableString):
		return child     
	return child.string  
# text.string.fset 
@string.setter 
def string(self, string):
	&quot;&quot;&quot;Replace this PageElement&#39;s contents with &#96;string&#96;.&quot;&quot;&quot;
	self.clear()
	self.append(string.__class__(string))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><code>return</code> 那里解释说到：<code>If this element is itself a string, has no children, or has more than one child, return value is None.</code> 所以大概是还有一个 <code>(*[, store_precision, ...])</code> 算是 <code>have more than one child</code> 吧。</p>
<p><code>api[50].text</code> 的输出为 <code>&#39;covariance.OAS(*[,\xa0store_precision,\xa0...])&#39;</code>。</p>
<h2 id="0x04-存储爬取的信息"><a href="#0x04-存储爬取的信息" class="headerlink" title="0x04 存储爬取的信息"></a>0x04 存储爬取的信息</h2><p>将爬取到的模块名作为 Key，API 作为 Value 存储为字典</p>
<p>其实跟上面的两个任务大同小异，关键是怎么搜索到每个 Moudule 对应的 API。</p>
<p>因为每个 Module 里面有多少 <code>&lt;tbody&gt;</code> 是不固定的，所以要找到每个 Module 外面包围的那个标签，不能像上一节一样只对 <code>&lt;tbody&gt;</code> 做处理。</p>
<p>看了源码可以发现每个 Module 对应了一个 <code>&lt;section&gt;</code> 标签，并且里面的 <code>id</code> 描述了这一个 <code>&lt;section&gt;</code>。</p>
<pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>section</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>module-sklearn.base<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>先对第一个 Module 试试：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sec &#x3D; soup.find(&#39;section&#39;, id &#x3D; &#39;module-sklearn.base&#39;)
api &#x3D; []

for tbody in sec.find_all(&#39;tbody&#39;):  # 找到所有 &lt;tbody&gt;
    for tr in tbody.find_all(&#39;tr&#39;):  # 对每一个 &lt;tbody&gt; 的子孙节点遍历
        if isinstance(tr, bs4.element.Tag):  # 检测 tr 是否为标签类型
            tds &#x3D; tr(&#39;td&#39;)
            api.append(tds[0](&#39;span&#39;)[0].string)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text"># api
['base.BaseEstimator',
 'base.BiclusterMixin',
 'base.ClassifierMixin',
 'base.ClusterMixin',
 'base.DensityMixin',
 'base.RegressorMixin',
 'base.TransformerMixin',
 'feature_selection.SelectorMixin',
 'base.clone',
 'base.is_classifier',
 'base.is_regressor',
 'config_context',
 'get_config',
 'set_config',
 'show_versions']<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>使用我们之前存储的 <code>module</code> 列表，推广到所有的 <code>&lt;section&gt;</code> ：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def api_find(sec):
    for tbody in sec.find_all(&#39;tbody&#39;):  # 找到所有 &lt;tbody&gt;
        for tr in tbody.find_all(&#39;tr&#39;):  # 对每一个 &lt;tbody&gt; 的子孙节点遍历
            if isinstance(tr, bs4.element.Tag):  # 检测 tr 是否为标签类型
                tds &#x3D; tr(&#39;td&#39;)
                print(tds[0](&#39;span&#39;)[0].string)
    print(&#39;------------------&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">for mo in module:
    print(mo)
    print(&#39;------------------&#39;)
    sec &#x3D; soup.find(&#39;section&#39;, id &#x3D; &#39;module-&#39;+mo)
    api_find(sec)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text"># 输出
sklearn.base
------------------
base.BaseEstimator
...
show_versions
------------------
sklearn.calibration
------------------
calibration.CalibratedClassifierCV
calibration.calibration_curve
------------------
sklearn.cluster
------------------
cluster.AffinityPropagation
cluster.AgglomerativeClustering
...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>存为字典</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">def api_find(sec, dic, mo:str):
    dic[mo] &#x3D; []  # 创建字典当前 key 的 value list
    if sec:  # 添加这个是因为有的 sec 为 None，不加会报错，不知原因
        for tbody in sec.find_all(&#39;tbody&#39;):
            for tr in tbody.find_all(&#39;tr&#39;):
                if isinstance(tr, bs4.element.Tag):
                    tds &#x3D; tr(&#39;td&#39;)
                    dic[mo].append(tds[0](&#39;span&#39;)[0].string)

sklearn_dic &#x3D; &#123;&#125;

for mo in module:
    sec &#x3D; soup.find(&#39;section&#39;, id &#x3D; &#39;module-&#39; + mo)
    api_find(sec, sklearn_dic, mo)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-text" data-language="text"><code class="language-text"># sklearn_dic
&#123;'sklearn.base': ['base.BaseEstimator',
  'base.BiclusterMixin',
  ...
  'show_versions'],
 'sklearn.calibration': ['calibration.CalibratedClassifierCV',
  'calibration.calibration_curve'],
 'sklearn.cluster': ['cluster.AffinityPropagation',
  'cluster.AgglomerativeClustering',
  'cluster.Birch',
  ...
  'cluster.spectral_clustering',
  'cluster.ward_tree'],
 'sklearn.compose': ['compose.ColumnTransformer',
  'compose.TransformedTargetRegressor',
  'compose.make_column_transformer',
  'compose.make_column_selector'],
 'sklearn.covariance': ['covariance.EmpiricalCovariance',
  'covariance.EllipticEnvelope',
  'covariance.GraphicalLasso',
  ...&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>也可以用 <code>from collections import defaultdict</code>，免得创建空列表了。</p>
<h2 id="0x05-Conclusion"><a href="#0x05-Conclusion" class="headerlink" title="0x05 Conclusion"></a>0x05 Conclusion</h2><p>终于写完这一章了，感觉写的快要了老命了。后面的内容，如果有总结的很好的基础知识，考虑是不是不用重复造轮子。但写一遍好像又会帮我掌握的更好一些。基础知识这块尽量还是写的更加精简，把自己的思考写出来就好。</p>
<p>这章写完体会就是，重点还是在动手，实战过程中会遇到各种问题，可以引发更多的思考，或者对知识查缺补漏。</p>
<p>通过学习，掌握了 BS4 的基本用法。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p> <a href="http://c.biancheng.net/python_spider/bs4.html">Python BS4 解析库用法详解</a><br> <a href="https://www.bilibili.com/video/BV1pt41137qK?p=23&spm_id_from=pageDriver">Python 爬虫视频教程全集（62P）| bilibili</a><br> <a href="https://blog.csdn.net/learner_syj/article/details/120590574">BeautifulSoup 利用 find_all() 多级标签索引和获取标签中的属性内容</a><br> <a href="https://zhuanlan.zhihu.com/p/344114093">Python 中的 x for y in z for x in y语法详解</a><br> <a href="https://blog.csdn.net/weixin_44912159/article/details/108457413">python在字典中创建一键多值的几种方法</a><br> <a href="https://zhuanlan.zhihu.com/p/349471029">Python 中3种创建字典数据的方法</a><br> <a href="http://c.biancheng.net/view/2212.html">Python字典及基本操作（超级详细）</a></p>
]]></content>
      <categories>
        <category>爬虫与网络编程基础</category>
      </categories>
      <tags>
        <tag>Coggle 30 Days of ML</tag>
        <tag>Spider</tag>
      </tags>
  </entry>
  <entry>
    <title>搭建Hexo博客&amp;在GitHub+Gitee双重部署</title>
    <url>/2RYYQJ0/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>大致记录了如何快速搭建一个简单的个人博客，并且部署到 GitHub 与 Gitee。中间也碰到了一些坑，记录一下，帮助后面可能会碰到这些问题的同学。</p>
<span id="more"></span>

<h2 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1. 环境准备"></a>1. 环境准备</h2><p>这里使用的是 WSL2 的 Ubuntu18.04 系统<br>首先设置代理，使得可以使用 windows 的 clash 代理。详见 <a href="wsl2%20%E5%AD%90%E7%B3%BB%E7%BB%9F%E4%BB%A3%E7%90%86.md">wsl2 子系统代理</a></p>
<h3 id="安装-node-js"><a href="#安装-node-js" class="headerlink" title="- 安装 node.js"></a>- 安装 node.js</h3><p><a href="https://docs.microsoft.com/zh-cn/windows/dev-environment/javascript/nodejs-on-wsl">在 WSL 2 上设置 Node.js | Microsoft Docs</a><br><a href="https://hexo.io/zh-cn/docs/#%E5%AE%89%E8%A3%85-Hexo">文档 | Hexo</a><br><a href="https://dkvirus.gitbooks.io/-npm/content/di-sanzhang-npm-chuang-jian-xiang-mu/31-npm-init-shi-yong.html">3.1 npm init 使用 · 通俗易懂的 npm 入门教程</a></p>
<p>我使用的是微软子系统官方文档里的方法安装 <code>nvm</code>，照着做就行。</p>
<h3 id="安装-git"><a href="#安装-git" class="headerlink" title="- 安装 git"></a>- 安装 git</h3><p><a href="https://git-scm.com/downloads">Git - Downloads</a></p>
<h3 id="安装-Hexo"><a href="#安装-Hexo" class="headerlink" title="- 安装 Hexo"></a>- 安装 Hexo</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">npm install -g hexo-cli

# 验证安装成功
hexo -v<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Tips："><a href="#Tips：" class="headerlink" title="Tips："></a>Tips：</h4><p>这里有个坑，<code>nvm</code> 是用来管理 <code>node.js</code> 版本的。但是当我用 <code>nvm</code> 下载了另一个更新版本的 <code>node.js</code> 并且切换之后，<code>hexo</code> 这个命令就找不到了（应该是因为当时 <code>hexo</code> 安装在旧版的 <code>node.js</code> ），切回之前使用的版本才找得到。所以安装后尽量就不要换 <code>node.js</code> 的版本了。</p>
<p>比如我安装了一个新版的，之前有一个老版的，如下：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">➜  inno:[&#x2F;mnt&#x2F;w&#x2F;github_blog] nvm ls
-&gt;     v16.13.2
       v16.14.0
default -&gt; v16.13.2
iojs -&gt; N&#x2F;A (default)
unstable -&gt; N&#x2F;A (default)
node -&gt; stable (-&gt; v16.14.0) (default)
stable -&gt; 16.14 (-&gt; v16.14.0) (default)
lts&#x2F;* -&gt; lts&#x2F;gallium (-&gt; v16.14.0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>安装新版之后，每次默认启动的都是新版的，所以 <code>hexo</code> 是找不到的，要切回之前版本：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">nvm use v16.13.2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>切回后可以使用。</p>
<p>为了避免每次都要切换  <code>node.js</code>，更改默认版本：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">nvm alias default v16.13.2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h2 id="2-搭建仓库"><a href="#2-搭建仓库" class="headerlink" title="2. 搭建仓库"></a>2. 搭建仓库</h2><blockquote>
<p>2022.11.15 后记<br>最近换了 Mac 发现 SSH 的添加与 Windows 不太一样，多了一步添加钥匙链。<br>因此所有生成 SSH Keys 参考官方文档 <a href="https://docs.github.com/cn/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent">Generating a new SSH key and adding it to the ssh-agent - GitHub Docs</a><br>以下内容不再确保实效性。</p>
</blockquote>
<h3 id="生成-SSH-Keys"><a href="#生成-SSH-Keys" class="headerlink" title="- 生成 SSH Keys"></a>- 生成 SSH Keys</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ssh-keygen -t rsa -C &quot;你的邮箱账号&quot; # 不带双引号<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>全部默认，四次回车，显示以下则建立成功。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Generating public&#x2F;private rsa key pair.
Enter file in which to save the key (&#x2F;home&#x2F;inno&#x2F;.ssh&#x2F;id_rsa):
Created directory &#39;&#x2F;home&#x2F;inno&#x2F;.ssh&#39;.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in &#x2F;home&#x2F;inno&#x2F;.ssh&#x2F;id_rsa.
Your public key has been saved in &#x2F;home&#x2F;inno&#x2F;.ssh&#x2F;id_rsa.pub.
The key fingerprint is:
SHA256:llz0nPfP************Ua0&#x2F;as4********I9g4 
The key&#39;s randomart image is:
+---[RSA 2048]----+
|         .&#x3D;      |
|         *****   |
|        &#x3D; * B .  |
|       ***** + . |
|        E *.o . o|
|       . * *o. +o|
|          O. +o o|
|         *****+ .|
|        ..S**+.o |
+----[SHA256]-----+<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>查看 ssh 密钥并且复制到 Github：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd ~&#x2F;.ssh

ll
# 目录文件如下
	drwx------ 2 inno inno 4096 Jan 24 16:44 .
	drwxr-xr-x 9 inno inno 4096 Jan 24 16:49 ..
	-rw------- 1 inno inno 1675 Jan 24 16:44 id_rsa
	-rw-r--r-- 1 inno inno  399 Jan 24 16:44 id_rsa.pub

# vim 打开 id_rsa.pub 文件
vim  id_rsa.pub

# 全选复制 然后粘贴到 GitHub 的 [SSH keys](https:&#x2F;&#x2F;github.com&#x2F;settings&#x2F;keys) &#x2F; Add new<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>检查 ssh 是否绑定成功：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ssh -T git@github.com<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>Success：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">The authenticity of host &#39;github.com (20.205.243.166)&#39; can&#39;t be established.
ECDSA key fingerprint is SHA256:p2QAM********trVc98&#x2F;R1*****3&#x2F;L*******M.
Are you sure you want to continue connecting (yes&#x2F;no)? yes

Warning: Permanently added &#39;github.com,20.205.243.166&#39; (ECDSA) to the list of known hosts.
Hi 1nnoh! You&#39;ve successfully authenticated, but GitHub does not provide shell access.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="新建仓库"><a href="#新建仓库" class="headerlink" title="- 新建仓库"></a>- 新建仓库</h3><h4 id="Github"><a href="#Github" class="headerlink" title="- Github"></a>- Github</h4><p>在 GitHub 新建一个仓库（public），命名如下：</p>
<ul>
<li>我的仓库地址是: <code>1nnoh/1nnoh.github.io</code></li>
<li>仓库命名规则为 前面是用户名（GitHub用户名，点击右上角头像即可看到: signed as xxx），后面为 <code>.github.io</code></li>
</ul>
<p>更详细的内容可以看 <a href="#Reference">Reference</a> 里初级建站那个视频。</p>
<h4 id="Gitee"><a href="#Gitee" class="headerlink" title="- Gitee"></a>- Gitee</h4><p>在 Gitee 新建仓库（开源或私有都可以），命名如下：</p>
<ul>
<li>仓库名称：可以随便填，比如 <code>blog</code></li>
<li>仓库路径：填入用户名，跟 GitHub 一样，只是后面不用加 <code>.github.io</code>。至于为什么这样设置，可以自己试试其他的路径，然后看看 服务选项中 Gitee Pages 服务 生成的网站地址的区别。</li>
<li>开启 Gitee Pages 服务：进入仓库选择 服务，选择 Gitee Pages。第一次使用需要身份验证，一般半天可以通过人工审核。选取 强制使用 HTTPS，然后生成网站。</li>
<li>Tips：Github 的 Pages 服务可以自动更新，但Gitee 的 Pages 服务不能自动更新（开启的话需要付费）。所以每次从本地部署网站到线上之后，需要在 Gitee Pages 服务 中更新网站。</li>
</ul>
<h2 id="3-本地生成博客内容"><a href="#3-本地生成博客内容" class="headerlink" title="3. 本地生成博客内容"></a>3. 本地生成博客内容</h2><p>进入想要生成博客内容的目录，然后生成：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># 初始化 hexo 博客
mkdir hexo-blog
hexo init

# Output
	INFO  Cloning hexo-starter https:&#x2F;&#x2F;github.com&#x2F;hexojs&#x2F;hexo-starter.git
	INFO  Install dependencies
	INFO  Start blogging with Hexo!<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>随后本地生成 hexo 界面：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo s # hexo server 启动本地服务器

# Output 点击链接可以进入
# INFO  Validating config
# INFO  Start processing
# INFO  Hexo is running at http:&#x2F;&#x2F;localhost:4000&#x2F; . Press Ctrl+C to stop.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>Press Ctrl+C 点击生成的本地地址即可访问初步建好的 Hexo博客。或者将 <code>http://localhost:4000/</code> 复制到浏览器地址栏打开。</p>
<h2 id="4-发布到-GitHub-与-Gitee"><a href="#4-发布到-GitHub-与-Gitee" class="headerlink" title="4. 发布到 GitHub 与 Gitee"></a>4. 发布到 GitHub 与 Gitee</h2><h3 id="修改-config-yml"><a href="#修改-config-yml" class="headerlink" title="- 修改  _config.yml"></a>- 修改  _config.yml</h3><p>进入上节生成博客的目录，打开 _config.yml  文件（或者在上节中生成博客的目录中找到该文件，用记事本或 VS Code 等代码编辑器打开）：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">vim _config.yml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>修改文档末尾的 deploy 设置：</p>
<pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"># 只配置 Github
# Deployment
# Docs: https:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;one-command-deployment
deploy:
  type: git
  repository: https:&#x2F;&#x2F;github.com&#x2F;1nnoh&#x2F;1nnoh.github.io.git
  branch: main # GitHub 的分支改成 main 了， Gitee 是 master

# Github 与 Gitee 一起部署
deploy:
 type: git
 repository:
	 gitee: git@gitee.com:innoh&#x2F;innoh.git,master
	 github: https:&#x2F;&#x2F;github.com&#x2F;1nnoh&#x2F;1nnoh.github.io.git,main<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Tips：</p>
<ul>
<li>repository 是 GitHub 上建立的博客仓库</li>
<li>进入仓库后，点击 Code 即可看到 https 地址（就是 clone 的地方）。</li>
</ul>
<p>更改完成后保存。</p>
<h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="- 安装依赖"></a>- 安装依赖</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">npm install hexo-deployer-git --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>部署：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># hexo generate
# hexo deploy 部署到服务器
hexo clean &amp;&amp; hexo g &amp;&amp; hexo d

# 输入 GitHub用户名 与 token（是用户名，不是邮箱；是token，不是登录密码！）<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>重点！！！：输入密码的时候，去 GitHub 生成 token，把 token 作为部署时登录密码</strong></p>
<p>&#x3D;&#x3D;<strong>以后每次推送的时候，如果需要输入密码，记得也是输入令牌！！！</strong>&#x3D;&#x3D;</p>
<p>进入 GitHub 的 Settings &#x2F; Developer settings: 选择 Personal access tokens</p>
<p>生成 tokens，权限全选。生成令牌后复制，并粘贴输入密码。</p>
<p><strong>token 备份</strong></p>
<pre class="line-numbers language-none"><code class="language-none">ghp_DIvVJ*******************83i<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<p>&#x3D;&#x3D;<strong>这个 token令牌 是有一个有效期的，默认为 30 天，所以每三十天记得更新一下令牌权限，否则推送的时候会出现错误！！！另外，每次更新之后，令牌也会变，记得重新复制保存！</strong>&#x3D;&#x3D;</p>
<h3 id="检验发布"><a href="#检验发布" class="headerlink" title="- 检验发布"></a>- 检验发布</h3><p>到博客仓库 1nnoh &#x2F; 1nnoh.github.io 进入 Settings。</p>
<p>最下面有一个 GitHub Pages：Pages settings now has its own dedicated tab! Check it out here!</p>
<p>点击即可进入博客网站。</p>
<p>Gitee 则是从 Gitee Pages 服务 中生成的网站地址进入。</p>
<h2 id="5-修改主题"><a href="#5-修改主题" class="headerlink" title="5. 修改主题"></a>5. 修改主题</h2><p>我使用的是一款简约整洁的主题：</p>
<ul>
<li><a href="https://github.com/next-theme/hexo-theme-next">GitHub - next-theme&#x2F;hexo-theme-next: 🎉 Elegant and powerful theme for Hexo.</a></li>
</ul>
<p>还有其他比较优秀的主题比如：</p>
<ul>
<li><a href="https://github.com/blinkfox/hexo-theme-matery">GitHub - blinkfox&#x2F;hexo-theme-matery: A beautiful hexo blog theme with material design and responsive design.一个基于材料设计和响应式设计而成的全面、美观的Hexo主题。国内访问：http://blinkfox.com</a></li>
<li><a href="https://github.com/fluid-dev/hexo-theme-fluid">GitHub - fluid-dev&#x2F;hexo-theme-fluid: 一款 Material Design 风格的 Hexo 主题 &#x2F; An elegant Material-Design theme for Hexo</a></li>
<li><a href="https://github.com/jerryc127/hexo-theme-butterfly">GitHub - jerryc127&#x2F;hexo-theme-butterfly: 🦋 A Hexo Theme: Butterfly</a></li>
</ul>
<p>后面这三款会有更加丰富的一些外观设计。</p>
<h2 id="6-后记"><a href="#6-后记" class="headerlink" title="6. 后记"></a>6. 后记</h2><p>最近两天打算在 Gitee 和 Github 同时推送，但 Github 一直推送不成功，还以为是 <code>_config.yml</code> 里 deploy 那一块儿的格式问题。</p>
<p>报错：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">fatal: Authentication failed for &#39;https:&#x2F;&#x2F;github.com&#x2F;1nnoh&#x2F;1nnoh.github.io.git&#x2F;&#39;
FATAL &#123;
  err: Error: Spawn failed
      at ChildProcess.&lt;anonymous&gt; (&#x2F;mnt&#x2F;w&#x2F;github_blog&#x2F;node_modules&#x2F;hexo-util&#x2F;lib&#x2F;spawn.js:51:21)
      at ChildProcess.emit (node:events:390:28)
      at Process.ChildProcess._handle.onexit (node:internal&#x2F;child_process:290:12) &#123;
    code: 128
  &#125;
&#125; Something&#39;s wrong. Maybe you can find the solution here: %s https:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;troubleshooting.html<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>找了两天原因，最后才发现，推送的时候输入的是 GitHub 的用户名与 token，而 token 默认是只有 30 天的有效期，过期了之后要重新生成，并且 token 也会变，需要重新复制保存。</p>
<p>当时美化 Next 主题也是花了很多时间，因为 Hexo 和 Next 经历好多版本的更新，很多老一点的教程都是不适用的。调各种细节，增加一些统计或者评论的模块，都还蛮繁琐的。我就不重复造轮子了，在 References 里贴出一些我参考的教程。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>初级建站：<a href="https://www.bilibili.com/video/BV1mU4y1j72n?p=4">【2021最新版】保姆级Hexo+github搭建个人博客_哔哩哔哩_bilibili</a></p>
<p>后续更改主题可参考：<a href="https://voyage-li.github.io/2021/12/26/experience/">建站经历 | Twilight</a> <a href="https://www.bilibili.com/read/cv4499195/#:~:text=Hexo%E4%B8%BB%E9%A2%98%E6%8E%A8%E8%8D%90%201%20%E3%80%81%20Butterfly%202%20%E3%80%81NexT%203%20%E3%80%81,5%20%E3%80%81Material%20X%206%20%E3%80%81%20Pure%207%20%E3%80%81Icarus">Hexo主题推荐 - 哔哩哔哩</a></p>
<p>查找适合主题：<a href="https://hexo.io/themes/">Themes | Hexo</a></p>
<p>Hexo 官网：<a href="https://hexo.io/zh-cn/docs/#%E5%AE%89%E8%A3%85-Hexo">文档 | Hexo</a></p>
<p>主题美化：</p>
<p><a href="https://theme-next.js.org/docs/theme-settings/">https://theme-next.js.org/docs/theme-settings/</a> </p>
<p><a href="https://www.techgrow.cn/posts/755ff30d.html">https://www.techgrow.cn/posts/755ff30d.html</a></p>
<p><a href="https://iitii.github.io/2021/05/28/1/">https://iitii.github.io/2021/05/28/1/</a></p>
<p><a href="https://happyseashell.gitee.io/2021/09/26/hexo3/#%E4%B8%BB%E9%A2%98%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE">https://happyseashell.gitee.io/2021/09/26/hexo3/#%E4%B8%BB%E9%A2%98%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Gitee</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里灵杰_Task01_环境配置与实践数据下载</title>
    <url>/25S7X1E/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><ul>
<li>任务内容：<ul>
<li>从比赛官网下载数据集，并使用 Python 读取数据</li>
<li>使用 <code>jieba</code> 对文本进行分词</li>
<li>使用 <code>TFIDF</code> 对文本进行编码</li>
<li>思考如何使用 TFIDF 计算文本相似度？</li>
</ul>
</li>
<li>学习资料： <a href="https://coggle.club/blog/tianchi-open-search">https://coggle.club/blog/tianchi-open-search</a><span id="more"></span></li>
</ul>
<h2 id="0x01-读取数据"><a href="#0x01-读取数据" class="headerlink" title="0x01 读取数据"></a>0x01 读取数据</h2><p>这里只用 <code>train.query.txt</code> 中的前十条数据。同整个数据集的数据处理同理。</p>
<p>首先导入需要用到的包：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import pandas as pd 
import jieba  # 分词
from gensim import corpora, models, similarities  # TF-IDF<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>读取数据：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">with open(&#39;data&#x2F;train.query.txt&#39;, encoding&#x3D;&quot;utf-8&quot;) as f:
    train_querys &#x3D; f.readlines()

train_query &#x3D; &#123;&#125;
for line in train_querys:
    line &#x3D; line.strip().split(&#39;\t&#39;)
    query_id &#x3D; line[0]
    query &#x3D; line[1]
    train_query[int(query_id)] &#x3D; query<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>取字典 <code>train_query</code> 的前十条数据：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 字典切片
def dict_slice(adict, start, end):
    keys &#x3D; adict.keys()
    dict_slice &#x3D; &#123;&#125;
    for k in list(keys)[start:end]:
        dict_slice[k] &#x3D; adict[k]
    return dict_slice

wordListTop10 &#x3D; dict_slice (train_query, 0, 10)
wordListTop10<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>{1: ‘美赞臣亲舒一段’,<br>2: ‘慱朗手动料理机’,<br>3: ‘電力貓’,<br>4: ‘掏夹缝工具’,<br>5: ‘飞推 vip’,<br>6: ‘多功能托地把’,<br>7: ‘充气浮力袖’,<br>8: ‘盒马花胶鸡汤锅’,<br>9: ‘塞塞乐’,<br>10: ‘广汽传祺 gs5 挡风遮雨条子’}</p>
</blockquote>
<h2 id="0x02-使用-jieba-对文本进行分词"><a href="#0x02-使用-jieba-对文本进行分词" class="headerlink" title="0x02 使用 jieba 对文本进行分词"></a>0x02 使用 <code>jieba</code> 对文本进行分词</h2><p>使用 <code>jieba</code> 中的搜索引擎模式对文本分词。关于 <code>jieba</code> 的更多信息可以参考 <a href="https://github.com/fxsjy/jieba">GitHub - fxsjy&#x2F;jieba: 结巴中文分词</a> 。</p>
<p>有三种分词模式，这里使用默认的精确模式。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">seg_list &#x3D; []
for i in range(1,11):
    seg &#x3D; list(jieba.cut(wordListTop10[i],HMM&#x3D;True))
    seg_list.append(seg)  # 把前10个数据的所有分词放进一个列表中
seg_list<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[[&#39;美赞臣&#39;, &#39;亲舒&#39;, &#39;一段&#39;], [&#39;慱&#39;, &#39;朗&#39;, &#39;手动&#39;, &#39;料理&#39;, &#39;机&#39;], [&#39;電力貓&#39;], [&#39;掏&#39;, &#39;夹缝&#39;, &#39;工具&#39;], [&#39;飞&#39;, &#39;推&#39;, &#39;vip&#39;], [&#39;多功能&#39;, &#39;托地&#39;, &#39;把&#39;], [&#39;充气&#39;, &#39;浮力&#39;, &#39;袖&#39;], [&#39;盒马花胶&#39;, &#39;鸡汤&#39;, &#39;锅&#39;], [&#39;塞塞&#39;, &#39;乐&#39;], [&#39;广汽传祺&#39;, &#39;gs5&#39;, &#39;挡风遮雨&#39;, &#39;条子&#39;]]</code></p>
</blockquote>
<h2 id="0x03-使用-TF-IDF-对文本进行编码"><a href="#0x03-使用-TF-IDF-对文本进行编码" class="headerlink" title="0x03 使用 TF-IDF 对文本进行编码"></a>0x03 使用 <code>TF-IDF</code> 对文本进行编码</h2><h3 id="什么是-TF-IDF-检索"><a href="#什么是-TF-IDF-检索" class="headerlink" title="什么是 TF-IDF 检索"></a>什么是 <code>TF-IDF</code> 检索</h3><blockquote>
<p>可以查看 <a href="https://1nnoh.github.io/280EQA3/">矢量语义与嵌入之 TF-IDF 检索</a> 进行学习。</p>
</blockquote>
<h3 id="使用-TF-IDF-对我们的数据集编码"><a href="#使用-TF-IDF-对我们的数据集编码" class="headerlink" title="使用 TF-IDF 对我们的数据集编码"></a>使用 <code>TF-IDF</code> 对我们的数据集编码</h3><p><a href="https://radimrehurek.com/gensim/models/tfidfmodel.html">models.tfidfmodel – TF-IDF model — gensim</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 制作字典
dictionary &#x3D; corpora.Dictionary(seg_list)

# 可以通过 token2id 得到特征数字
print(dictionary.token2id)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>{‘一段’: 0, ‘亲舒’: 1, ‘美赞臣’: 2, ‘慱’: 3, ‘手动’: 4, ‘料理’: 5, ‘朗’: 6, ‘机’: 7, ‘電力貓’: 8, ‘夹缝’: 9, ‘工具’: 10, ‘掏’: 11, ‘vip’: 12, ‘推’: 13, ‘飞’: 14, ‘多功能’: 15, ‘托地’: 16, ‘把’: 17, ‘充气’: 18, ‘浮力’: 19, ‘袖’: 20, ‘盒马花胶’: 21, ‘锅’: 22, ‘鸡汤’: 23, ‘乐’: 24, ‘塞塞’: 25, ‘gs5’: 26, ‘广汽传祺’: 27, ‘挡风遮雨’: 28, ‘条子’: 29}</p>
</blockquote>
<p>将所有的词存入一个字典，上面打印出了每个词对应的 <code>id</code>。所以上面这一步其实就是将词语都映射为数字，因为机器只能理解数字呀。</p>
<p>然后首先将这十条数据放入词袋模型，之后再对词袋模型中的数据使用 <code>TF-IDF</code> 计算权值，得到 <code>TF-IDF</code> 编码。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># Convert corpus to BoW format
# 制作数字向量类型的语料库（doc2bow）
# ----&gt; 将字符串转换成数字向量类型的词袋模型(稀疏向量)
# 源文件不做处理是一个字符串类型的语料库
corpus &#x3D; [dictionary.doc2bow (doc) for doc in seg_list]  
corpus<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[[(0,1), (1,1), (2,1)], [(3,1), (4,1), (5,1), (6,1), (7,1)], [(8,1)], [(9,1), (10,1), (11,1)], [(12,1), (13,1), (14,1)], [(15,1), (16,1), (17,1)], [(18,1), (19,1), (20,1)], [(21,1), (22,1), (23,1)], [(24,1), (25,1)], [(26,1), (27,1), (28,1), (29,1)]]</code></p>
</blockquote>
<p>这里是将十条数据转为词袋模型，下面贴出之前这十条数据的分词结果：</p>
<blockquote>
<p><code>[[&#39;美赞臣&#39;, &#39;亲舒&#39;, &#39;一段&#39;], [&#39;慱&#39;, &#39;朗&#39;, &#39;手动&#39;, &#39;料理&#39;, &#39;机&#39;], [&#39;電力貓&#39;], [&#39;掏&#39;, &#39;夹缝&#39;, &#39;工具&#39;], [&#39;飞&#39;, &#39;推&#39;, &#39;vip&#39;], [&#39;多功能&#39;, &#39;托地&#39;, &#39;把&#39;], [&#39;充气&#39;, &#39;浮力&#39;, &#39;袖&#39;], [&#39;盒马花胶&#39;, &#39;鸡汤&#39;, &#39;锅&#39;], [&#39;塞塞&#39;, &#39;乐&#39;], [&#39;广汽传祺&#39;, &#39;gs5&#39;, &#39;挡风遮雨&#39;, &#39;条子&#39;]]</code></p>
</blockquote>
<p>以第一条数据为例：<br>第一条数据的词袋模型：<code>[(0,1), (1,1), (2,1)]</code><br>第一条数据的分词结果：<code>[&#39;美赞臣&#39;, &#39;亲舒&#39;, &#39;一段&#39;]</code><br>第一条数据的 <code>token2id</code> ：  <code>&#39;一段&#39;: 0, &#39;亲舒&#39;: 1, &#39;美赞臣&#39;: 2</code><br>词袋中第一个词 <code>（0，1）</code> 对应着：“一段”（编码为 0），数据中出现一次，所以 <code>（0，1）</code>。第一个数字代表哪一个词，第二个数字代表出现了几次（词频）。后面的两个词同理。三个词放一起，组成词袋，来代表整个第一条数据。</p>
<p>可以发现，不存在的词是不放入每条数据的词袋的，通过这样的方式来节约内存。这种技术叫稀疏矩阵（Sparse Matrix）：只存储有内容的值，而忽略无内容的值。</p>
<p>最后得到 <code>TF-IDF</code> 编码（以 <code>corpus</code> 中第一条数据为例 ）：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">tfidf &#x3D; models.TfidfModel(corpus)  # fit model
vector &#x3D; tfidf[corpus[0]]  # apply model to the first corpus document
vector<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p><code>[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]</code></p>
</blockquote>
<p>你也可以自己动手，看看后面的几条数据编码是什么样子的。</p>
<h2 id="0x04-使用-TF-IDF-计算文本相似度"><a href="#0x04-使用-TF-IDF-计算文本相似度" class="headerlink" title="0x04 使用 TF-IDF 计算文本相似度"></a>0x04 使用 <code>TF-IDF</code> 计算文本相似度</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">def semblance(text, corpus):
    # 对测试文本分词
    dic_text_list &#x3D; list(jieba.cut(text))
 
    # 制作测试文本的词袋
    doc_text_vec &#x3D; dictionary.doc2bow(dic_text_list)
 
    # 获取语料库每个文档中每个词的tfidf值，即用tfidf模型训练语料库
    tfidf &#x3D; models.TfidfModel(corpus)
 
    # 对稀疏向量建立索引
    index &#x3D; similarities.SparseMatrixSimilarity(tfidf[corpus], num_features&#x3D;len(dictionary.keys()))
    sim &#x3D; index[tfidf[doc_text_vec]]  # 相当于sim &#x3D; index.get_similarities(tfidf[doc_text_vec])
    print(&quot;相似度评估：&quot;)
    print(sim)
    # 按照相似度来排序
    sim_sorted &#x3D; sorted(enumerate(sim, 1), key&#x3D;lambda x: -x[1])  # enumerate(x, 1) 代表从1开始设立索引
    # 相当于sorted(enumerate(sim), key&#x3D;lambda x: x[1], reverse&#x3D;True
    print(&quot;相似度排序：&quot;)
    print(sim_sorted)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">text &#x3D; &#39;想喝鸡汤&#39;
semblance (text, corpus)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>Output:</p>
</blockquote>
<pre class="line-numbers language-text" data-language="text"><code class="language-text">相似度评估：
[0.         0.         0.         0.         0.         0.
 0.         0.81649655 0.         0.        ]
10
相似度排序：
[(8, 0.81649655), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (9, 0.0), (10, 0.0)]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>没有做过 NLP 的东西，趁着做今天的任务大概了解了分词，文本向量化表示之类的内容。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>jieba 分词：<br> <a href="https://github.com/fxsjy/jieba">GitHub - fxsjy&#x2F;jieba: 结巴中文分词</a><br> <a href="https://alvinntnu.github.io/python-notes/corpus/jieba.html">Chinese Word Segmentation (jieba) — Python Notes for Linguistics</a><br> <a href="https://zhuanlan.zhihu.com/p/361052986">中文分词工具 jieba 的简介｜自然语言处理</a><br> <a href="https://zhuanlan.zhihu.com/p/207057233">jieba 分词-强大的 Python 中文分词库</a></p>
<p>Gensim:<br> <a href="https://gensim.apachecn.org/#/blog/tutorial/README">https://gensim.apachecn.org/#/blog/tutorial/README</a></p>
<p>TF-IDF:<br> <a href="https://blog.csdn.net/weixin_44799217/article/details/116423520">自然语言处理(NLP)之使用 TF-IDF 模型计算文本相似度</a><br> <a href="https://mofanpy.com/tutorials/machine-learning/nlp/intro-search/">你天天用的搜索引擎是怎么工作的 - 自然语言处理 | 莫烦 Python</a><br> <a href="https://mofanpy.com/tutorials/machine-learning/nlp/tfidf/">统计学让搜索速度起飞 - 自然语言处理 | 莫烦 Python</a></p>
]]></content>
      <categories>
        <category>“阿里灵杰”问天引擎电商搜索算法赛</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里灵杰_Task02_词向量介绍与训练</title>
    <url>/CGCSR4/</url>
    <content><![CDATA[<h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><ul>
<li>任务内容：<ul>
<li>使用任务 1 得到数据使用 <code>gensim</code> 训练词向量</li>
<li>计算与 <code>格力</code> 相似的 Top10 单词</li>
<li>使用词向量完成句子编码（例如单词编码为 128 维度，一个句子包含十个单词为 10*128）</li>
<li>对句子编码 10*128 进行求均值，转变为 128 维度</li>
<li>扩展：你能使用计算得到的词向量，计算 train.query.txt 和 corpus.tsv 文本的相似度吗（train 选择 100 条文本，corpus 选择 100 条文本）？</li>
</ul>
</li>
<li>学习资料：<ul>
<li><a href="https://coggle.club/blog/tianchi-open-search">https://coggle.club/blog/tianchi-open-search</a></li>
<li><a href="https://radimrehurek.com/gensim/models/word2vec.html">https://radimrehurek.com/gensim/models/word2vec.html</a><span id="more"></span></li>
</ul>
</li>
</ul>
<h2 id="0x01-使用-gensim-训练词向量"><a href="#0x01-使用-gensim-训练词向量" class="headerlink" title="0x01 使用 gensim 训练词向量"></a>0x01 使用 <code>gensim</code> 训练词向量</h2><h3 id="数据集读取"><a href="#数据集读取" class="headerlink" title="数据集读取"></a>数据集读取</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 导入相关库
import numpy as np
import pandas as pd
import os
from tqdm import tqdm_notebook<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 读取数据集
corpus_data &#x3D; pd.read_csv( &quot;.&#x2F;data&#x2F;corpus.tsv&quot;, sep&#x3D;&quot;\t&quot;, names&#x3D;[&quot;doc&quot;, &quot;title&quot;])
train_data &#x3D; pd.read_csv(&quot;.&#x2F;data&#x2F;train.query.txt&quot;, sep&#x3D;&quot;\t&quot;, names&#x3D;[&quot;query&quot;, &quot;title&quot;])
qrels &#x3D; pd.read_csv(&quot;.&#x2F;data&#x2F;qrels.train.tsv&quot;, sep&#x3D;&quot;\t&quot;, names&#x3D;[&quot;query&quot;, &quot;doc&quot;])
dev_data &#x3D; pd.read_csv (&quot;.&#x2F;data&#x2F;dev.query.txt&quot;, sep&#x3D;&quot;\t&quot;, names&#x3D;[&quot;query&quot;, &quot;title&quot;])

# 将原文中 index 设为 df 的 index
corpus_data &#x3D; corpus_data.set_index(&quot;doc&quot;)
train_data &#x3D; train_data.set_index(&quot;query&quot;)
qrels &#x3D; qrels.set_index(&quot;query&quot;)
dev_data &#x3D; dev_data.set_index (&quot;query&quot;)

# 查看一下刚刚导入的数据集
train_data.head()
qrels.head()
qrels.loc[1][&quot;doc&quot;]

# 查看一下前 19 个训练集的 query 与 doc 对应的字段
for idx in range(1,20): 
    print(
        train_data.loc[idx][&quot;title&quot;],
        &quot;\t&quot;,
        corpus_data.loc[qrels.loc[idx][&quot;doc&quot;]][&quot;title&quot;])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>美赞臣亲舒一段        领券满减】美赞臣安婴儿 A+亲舒 婴儿奶粉 1 段 850 克 0-12 个月宝宝<br>慱朗手动料理机        Braun&#x2F;博朗 MQ3035&#x2F;3000&#x2F;5025 料理棒手持小型婴儿辅食家用搅拌机<br>電力貓    小米 WiFi 电力猫无线路由器套装一对 300M 穿墙宝家用信号增强扩展器</p>
</blockquote>
<p>这里只截取一部分展示。</p>
<h3 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 导入 jieba 分词
import jieba

# 先对一个字符串分词看看
&quot; &quot;.join(jieba.cut(&quot;慱朗手动料理机&quot;))

&#39;&#39;&#39;
输出 -&gt; &#39;慱 朗 手动 料理 机&#39;
&#39;&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>上分点：词典（品牌）</strong></p>
<blockquote>
<p>由于 jieba 对品牌的分词效果不一定好，所以可以自行找一些品牌的词典来对这些标题或者 query 分词。</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 对整个数据集分词
def title_cut(x:str):
    return list(jieba.cut(x, HMM&#x3D;True))

from joblib import Parallel, delayed

corpus_title &#x3D; Parallel(n_jobs&#x3D;4)(delayed(title_cut)(title) for title in corpus_data[&quot;title&quot;])
train_title &#x3D; Parallel(n_jobs&#x3D;4)(delayed(title_cut)(title) for title in train_data[&quot;title&quot;])
dev_title &#x3D; Parallel(n_jobs&#x3D;4)(delayed(title_cut)(title) for title in dev_data[&quot;title&quot;])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 使用 gensim 中的 Word2Vec 得到数据集的词向量
from gensim.models import Word2Vec
from gensim.test.utils import common_texts

if os.path.exists(&quot;word2vec.model&quot;):
    model &#x3D; Word2Vec.load(&quot;word2vec.model&quot;)
else: 
    model &#x3D; Word2Vec(
        sentences&#x3D;list(corpus_title) + list(train_title) + list(dev_title),
        vector_size&#x3D;128, # 赛题需要提交的维度
        window&#x3D;5,
        min_count&#x3D;1,
        workers&#x3D;4,
    )
    model.save(&quot;word2vec.model&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="0x02-初探词向量"><a href="#0x02-初探词向量" class="headerlink" title="0x02 初探词向量"></a>0x02 初探词向量</h2><p>经过上面的步骤，我们已经获得了数据集中所有词的词向量。</p>
<h3 id="计算与-格力-相似的-Top10-单词"><a href="#计算与-格力-相似的-Top10-单词" class="headerlink" title="计算与 格力 相似的 Top10 单词"></a>计算与 <code>格力</code> 相似的 Top10 单词</h3><p>下面可以先来看一下一个词的词向量，比如说“格力”：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model.wv[&quot;格力&quot;], model.wv[&quot;格力&quot;].shape<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203221506144.png" alt="|500"></p>
<p>可以看到，输出就是一个 128 维的连续向量。</p>
<p>再来找一下与格力相似的 Top 10 单词：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model.wv.most_similar(&quot;格力&quot;)

&#39;&#39;&#39;
输出：
[(&#39;奥克斯&#39;, 0.8410876989364624),
 (&#39;GREE&#39;, 0.8224707245826721),
 (&#39;柜机&#39;, 0.8208969235420227),
 (&#39;海尔&#39;, 0.8145878911018372),
 (&#39;美的&#39;, 0.8133372664451599),
 (&#39;变频空调&#39;, 0.8063334226608276),
 (&#39;1p1.5&#39;, 0.790867269039154),
 (&#39;挂机&#39;, 0.7852355241775513),
 (&#39;中央空调&#39;, 0.7766402959823608),
 (&#39;新飞&#39;, 0.7650886178016663)]
&#39;&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="获取词向量的-Index"><a href="#获取词向量的-Index" class="headerlink" title="获取词向量的 Index"></a>获取词向量的 Index</h3><p>这是一种 NLP 建模中比较常用的手段，可以得到每个词向量的 ID。因为将这些词（字符串）映射为 ID，由 ID 来进行操作会方便一些。也可以不进行这一步操作。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 通过 index 看一下词向量的前十条是什么
model.wv.index_to_key[0:10]
&#39;&#39;&#39;
输出：
[&#39; &#39;, &#39;新款&#39;, &#39;女&#39;, &#39;&#x2F;&#39;, &#39;2021&#39;, &#39;-&#39;, &#39;加厚&#39;, &#39;儿童&#39;, &#39;秋冬&#39;, &#39;外套&#39;]
&#39;&#39;&#39;

# 找一下“女”这个词的 index
model.wv.key_to_index[&quot;女&quot;]
&#39;&#39;&#39;
输出：
2
&#39;&#39;&#39;
# 可以发现跟之前的是对应的<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>既然 <code>index</code> 可以索引到词，那么我们再来验证一下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 看一下“格力”的 index
model.wv.key_to_index[&quot;格力&quot;]
&#39;&#39;&#39;
输出：
5024
&#39;&#39;&#39;

# 输入“格力”的 index，看看与之前打印的“格力”的词向量是否一样
model.wv[5024], model.wv[5024].shape
# 结果是一样的，可以自行测试<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>获取数据集的 <code>index</code> ：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 从单词 -&gt; id
# 我 爱 阿水
# 2 230 50
train_w2v_ids &#x3D; [[model.wv.key_to_index[xx] for xx in x] for x in train_title]
corpus_w2v_ids &#x3D; [[model.wv.key_to_index[xx] for xx in x] for x in corpus_title]
dev_w2v_ids &#x3D; [[model.wv.key_to_index[xx] for xx in x] for x in dev_title]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="0x03-使用词向量完成句子编码"><a href="#0x03-使用词向量完成句子编码" class="headerlink" title="0x03 使用词向量完成句子编码"></a>0x03 使用词向量完成句子编码</h2><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>先使用 TF-IDF 计算一下词的重要性（区分力），识别 query 与 doc 中哪些词是不重要的。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

idf &#x3D; TfidfVectorizer(analyzer&#x3D;lambda x: x)
idf.fit(train_title + corpus_title)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">idf.idf_, len(idf.idf_)
&#39;&#39;&#39;
输出：
(array([ 2.46292242,  8.5771301 ,  7.7050655 , ..., 14.21903717,
        14.21903717, 14.21903717]),
 640554)
&#39;&#39;&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看见已经计算出了每个词的 TF-IDF。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">token &#x3D; np.array(idf.get_feature_names())

# 选出需要去除的词
# 分数为什么设置小于 10，来自水哥的先验经验
drop_token &#x3D; token[np.where(idf.idf_ &lt; 10)[0]]  # 统计分数小于 10 的词
drop_token &#x3D; list(set(drop_token))
drop_token +&#x3D; [&#39;领券&#39;]

# 得到不重要的单词的 index，以便后面过滤
drop_token_ids &#x3D; [model.wv.key_to_index[x] for x in drop_token]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="句子编码"><a href="#句子编码" class="headerlink" title="句子编码"></a>句子编码</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">def unsuper_w2c_encoding(s, pooling&#x3D;&quot;max&quot;):
    feat &#x3D; []
    corpus_query_word &#x3D; [x for x in s if x not in drop_token_ids]  # 将上一步中得到的不重要的（没有区分力的）词过滤
    if len(corpus_query_word) &#x3D;&#x3D; 0:
        return np.zeros(128)
    
    # 获取句子的词向量
    # N * 128 的矩阵
    # N 是每条句子中筛去不重要的词，剩下的词的数量
    # 这一步得到的是一个矩阵，当我们最终需要的是一个句向量
    # 所以下一步就是把矩阵处理成向量
    feat &#x3D; model.wv[corpus_query_word]

    # 通过 pooling 得到句子的词向量
    # 通过池化将矩阵降维到一个 128 维向量
    if pooling &#x3D;&#x3D; &quot;max&quot;:
        return np.array(feat).max(0)  # 对每一列取最大值，即最大池化
    if pooling &#x3D;&#x3D; &quot;avg&quot;:
        return np.array(feat).mean(0)  # 对每一列取均值，即均值池化<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 用第一条数据测试一下
unsuper_w2c_encoding(train_w2v_ids[0]).shape<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<blockquote>
<p>(128,)</p>
</blockquote>
<p>可以看到已经得到了 128 维的句向量。</p>
<p>那么对整个数据集进行句向量编码：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from tqdm import tqdm_notebook
# [corpus_w2v_ids[x] for x in qrels[&#39;doc&#39;].values[:100] - 1]

corpus_mean_feat &#x3D; [
    unsuper_w2c_encoding(s) for s in tqdm_notebook(corpus_w2v_ids[:1000])
]
corpus_mean_feat &#x3D; np.vstack(corpus_mean_feat)

train_mean_feat &#x3D; [
    unsuper_w2c_encoding(s) for s in tqdm_notebook(train_w2v_ids[:100])
]
train_mean_feat &#x3D; np.vstack(train_mean_feat)

dev_mean_feat &#x3D; [
    unsuper_w2c_encoding(s) for s in tqdm_notebook(dev_w2v_ids[:100])
]
dev_mean_feat &#x3D; np.vstack(dev_mean_feat)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img data-src=" https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202203221710025.png" style="zoom: 67%;" />

<p>这里只用 <code>corpus</code> 的前 1000 条，<code>train</code> 和 <code>dev</code> 的前 100 条作为演示。对整个数据集编码的话，去掉切片即可。</p>
<h2 id="0x04-检索"><a href="#0x04-检索" class="headerlink" title="0x04 检索"></a>0x04 检索</h2><p>既然已经计算得到的词向量，那么就可以用词向量来计算 <code>train.query.txt</code> 和 <code>corpus.tsv</code> 文本的相似度，然后再做一个相似度排序就可以实现检索。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.preprocessing import normalize

# 归一化
corpus_mean_feat &#x3D; normalize(corpus_mean_feat)
train_mean_feat &#x3D; normalize(train_mean_feat)
dev_mean_feat &#x3D; normalize(dev_mean_feat)

# 计算 Loss
mrr &#x3D; []
for idx in tqdm_notebook(range(1, 10)):
    # 首先计算 train 与 corpus 的相似度，然后排序
    dis &#x3D; np.dot(train_mean_feat[idx - 1], corpus_mean_feat.T)
    #print(dis)
    ids &#x3D; np.argsort(dis)[::1]
    #print(ids)
    
    print(train_title[idx-1], corpus_data.loc[qrels.loc[idx].ravel()[0]][&quot;title&quot;],  dis[qrels.loc[idx].ravel()-1])
    print(corpus_title[ids[0]])

    # 计算每个检索的 MRR Loss（赛题中评价指标是用的 MRR）
    mrr.append(1&#x2F;(np.where(ids &#x3D;&#x3D; qrels.loc[idx].ravel()[0] - 1)[0][0] + 1))
    print(&#39;&#39;)

# 打印平均 MRR
print(np.mean(mrr))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>&#96;[‘美赞臣’, ‘亲舒’, ‘一段’] 领券满减】美赞臣安婴儿A+亲舒婴儿奶粉1段850克 0-12个月宝宝 [0.68474327]<br>[‘现货’, ‘加拿大’, ‘美赞臣’, ‘1’, ‘段’, ‘EnfamilA’, ‘+’, ‘一段’, ‘DHA’, ‘奶粉’, ‘765’, ‘克’, ‘超值’, ‘装金装’]</p>
<p>[‘慱’, ‘朗’, ‘手动’, ‘料理’, ‘机’] Braun&#x2F;博朗 MQ3035&#x2F;3000&#x2F;5025料理棒手持小型婴儿辅食家用搅拌机 [0.71025692]<br>[‘朗’, ‘诗’, ‘LS22A1203’, ‘时尚’, ‘气质’, ‘休闲’, ‘百搭显’, ‘瘦’, ‘拼色’, ‘假’, ‘两件’, ‘衬衣’, ‘小衫’, ‘2022’, ‘春装’]</p>
<p>[‘電力貓’] 小米WiFi电力猫无线路由器套装一对300M穿墙宝家用信号增强扩展器 [0.03024337]<br>[‘[‘, ‘新华书店’, ‘]’, ‘ ‘, ‘党委会’, ‘的’, ‘工作’, ‘方法’, ‘ ‘, ‘毛’]&#96;</p>
</blockquote>
<blockquote>
<p>MRR &#x3D; 0.01528688737011513</p>
</blockquote>
<p>这里只展示部分检索结果，可以看到有些j检索结果并不好，有些品牌的分词也不太对。所以后面还可以考虑去掉一些停用词或者标点符号之类的数据清洗手段，达到更好的效果。毕竟数据为王，数据科学中最重要，最耗时的一步其实就是处理数据的工作。</p>
<p>最后将词向量存储到本地：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">with open(&#39;query_embedding&#39;, &#39;w&#39;) as up :
    for id, feat in zip(dev_data.index, dev_mean_feat):
        up.write(&#39;&#123;0&#125;\t&#123;1&#125;\n&#39;.format(id, &#39;,&#39;.join([str(x)[:6] for x in feat])))
        
with open(&#39;doc_embedding&#39;, &#39;w&#39;) as up :
    for id, feat in zip(corpus_data.index, corpus_mean_feat):
        up.write(&#39;&#123;0&#125;\t&#123;1&#125;\n&#39;.format(id, &#39;,&#39;.join([str(x)[:6] for x in feat])))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>水哥讲的太好了，对我这种刚入门 NLP 的小白帮助很大，学到了 NLP 完整的一套训练流程。</p>
<p>感觉句子编码 <code>pooling</code> 的时候好像也可以做一些技巧。目前是直接用 Word2Vec 得到的词向量，这个比赛本质上是看谁的词向量训练的好，所以后面尝试用 <code>SimCSE</code> 这些深度模型来训练，效果应该会提升一些。当然了，提分的话首先还是从数据入手，比如 <code>corpus</code> 的 <code>title</code> 长度很长，会有一百多个字符，而 <code>query</code> 就是比较短的搜索。所以 <code>corpus</code> 里的那些非核心词（噪声）势必会对训练产生影响。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/ECommerceSearch">team-learning-data-mining&#x2F;ECommerceSearch at master · datawhalechina&#x2F;team-learning-data-mining · GitHub</a></p>
<p><a href="https://www.bilibili.com/video/BV1dU4y1R7a2?spm_id_from=333.999.0.0">天池大神阿水解读：阿里灵杰电商搜索算法赛 | bilibili</a></p>
]]></content>
      <categories>
        <category>“阿里灵杰”问天引擎电商搜索算法赛</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
