<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222" media="(prefers-color-scheme: light)"><meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0"><link rel="preconnect" href="https://fonts.loli.net" crossorigin><link rel="preconnect" href="https://lib.baomitu.com" crossorigin><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png"><link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CLato:300,300italic,400,400italic,700,700italic%7C'Noto+Sans':300,300italic,400,400italic,700,700italic%7Csans-serif:300,300italic,400,400italic,700,700italic%7C'Noto+Sans+SC':300,300italic,400,400italic,700,700italic%7CRoboto+Serif:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="https://lib.baomitu.com/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous"><script class="next-config" data-name="main" type="application/json">{"hostname":"1nnoh.top","root":"/","images":"/images","scheme":"Mist","darkmode":"auto","version":"8.13.2","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"width":269},"copycode":{"enable":false,"style":"flat"},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script><link rel="stylesheet" href="/js/prism/prism.css"><meta name="description" content="1.3 中文分词0x00 Abstract中文分词的目的：对中文句子中的词与词之间加上边界标记，本质就是对中文句子做划分词的边界。  为什么要做中文分词？ 因为英文单词之间是天然分开的，但中文没有   怎么做中文分词？ 基于统计学习的方法 基于机器学习的方法等"><meta property="og:type" content="article"><meta property="og:title" content="1.3 中文分词"><meta property="og:url" content="https://1nnoh.top/2ZZ2RHB/index.html"><meta property="og:site_name" content="1nnoh&#39;s Blog"><meta property="og:description" content="1.3 中文分词0x00 Abstract中文分词的目的：对中文句子中的词与词之间加上边界标记，本质就是对中文句子做划分词的边界。  为什么要做中文分词？ 因为英文单词之间是天然分开的，但中文没有   怎么做中文分词？ 基于统计学习的方法 基于机器学习的方法等"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031719270.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031724760.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031731152.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031753916.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206032350989.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051658209.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051659302.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051700333.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051704065.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206112109344.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206112117841.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122110155.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122111277.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122117079.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122119587.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122124565.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122124567.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122154778.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122157720.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122202063.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122213974.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206132248906.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206132304514.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206132321801.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140002491.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140005456.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140008489.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140012676.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140011610.png"><meta property="og:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206112109344.png"><meta property="article:published_time" content="2022-05-13T12:52:15.000Z"><meta property="article:modified_time" content="2022-05-13T12:52:15.000Z"><meta property="article:author" content="1nnoh"><meta property="article:tag" content="NLP"><meta property="article:tag" content="FunRec"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031719270.png"><link rel="canonical" href="https://1nnoh.top/2ZZ2RHB/"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://1nnoh.top/2ZZ2RHB/","path":"2ZZ2RHB/","title":"1.3 中文分词"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>1.3 中文分词 | 1nnoh's Blog</title><script src="/js/third-party/analytics/baidu-analytics.js"></script><script async src="https://hm.baidu.com/hm.js?b20166297c5501f4ebb46c56425f8cb4"></script><script async defer data-website-id="" src=""></script><script defer data-domain="" src=""></script><link rel="dns-prefetch" href="https://waline-server-one.vercel.app/"><link rel="stylesheet" href="/js/prism/prism.css"><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body itemscope itemtype="http://schema.org/WebPage"><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">1nnoh's Blog</p><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-3-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D"><span class="nav-text">1.3 中文分词</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#0x00-Abstract"><span class="nav-text">0x00 Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x01-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%9A%84%E9%9A%BE%E7%82%B9"><span class="nav-text">0x01 中文分词的难点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x02-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-text">0x02 中文分词的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x03-%E6%A1%88%E4%BE%8B1%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%AD%97%E5%85%B8%E5%8C%B9%E9%85%8D%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-text">0x03 案例1：基于字典匹配的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%B8%80%E7%A7%8D%E9%87%8D%E8%A6%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%EF%BC%88%E5%89%8D%E7%BC%80%E6%A0%91%EF%BC%89"><span class="nav-text">1. 一种重要的数据结构（前缀树）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%A6%82%E7%8E%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-text">2. 概率语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86"><span class="nav-text">基础原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#N-%E5%85%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88N-Gram%EF%BC%89"><span class="nav-text">N 元语言模型（N-Gram）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%AE%9E%E8%B7%B5"><span class="nav-text">3. 实践</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E6%9C%80%E5%A4%A7%E9%95%BF%E5%BA%A6%E5%8C%B9%E9%85%8D%EF%BC%88%E5%8F%8D%E5%90%91%EF%BC%89"><span class="nav-text">方法一：最大长度匹配（反向）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E6%9C%80%E5%A4%A7%E6%A6%82%E7%8E%87-Bigram"><span class="nav-text">方法二：最大概率(Bigram)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%B0%8F%E7%BB%93"><span class="nav-text">4. 小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x04-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E2%80%94%E2%80%94Viterbi-%E7%AE%97%E6%B3%95"><span class="nav-text">0x04 动态规划——Viterbi 算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x05-%E6%A1%88%E4%BE%8B2%EF%BC%9A%E5%A4%8D%E7%94%A8%E8%BD%AE%E5%AD%90%E2%80%94%E2%80%94jieba"><span class="nav-text">0x05 案例2：复用轮子——jieba</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%AE%89%E8%A3%85"><span class="nav-text">1. 安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%95%B4%E4%BD%93%E9%80%BB%E8%BE%91"><span class="nav-text">2. 整体逻辑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%B8%BE%E4%BE%8B%E5%AD%90"><span class="nav-text">3. 举例子</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x06-%E6%A1%88%E4%BE%8B3%EF%BC%9A%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-text">0x06 案例3：基于统计机器学习的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B"><span class="nav-text">1. 马尔科夫模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BE%8B%E5%AD%90"><span class="nav-text">例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B1%80%E9%99%90"><span class="nav-text">局限</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B-HMM"><span class="nav-text">2. 隐马尔科夫模型 (HMM)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B-1"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%94%E7%94%A8"><span class="nav-text">应用</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x07-%E5%AE%9E%E8%B7%B5"><span class="nav-text">0x07 实践</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%89%B9%E9%87%8F%E5%88%86%E8%AF%8D"><span class="nav-text">1. 批量分词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%A2%9E%E9%87%8F%E5%88%86%E8%AF%8D%EF%BC%88%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%EF%BC%89"><span class="nav-text">2. 增量分词（流式处理）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x08-%E4%BD%9C%E4%B8%9A"><span class="nav-text">0x08 作业</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="1nnoh" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">1nnoh</p><div class="site-description" itemprop="description">如常.</div></div><div class="site-state-wrap site-overview-item animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">15</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author site-overview-item animated"><span class="links-of-author-item"><a href="https://github.com/1nnoh" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;1nnoh" rel="noopener external nofollow noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:jhhou.cn@gmail.com" title="E-Mail → mailto:jhhou.cn@gmail.com" rel="noopener external nofollow noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div><div class="cc-license site-overview-item animated" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener external nofollow noreferrer" target="_blank"><img src="https://lib.baomitu.com/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a></div></div></div></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://1nnoh.top/2ZZ2RHB/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="1nnoh"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="1nnoh's Blog"><meta itemprop="description" content="如常."></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="1.3 中文分词 | 1nnoh's Blog"><meta itemprop="description" content=""></span><header class="post-header"><h1 class="post-title" itemprop="name headline">1.3 中文分词</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-05-13 20:52:15" itemprop="dateCreated datePublished" datetime="2022-05-13T20:52:15+08:00">2022-05-13</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/FunRec/" itemprop="url" rel="index"><span itemprop="name">FunRec</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/FunRec/01-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">01-自然语言文本处理基础</span></a> </span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Waline：</span> <a title="waline" href="/2ZZ2RHB/#waline" itemprop="discussionUrl"><span class="post-comments-count waline-comment-count" data-path="/2ZZ2RHB/" itemprop="commentCount"></span> </a></span><span class="post-meta-break"></span> <span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>8.2k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>19 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><h1 id="1-3-中文分词"><a href="#1-3-中文分词" class="headerlink" title="1.3 中文分词"></a>1.3 中文分词</h1><h2 id="0x00-Abstract"><a href="#0x00-Abstract" class="headerlink" title="0x00 Abstract"></a>0x00 Abstract</h2><p>中文分词的目的：对中文句子中的词与词之间加上边界标记，本质就是对中文句子做划分词的边界。</p><ul><li>为什么要做中文分词？<ul><li>因为英文单词之间是天然分开的，但中文没有</li></ul></li><li>怎么做中文分词？<ul><li>基于统计学习的方法</li><li>基于机器学习的方法等</li></ul></li></ul><span id="more"></span><h2 id="0x01-中文分词的难点"><a href="#0x01-中文分词的难点" class="headerlink" title="0x01 中文分词的难点"></a>0x01 中文分词的难点</h2><p><strong>中文分词面临以下难点</strong>：</p><ul><li>标准（颗粒度选择，场景多样）<ul><li>搜索领域、推荐领域以及其他垂直领域的<strong>需求不同</strong></li></ul></li><li>歧义（一词多义）<ul><li>苹果 =&gt; 手机还是水果？</li><li>炒鱿鱼？</li></ul></li><li>新词（未登录词）</li></ul><p>后面两点很容易理解，第一点需要详细阐述一下。在不同的场景中，对于分词颗粒度的选择是不同的。下面举两个例子。</p><ul><li>切词颗粒度：<ul><li>粗颗粒度：适合推荐场景<ul><li>例子：分成 =&gt; /环球影城/<ul><li>推荐环球影城有关的</li><li>而不是跟环球有关，或者其他跟影城有关的</li></ul></li></ul></li><li>细颗粒度：适合搜索场景<ul><li>例子：需要分成 =&gt; /环球/影城/</li><li>保证召回质量，出现的词都涉及到</li><li>而不是只召回了有关<strong>环球影城</strong>的，其余的召回就全是无关的。</li></ul></li></ul></li></ul><p><strong>也就是说对于推荐场景来说，哪怕漏掉一些也没关系，但一定要准；但对于搜索来说，要保证召回的全面。</strong></p><h2 id="0x02-中文分词的方法"><a href="#0x02-中文分词的方法" class="headerlink" title="0x02 中文分词的方法"></a>0x02 中文分词的方法</h2><ul><li>基于规则字典：<ul><li>最大长度匹配（正、逆、双序）<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/103392455">中文分词算法之–最大匹配法 - 知乎</a></li><li>加入前缀树改进最大长度匹配</li></ul></li><li>基于机器学习：<ul><li>HMM——双序列标注</li><li>CRF——标注训练</li></ul></li></ul><h2 id="0x03-案例1：基于字典匹配的方法"><a href="#0x03-案例1：基于字典匹配的方法" class="headerlink" title="0x03 案例1：基于字典匹配的方法"></a>0x03 案例1：基于字典匹配的方法</h2><p>在字典树里寻找逐字寻找匹配的词，如果已经匹配切分好的词，那么结束这个词，下一个字符从新的字符开始。</p><p>那么如何实现？首先基于<strong>前缀树的数据结构</strong>构成字典树，然后通过<strong>概率语言模型</strong>预测分词结果。</p><h3 id="1-一种重要的数据结构（前缀树）"><a href="#1-一种重要的数据结构（前缀树）" class="headerlink" title="1. 一种重要的数据结构（前缀树）"></a>1. 一种重要的数据结构（前缀树）</h3><p>Trie 树，即字典树，又称单词查找树或者键树，是一种树形结构，是一种哈希树的变种，常被搜索引擎系统用于文本词频统计。</p><p><strong>优点</strong>： 最大限度地减少无谓的字符串比较，查询效率高于哈希表。</p><p>有了字典树，就可以实现基于最大长度分词，如下。</p><p>正向和逆向最大长度匹配（对输入字符串开始匹配方向不同）：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031719270.png"></p><p>正向最大长度匹配对字符串从前向后匹配，逆向最大长度匹配从后向前匹配。</p><p>但是，仅仅基于最大长度匹配的分词方法<strong>靠谱吗？</strong></p><p>正向与反向最大长度匹配都可能出现特例，比如：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031724760.png"></p><p>在这个例子中，正向不如反向的效果。一般来讲，因为中文比较复杂以及中文的特殊性，逆向最大匹配大多时候往往会比正向要准确。</p><p>由此也可以发现只通过最大长度匹配来分词是不够的。</p><p>所以，需要借助<strong>概率语言模型</strong>来完善分词的效果。</p><h3 id="2-概率语言模型"><a href="#2-概率语言模型" class="headerlink" title="2. 概率语言模型"></a>2. 概率语言模型</h3><h4 id="基础原理"><a href="#基础原理" class="headerlink" title="基础原理"></a>基础原理</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031731152.png"></p><blockquote><p>输入=&gt; 对于输入的字符串，以字为单位，根据字典，将所有可能的词都连起来，构成一个有向无环图（DAG）。<br>如何输出？=&gt; 将所有可能的词的组合概率都计算出来，然后取概率最高的组合。</p></blockquote><p>==输入==：字串 C = c1, c2, …, cn（以字为单位）<br>==输出==：词串 S = w1, w2, …, wn （以词为单位）</p><p>从输入到输出，由条件概率 P(S|C) 来衡量——P(S|C)：<strong>字串 C</strong> 产生<strong>切分 S</strong> 的概率。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206031753916.png"></p><blockquote><p>说明：上面式子的变形基于贝叶斯公式。<br>P(A|B) = P(A,B) / P(B) = P(B|A) * P(A) / P(B)<br>P(A, B) = P(B|A) * P(A) = P(A|B) * P(B)</p></blockquote><p>=&gt; 上面的式子可以继续化简。<br>**因为 P(C) = 1 并且 P(C|S) = 1，上式只剩下 P(S)**。P(C) = 1 很容易理解，那么为什么 P(C|S) = 1？结合下面的例子理解。</p><p>==举个栗子：==<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206032350989.png"></p><p>以 P(C|S1) 为例，S1: 南京市/长江/大桥，C：南京市长江大桥，所以 S1 去掉那两个分词斜杠就得到了 C，可得根据 S1 必然能得到 C。因此 P(C|S1) = 1 对于其他的词串字串也一样。</p><p>对于字串 C 的分词结果 =&gt; seg(C) = argmax P(S) ：令 P(S) 取到最大的 S。</p><p>那么在本例就只需要计算 P(S1) 与 P(S2)。</p><blockquote><p>如何计算 P(S1)？<br>P(S1) = P(南京市)*P(长江)*P(大桥)<br>以 P(南京市) 为例。<br>假设有十篇文章，词语总量为 100，<strong>南京市</strong>总共出现了 2 次。<br>那么 P(南京) = 2 / 100 = 0.02<br>以此类推，算出所有概率。</p></blockquote><p>==优化：==<br>概率连乘，而且这些概率一般都是小数点后几位，比如 <code>0.00002</code>,那么概率连乘后，最后得到的概率会在小数点后很多位，因此会导致计算机的精度不够。如何解决？</p><p>=&gt; 取 log。</p><ul><li>取 log 的作用<ul><li>防止向下溢出</li><li>加法比乘法快</li></ul></li></ul><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051658209.png"><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051659302.png"><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051700333.png"></p><h4 id="N-元语言模型（N-Gram）"><a href="#N-元语言模型（N-Gram）" class="headerlink" title="N 元语言模型（N-Gram）"></a>N 元语言模型（N-Gram）</h4><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206051704065.png"></p><ul><li>N-Gram<ul><li>Unigram model：不考虑前面出现的词</li><li>Bigram model：考虑前面的一个词</li><li>Trigram model：考虑前面的两个词</li></ul></li></ul><p>如何计算 N-Gram 的概率？</p><p>以“广州/本田/雅阁/汽车”为例。</p><ul><li>一元：P（广州，本田，雅阁，汽车）= P(广州) * P(本田) * P(雅阁) * P(汽车)</li><li>二元：P（广州，本田，雅阁，汽车）= P(广州) * P(本田|广州) * P(雅阁|本田) * P(汽车|雅阁)</li><li>三元：P（广州，本田，雅阁，汽车）= P(广州) * P(本田|广州) * P(雅阁|本田，广州) * P(汽车|雅阁，本田)</li></ul><p>以 P(本田|广州) 为例，怎么计算？</p><p>P(本田|广州) =<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-1.238ex" xmlns="http://www.w3.org/2000/svg" width="14.16ex" height="3.607ex" role="img" focusable="false" viewBox="0 -1047.1 6258.8 1594.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">本</text></g><g data-mml-node="mi" transform="translate(3840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">田</text></g><g data-mml-node="mi" transform="translate(4840,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(5840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">广</text></g><g data-mml-node="mi" transform="translate(6840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">州</text></g><g data-mml-node="mo" transform="translate(7840,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(1280.7,-370.3) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">广</text></g><g data-mml-node="mi" transform="translate(3840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">州</text></g><g data-mml-node="mo" transform="translate(4840,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="6018.8" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></p><p>即计算<strong>本田和广州</strong>同时出现的次数（一定要广州在前，本田在后），以及<strong>广州</strong>出现的次数。</p><h3 id="3-实践"><a href="#3-实践" class="headerlink" title="3. 实践"></a>3. 实践</h3><h4 id="方法一：最大长度匹配（反向）"><a href="#方法一：最大长度匹配（反向）" class="headerlink" title="方法一：最大长度匹配（反向）"></a>方法一：最大长度匹配（反向）</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">import sys

WordDic = {}
MaxWordLen = 1

# 写入字典
# 并且找到词语最大长度
def LoadLexicon(lexiconFile):
    global MaxWordLen
    infile = open(lexiconFile, 'r', encoding='gb2312')
    s = infile.readline().strip()  # 每次读一行
    while len(s) &gt; 0:  # 通过判断每行的长度，放在 while 循环里读完整个文件
        #s = s.decode("gb2312")
        WordDic[s] = 1  # 读到的每行的词，放入字典
        if len(s) &gt; MaxWordLen:
            MaxWordLen = len(s)  # 记录词语最大长度
        s = infile.readline().strip()
    infile.close()

def BMM(s):
    global MaxWordLen
    wordlist = []
    i = len(s)  # 句子长度为 i，代表有 i 个字
    while i &gt; 0:
        start = i - MaxWordLen  # 从后向前，以最大长度匹配
        if start &lt; 0:
            start = 0
        while start &lt; i:
            tmpWord = s[start:i]  # 框起来 start 到 i 的字
            if tmpWord in WordDic:
                wordlist.insert(0, tmpWord)  # 在字典中找到匹配的词，那么放入 wordlist
                break
            else:
                start += 1
        # 如果 start 一直向后移动，直到 i（句尾）还没有匹配的词
        # 那么将最后的一个字作为一个分词结果
        if start &gt;= i:
            wordlist.insert(0, s[i-1:i])
            start = i - 1
        i = start
        # 因为 start~i的字已经构成了分词结果
        # 所以下一次循环中，句尾坐标 i 重新赋值，左闭右开，所以不包括 start
    return wordlist

def PrintSegResult(wordlist):
    print("After word-seg:")
    for i in range(len(wordlist)-1):
        print(wordlist[i])
    print(wordlist[len(wordlist)-1])

LoadLexicon("./lexicon.txt")

inputStr = u"南京市长江大桥"

wordlist = BMM(inputStr)
PrintSegResult(wordlist)
# 输出：
# After word-seg:
# 南京
# 市
# 长江
# 大
# 桥<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="方法二：最大概率-Bigram"><a href="#方法二：最大概率-Bigram" class="headerlink" title="方法二：最大概率(Bigram)"></a>方法二：最大概率(Bigram)</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Step 1.
# convert token to id
python CreateLexicon.py

Step 2.
# gen lang model
python BiLMTrain.py

Step 3.
# viterbi
python ViterbiCWS.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h3><ul><li>中文分词<ul><li>容易区分的——登陆词（词表中已有的）=&gt; 字典树（Trie）</li><li>不容易区分的——未登录词（新词）=&gt; 单字</li></ul></li></ul><p>通过上面的学习，我们知道中文分词是这样处理的：对于登录词，使用字典树，对于未登录词，分为单字。</p><p>那么如何才能更合理的处理未登录词呢？比如 <code>广州/本田/雅/阁/汽车</code>，如何将雅阁分为一个词呢？</p><p>=&gt; <strong>隐马尔可夫模型</strong></p><blockquote><p>隐马模型可以像胶水一样，将他认为合适的词粘到一起，合并为一个词。</p></blockquote><p>最后重新总结一下中文分词的技术点，并且引出后面要学习的内容——动态规划。</p><ul><li>中文分词的技术<ul><li>字典树：处理登录词</li><li>隐马模型：处理未登录词</li><li>动态规划（Viterbi 算法）：支撑以上两个技术</li></ul></li></ul><h2 id="0x04-动态规划——Viterbi-算法"><a href="#0x04-动态规划——Viterbi-算法" class="headerlink" title="0x04 动态规划——Viterbi 算法"></a>0x04 动态规划——Viterbi 算法</h2><p>多步骤，每步多选择模型的最优选择问题。</p><p>每一步的所有选择都保存了前序步骤到当前步骤的最优选择。</p><p>依次计算完所有步骤后，通过回溯的方法找到最优选择路径。</p><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206112109344.png"></p><p>这里不是很容易理解，需要好好思考一下。<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.zhihu.com/question/20136144">如何通俗地讲解 viterbi 算法？ - 知乎</a></p><p>Viterbi 与枚举的区别到底在哪？以上图为例，枚举的话需要 <code>3×3×3</code> 次计算，Viterbi 需要 <code>3×3×2</code> 次计算。假设每一步有 N 种选择，一共有 L 步。那么枚举的时间复杂度为 O(N^L)，Viterbi 的时间复杂度为 O(N^2*L)，也就是说当这个序列越长（步数 L 越大）时，两种方法的复杂度差距会越大。这时，Viterbi 的优势就体现出来了。</p><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206112117841.png"></p><p>实现参考上一节的实践方法二中 <code>ViterbiCWS.py</code>。</p><h2 id="0x05-案例2：复用轮子——jieba"><a href="#0x05-案例2：复用轮子——jieba" class="headerlink" title="0x05 案例2：复用轮子——jieba"></a>0x05 案例2：复用轮子——jieba</h2><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">方式1：

pip install jieba

方式2：

先下载 http://pypi.python.org/pypi/jieba/

然后解压，运行 python setup.py install

// 具体信息可参考github文档：[https://github.com/fxsjy/jieba](https://link.zhihu.com/?target=https://github.com/fxsjy/jieba)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-整体逻辑"><a href="#2-整体逻辑" class="headerlink" title="2. 整体逻辑"></a>2. 整体逻辑</h3><p>梳理一下 <code>jieba</code> 分词的逻辑，也是重新复习一遍中文分词的相关技术。</p><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122110155.png"></p><p>整体逻辑如上，对于登录词通过前两步处理，对于未登录词，使用 HMM。</p><p>进一步拆分：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122111277.png"></p><blockquote><p>对于剩下的未登录词，不知道这些字应不应该粘到一起，使用 HMM 判断。</p></blockquote><h3 id="3-举例子"><a href="#3-举例子" class="headerlink" title="3. 举例子"></a>3. 举例子</h3><p>这里用一个例子带入上面的分词整体逻辑，理解分词的细节。</p><blockquote><p>句子：<strong>广州本田雅阁汽车</strong></p></blockquote><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122117079.png"></p><p>构成 ==DAG 表示==：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122119587.png"></p><p>如图可见，每个字用一个 id 表示。<code>0:[0,1,3]</code> 代表通过查字典，0 到 0：广，可以作为一个词；0 到 1：广州，可以作为一个词；0 到 3：广州本田，可以作为一个词。以此类推。</p><p>每种分词结果，比如 <code>广州、广州本田、汽车</code> 都有自己的概率。下一步就是找出概率最大的路径组合。</p><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122124565.png"></p><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122124567.png"></p><p>以上的概率是怎么算的？是根据词表（字典）算的。</p><p>以 <code>汽车</code> 为例。比如汽车在词表中的词频为 10，然后词表中所有词的词频相加为 1000，那么 p(汽车)= log(10/1000)，为了防止下溢，取个 log。</p><h2 id="0x06-案例3：基于统计机器学习的方法"><a href="#0x06-案例3：基于统计机器学习的方法" class="headerlink" title="0x06 案例3：基于统计机器学习的方法"></a>0x06 案例3：基于统计机器学习的方法</h2><h3 id="1-马尔科夫模型"><a href="#1-马尔科夫模型" class="headerlink" title="1. 马尔科夫模型"></a>1. 马尔科夫模型</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p><strong>每个状态只依赖之前有限个状态。</strong></p><ul><li>N 阶马尔科夫：依赖之前 n 个状态</li><li>1 阶马尔科夫：仅仅依赖前一个状态</li></ul><blockquote><p>所以二元语言模型相当于 1 阶马尔科夫。</p></blockquote><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122154778.png"></p><ul><li>参数：<ul><li>状态（S1, S2, …）——其实在这里就相当于词语</li><li>初始概率<ul><li>比如共 1000 篇文章，其中 100 篇文章以 <code>今天</code> 作为开头</li><li>那么 <code>今天</code> 的初始概率就为 100/1000</li></ul></li><li>状态转移概率<ul><li>其实就是 N-Gram 中，在前 n 个词出现的条件下，当前词出现的概率</li></ul></li></ul></li></ul><p>==最重要的两类概率：==<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122157720.png"></p><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>p(w1=今天，w2=我，w3=写，w4=了，w5=一个，w6=程序)<br>= p(w1=今天)p(w2=我|w1=今天)p(w3=写|w2=我)……p(w6=程序|w5=一个)</p><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122202063.png"></p><h4 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h4><p>马尔科夫是单序列模型，可以解决的业务问题有限。比如机器翻译、语音识别这种工作，马尔科夫是做不了的。</p><ul><li>机器翻译：源语言序列 &lt;–&gt; 目标语言序列</li><li>语音识别：语音信号序列 &lt;–&gt; 文字序列</li></ul><h3 id="2-隐马尔科夫模型-HMM"><a href="#2-隐马尔科夫模型-HMM" class="headerlink" title="2. 隐马尔科夫模型 (HMM)"></a>2. 隐马尔科夫模型 (HMM)</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p>马尔科夫是单序列建模，隐马是双序列建模。</p><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206122213974.png"></p><ul><li>参数：<ul><li>状态序列（未知）</li><li>初始概率</li><li>状态转移概率</li><li>观察序列（已知）</li><li>发射概率</li></ul></li></ul><p>以机器翻译为例，解释一下上面的五个参数。比如翻译 <code>我爱你 —&gt; I love you</code>。</p><ul><li><code>我爱你</code>，是已知的观察序列；<code>I love you</code> 是未知的状态序列（想要得到的）。</li><li>初始概率和状态转移概率同之前的马尔科夫模型一样，即代表 <code>I</code> 作为开头的概率，以及在前面一个词为 <code>I</code> 的条件下，下一个状态（词）为 <code>love</code> 的概率，等等。</li><li>发射概率：从状态序列到观察序列的概率。也就是说，<code>I</code> 翻译为 <code>我</code> 的概率，<code>love</code> 翻译为 <code>爱</code> 的概率，以此类推。</li></ul><blockquote><p>梳理一下整个模型的过程：<br>首先完成第一状态，然后依次由当前状态生成下一状态，最后每个状态发射出一个观测值。</p></blockquote><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206132248906.png"></p><p>因此，最后就是寻找一个最大的联合概率。</p><h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>那么在 <code>jieba 分词</code> 中，是如何运用隐马模型将那些未登录词的单个字符粘到一起的呢？</p><p>以 <code>广州塔</code> 为例，假设这个词是一个未登录词。那么输入模型就是以单字输入。</p><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206132304514.png"></p><p>如上图，以单字输入模型。</p><p>那么每个字的状态有以下几种：</p><ol><li>B —— 词语开头</li><li>M —— 词语中间</li><li>E —— 词语结尾</li><li>S —— 单字</li></ol><blockquote><p>举例：<br>汽车——BE<br>面包车——BME<br>天天向上——BMME<br>哦——S</p></blockquote><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206132321801.png"></p><p>因此通过这四种状态，预测出一个状态序列，就可以得到分词结果。</p><p>那么对应着 HMM 的五个参数，依次分析如何实现分词的隐马模型。</p><ul><li><p>观察序列（已知）：广 州 塔</p></li><li><p>状态序列（未知）：需要预测的，预测每个位置到底是哪种状态（BMES其中之一）</p></li><li><p><strong>初始概率</strong>（状态序列）</p><ul><li>四种概率：P(B), P(M), P(E), P(S)</li><li>比如，有 1000 篇文章，那么去统计这些文章的第一个字的状态。比如有 800 篇，第一个字的状态为 B；有 200 篇，第一个字的状态为 S。=&gt; 那么，P(B)=0.8，P(S)=0.2（不可能会有 E 或者 S 这两个状态，但是这两个状态概率要给一个很小的值，防止乘 0）。</li></ul></li></ul><blockquote><p>在 jieba 分词的代码中印证一下。</p></blockquote><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">git clone https://github.com/fxsjy/jieba.git

cd jieba/jieba/finalseg

cat prob_start.py

# 输出：
# P={'B': -0.26268660809250016,
# 'E': -3.14e+100,
# 'M': -3.14e+100,
# 'S': -1.4652633398537678}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到初始概率文件中一共有四种概率，其中 E 和 M 都是给了一个接近 0 的概率值。</p><ul><li><strong>转移概率</strong><ul><li>如 P(M|B), P(E|B) 等。</li><li>以 P(M|B) 为例，怎么计算？<ul><li>=&gt;<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-1.238ex" xmlns="http://www.w3.org/2000/svg" width="9.502ex" height="3.607ex" role="img" focusable="false" viewBox="0 -1047.1 4199.7 1594.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mo" transform="translate(3891,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(4169,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(4928,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(689.9,-370.3) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(3599,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="3959.7" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>统计一共有多少 B 状态，统计出现 B 状态后出现 M 状态的次数。然后相除。</li></ul></li></ul></li></ul><blockquote><p>在 jieba 分词的代码中印证一下。</p></blockquote><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd jieba/jieba/finalseg

cat prob_start.py

# 输出：
# P={'B': {'E': -0.510825623765990, 'M': -0.916290731874155},
# 'E': {'B': -0.5897149736854513, 'S': -0.8085250474669937},
# 'M': {'E': -0.33344856811948514, 'M': -1.2603623820268226},
# 'S': {'B': -0.7211965654669841, 'S': -0.6658631448798212}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><strong>发射概率</strong><ul><li>如 P(广|B), P(州|M) 等。</li><li>以 P(广|B) 为例，怎么计算？<ul><li>=&gt;<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-1.238ex" xmlns="http://www.w3.org/2000/svg" width="9.42ex" height="3.607ex" role="img" focusable="false" viewBox="0 -1047.1 4163.6 1594.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">广</text></g><g data-mml-node="mo" transform="translate(3840,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(4118,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(4877,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(671.8,-370.3) scale(0.707)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(918,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2090,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(2451,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2840,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(3599,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="3923.6" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>统计一共有多少 B 状态，然后统计有多少状态是B，同时这个字是“广”的次数。然后相除。</li></ul></li></ul></li></ul><blockquote><p>在 jieba 分词的代码中印证一下。</p></blockquote><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd jieba/jieba/finalseg

vim prob_emit.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>输出：<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140002491.png"></p><p>以上这些概率，都是通过人工标注，然后统计得来。</p><p>总结一下，从具体落地实现的角度，分别需要三张词表。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140005456.png"></p><p>上面的概率包含了词性。在 <code>jieba</code> 的这个目录可以看到：<code>./jieba/jieba/posseg]</code>。</p><ul><li>其中<ul><li>BEMS 表示位置信息：B（开头）、M（中间）、E（结尾）、S（独立成词）</li><li>词性：n（名词）、nr（人名）、ns（地名）、v（动词）<ul><li>参考：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/kevin_darkelf/article/details/39520881">中文分词词性对照表_kevin_darkelf的专栏-CSDN博客_ucn是什么词性的缩写</a></li></ul></li></ul></li></ul><p>回顾之前的例子：<strong>广州塔</strong></p><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140008489.png"></p><p>对于 HMM 的应用，最典型的就是给定 O（观察序列），找最优的 S（状态序列）。</p><p><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140012676.png"><br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206140011610.png"></p><h2 id="0x07-实践"><a href="#0x07-实践" class="headerlink" title="0x07 实践"></a>0x07 实践</h2><p>以分词为例，在实际业务落地过程中，需要<strong>批量处理</strong>与<strong>流式处理</strong>协同完成。比如当天24点之前的，昨天的数据使用批量处理；而今天过了24点，实时产生的数据使用流式处理。</p><p>使用 Hadoop 批量处理的数据可以追求模型效果，使用复杂的模型；而使用 Flink 等做流式处理的数据，需要考虑延时，追求高效性（避免阻塞），一般使用简单一些的模型，不要要求很完善的效果。</p><h3 id="1-批量分词"><a href="#1-批量分词" class="headerlink" title="1. 批量分词"></a>1. 批量分词</h3><blockquote><p>待更新</p></blockquote><h3 id="2-增量分词（流式处理）"><a href="#2-增量分词（流式处理）" class="headerlink" title="2. 增量分词（流式处理）"></a>2. 增量分词（流式处理）</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd ./nlp/1.nlp-foundation/1.2.chinese-segmentation/pyweb_pg_test

pip install web.py

python main.py 8888

# 在浏览器中输入：
# http://127.0.0.1:8888/?content=南京市长江大桥

# 或者
curl http://0.0.0.0:8888/\?content\=%E5%8D%97%E4%BA%AC%E5%B8%82%E9%95%BF%E6%B1%9F%E5%A4%A7%E6%A1%A5<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="0x08-作业"><a href="#0x08-作业" class="headerlink" title="0x08 作业"></a>0x08 作业</h2><p>作业：</p><ol><li>用自己的话，把“广州塔”的 HMM 逻辑大概介绍一下。</li><li>用自己的话，把 Viterbi 的逻辑大概介绍一下。</li></ol><p>面试题：</p><blockquote><p><strong>一道面试题：</strong><br>有一个非常非常长的字符串，标记为L，这个字符串里面的每个字符不限于字母和数字，由于字符串太大了，导致内存无法存储<br>另，有M个小的字符串，最大长度为N<br><strong>问题</strong>：统计出，这些M个小字符串，一共被包含多少次？</p><p><strong>答</strong>：同使用最大长度匹配做中文分词一样，小字符串构建前缀树，长字符串每次取 N 个字符去匹配小字符串，每次匹配的起点一位一位向后移动。</p></blockquote><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这篇笔记比较完备地记录了中文分词从理论到实践的全过程。</p><p><strong>中文分词的难点</strong>：</p><ul><li>中文不像英文天然分词，因此产生了中文分词的需求</li><li>以及中文分词在不同场景下的需求。<ul><li>比如搜索场景需要分的全，检索结果完善；</li><li>而推荐场景下，需要召回结果准确，不能分的太细，保留原始的完整语义。</li></ul></li></ul><p><strong>中文分词的方法</strong>：</p><ul><li>基于字典匹配<ul><li>基于最大长度匹配的方法</li><li>还有进一步改进，通过概率语言模型（N-Gram）优化的方法。</li><li>当然，无论是基于字典匹配或者后面的机器学习，词典都可以用前缀树来构建，提高运算的效率。</li></ul></li><li>基于机器学习方法<ul><li>HMM</li><li>CRF</li></ul></li></ul><p>接着也了解了动态规划——<strong>Viterbi 算法</strong>。HMM 以及 N-Gram 的实现都需要借助维特比算法，找到最大概率的分词组合。或者说，如下图，类似寻找最优路径的问题，从起点到终点，有 N 个步骤，每个步骤有 M 种选择。这样的问题都可以使用 Viterbi 算法，因为当 N 较大时，相比枚举，可节省大量时间空间。<br><img data-src="https://inno-figurebed.oss-cn-hangzhou.aliyuncs.com/img/202206112109344.png"></p><p>最后学习了从单序列建模的马尔科夫模型到双序列建模的隐马模型。</p><p><strong>马尔科夫模型</strong></p><ul><li><strong>每个状态只依赖之前有限个状态</strong><ul><li>1 阶马尔科夫（只依赖之前一个状态） == 二元语言模型</li></ul></li><li>参数<ul><li>状态序列</li><li>初始概率</li><li>转移概率</li></ul></li></ul><p><strong>隐马尔科夫模型（HMM）</strong>：</p><ul><li>单序列建模的马尔科夫模型可以解决的问题有限，因此需要双序列的 HMM<ul><li>处理如机器翻译，语音识别，词性标注，拼音纠错等业务</li><li>机器翻译：源语言序列 &lt;–&gt; 目标语言序列</li><li>语音识别：源语音信号序列 &lt;–&gt; 文字序列</li><li>词性标注：源文字序列 &lt;–&gt; 词性序列<ul><li>写/一个/程序</li><li>Verb/Num/Noun</li></ul></li></ul></li><li>参数<ul><li>状态序列（未知）</li><li>观察序列（已知）</li><li>初始概率</li><li>转移概率</li><li>发射概率（从状态到观察）</li></ul></li></ul><p>并且也对以上的方法都进行了实践。不过批量分词里配置这个 Hadoop 集群环境真的，搞得太开心了 –!</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>算法：<br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/103392455">中文分词算法之–最大匹配法 - 知乎</a><br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/32829048">自然语言处理中N-Gram模型介绍 - 知乎</a><br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/161436964">维特比算法(viterbi)原理以及简单实现 - 知乎</a></p><p>环境配置：<br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/u012757419/article/details/105431254">VMware虚拟机中Centos7网络配置及ping不通思路</a><br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/ren9436/article/details/118864737">虚拟机Linux网络配置——Net模式（CentOS7）</a><br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_45827423/article/details/122656775?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-122656775-blog-114282900.pc_relevant_antiscanv3&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-122656775-blog-114282900.pc_relevant_antiscanv3&utm_relevant_index=1">【2022】Centos7.4安装anaconda3</a><br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_44768806/article/details/118365402">secureCRT连接不上虚拟机解决方案</a><br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_40165004/article/details/121717405">SecureCRT连接虚拟机连接不上问题记录与解决</a></p></div><footer class="post-footer"><div class="reward-container"><div>Buy me a coffee</div><button>赞赏</button><div class="post-reward"><div><img src="/images/wechatpay.jpg" alt="1nnoh 微信"> <span>微信</span></div><div><img src="/images/alipay.png" alt="1nnoh 支付宝"> <span>支付宝</span></div></div></div><div class="post-copyright"><ul><li class="post-copyright-author"><strong>本文作者： </strong>1nnoh</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://1nnoh.top/2ZZ2RHB/" title="1.3 中文分词">https://1nnoh.top/2ZZ2RHB/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener external nofollow noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="post-tags"><a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a> <a href="/tags/FunRec/" rel="tag"><i class="fa fa-tag"></i> FunRec</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2K8XAGY/" rel="prev" title="爬虫与网络编程基础_Task03_BS4基础使用"><i class="fa fa-chevron-left"></i> 爬虫与网络编程基础_Task03_BS4基础使用</a></div><div class="post-nav-item"><a href="/3EGCD4S/" rel="next" title="矢量语义与编码之 TF-IDF 检索">矢量语义与编码之 TF-IDF 检索 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="waline"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">1nnoh</span></div><div class="wordcount"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-chart-line"></i> </span><span title="站点总字数">110k</span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">4:10</span></span></div><div class="busuanzi-count"><span class="post-meta-item" id="busuanzi_container_site_pv"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div><script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script><script src="/js/prism/prism.js" async></script><script src="/js/prism/prism.js" async></script></div></footer><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><div class="sidebar-dimmer"></div><div class="back-to-top" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up fa-lg"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a> <a href="https://github.com/1nnoh" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener external nofollow noreferrer" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script size="150" alpha="0.26" zindex="-2" src="https://lib.baomitu.com/ribbon.js/1.0.2/ribbon.min.js"></script><script src="https://lib.baomitu.com/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://lib.baomitu.com/medium-zoom/1.0.6/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script><script src="https://lib.baomitu.com/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="https://lib.baomitu.com/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script><script src="/js/third-party/search/local-search.js"></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script><script src="/js/third-party/math/mathjax.js"></script><script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"https://waline-server-one.vercel.app/","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"placeholder":"Just go ^ ^","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"visitor":false,"comment_count":false,"requiredFields":["nick"],"libUrl":"https://unpkg.com/@waline/client@v2/dist/waline.js","el":"#waline","comment":true,"path":"/2ZZ2RHB/"}</script><link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css"><script>document.addEventListener("page:loaded",()=>{NexT.utils.loadComments(CONFIG.waline.el).then(()=>NexT.utils.getScript(CONFIG.waline.libUrl,{condition:window.Waline})).then(()=>Waline.init(Object.assign({},CONFIG.waline,{el:document.querySelector(CONFIG.waline.el)})))})</script></body></html>